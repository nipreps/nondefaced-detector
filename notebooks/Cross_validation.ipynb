{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6435a710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-19T19:01:09.676886Z",
     "start_time": "2021-09-19T19:01:08.125564Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aec2cd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-19T19:56:20.958468Z",
     "start_time": "2021-09-19T19:56:20.526426Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv, pandas\n",
    "from nobrainer.io import read_csv\n",
    "\n",
    "\n",
    "all_csv = '/workspace/tf/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_defaced_reproduce/128/csv_F15/all.csv'\n",
    "prepend = '/workspace/tf'\n",
    "all_paths = read_csv(all_csv)\n",
    "print(all_paths)\n",
    "new_paths = []\n",
    "for p in all_paths:\n",
    "    new_paths.append((os.path.join(prepend, p[0].split('shank/')[1]), p[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2413199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_paths)\n",
    "with open('/workspace/tf/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_defaced_reproduce/128/csv_F15/new_all.csv','w') as out:\n",
    "    csv_out=csv.writer(out)\n",
    "    csv_out.writerow(['X','Y'])\n",
    "    for row in new_paths:\n",
    "        csv_out.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ed0c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-19T20:16:52.614776Z",
     "start_time": "2021-09-19T20:16:52.544771Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nobrainer.io import read_csv\n",
    "all_csv = '/workspace/tf/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_defaced_reproduce/128/csv_F15/new_all.csv'\n",
    "all_paths = read_csv(all_csv)\n",
    "\n",
    "X, Y = list(zip(*all_paths))\n",
    "X, Y = np.array(X), np.array(Y)\n",
    "\n",
    "base_path = '/workspace/tf/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_defaced_reproduce/128/csv_F15'\n",
    "\n",
    "skf = StratifiedKFold(n_splits=15, random_state=42, shuffle=True)\n",
    "\n",
    "i = 1\n",
    "for train_index, test_index in skf.split(X, Y):\n",
    "    \n",
    "    os.makedirs(os.path.join(base_path, f'train_test_fold_{i}'), exist_ok=True)\n",
    "    path = os.path.join(base_path, f'train_test_fold_{i}')\n",
    "    tpath = os.path.join(path, 'training.csv')\n",
    "    vpath = os.path.join(path, 'validation.csv')\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    with open(tpath,'w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['X','Y'])\n",
    "        for row in list(zip(X_train, y_train)):\n",
    "            csv_out.writerow(row)\n",
    "    \n",
    "    with open(vpath,'w') as out:\n",
    "        csv_out=csv.writer(out)\n",
    "        csv_out.writerow(['X','Y'])\n",
    "        for row in list(zip(X_test, y_test)):\n",
    "            csv_out.writerow(row)\n",
    "            \n",
    "    i+= 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d73f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-19T20:56:57.873365Z",
     "start_time": "2021-09-19T20:37:14.721902Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, nobrainer, csv, random\n",
    "from nobrainer.io import read_csv\n",
    "from nobrainer.tfrecord import write as write_tfrecord\n",
    "\n",
    "base_path = '/workspace/tf/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_defaced_reproduce/128/'\n",
    "\n",
    "for fold in range(1, 16):\n",
    "    csv_path = os.path.join(base_path, f'csv_F15/train_test_fold_{fold}')\n",
    "    tcsv_path = os.path.join(csv_path, 'training.csv')\n",
    "    vcsv_path = os.path.join(csv_path, 'validation.csv')\n",
    "\n",
    "    tfrecords_path = os.path.join(base_path, f'tfrecords_F15_shuffled_2/train_test_fold_{fold}')\n",
    "\n",
    "    os.makedirs(tfrecords_path, exist_ok=True)\n",
    "\n",
    "    num_parallel_calls = 10\n",
    "\n",
    "    t_filepaths = read_csv(tcsv_path)\n",
    "    v_filepaths = read_csv(vcsv_path)\n",
    "    \n",
    "    random.shuffle(t_filepaths)\n",
    "    random.shuffle(v_filepaths)\n",
    "    \n",
    "    write_tfrecord(\n",
    "            features_labels=t_filepaths,\n",
    "            filename_template=os.path.join(tfrecords_path, 'data-train_shard-{shard:03d}.tfrec'),\n",
    "            examples_per_shard=3,\n",
    "            processes=num_parallel_calls,\n",
    "    )\n",
    "    \n",
    "    write_tfrecord(\n",
    "            features_labels=v_filepaths,\n",
    "            filename_template=os.path.join(tfrecords_path, 'data-valid_shard-{shard:01d}.tfrec'),\n",
    "            examples_per_shard=1,\n",
    "            processes=num_parallel_calls,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562e3ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-19T20:57:43.223933Z",
     "start_time": "2021-09-19T20:57:43.197473Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Training module for nondefaced-detector.\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from nondefaced_detector.models import model as _model\n",
    "from nondefaced_detector.dataloaders.dataset import get_dataset\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 3:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "\n",
    "def train(\n",
    "    csv_path,\n",
    "    model_save_path,\n",
    "    tfrecords_path,\n",
    "    volume_shape=(128, 128, 128),\n",
    "    image_size=(128, 128),\n",
    "    dropout=0.2,\n",
    "    batch_size=16,\n",
    "    n_classes=2,\n",
    "    n_epochs=15,\n",
    "    mode=\"CV\",\n",
    "):\n",
    "    \"\"\"Train a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path: str - Path\n",
    "        Path to the csv file containing training volume paths, labels (X, Y).\n",
    "    model_save_path: str - Path\n",
    "        Path to where the save model and model weights.\n",
    "    tfrecords_path: str - Path\n",
    "        Path to preprocessed training tfrecords.\n",
    "    volume_shape: tuple of size 3, optional, default=(128, 128, 128)\n",
    "        The shape of the preprocessed volumes.\n",
    "    image_size: tuple of size 2, optional, default=(128, 128)\n",
    "        The shape of a 2D slice along each volume axis.\n",
    "    dropout: float, optional, default=0.4\n",
    "         Float between 0 and 1. Fraction of the input units to drop.\n",
    "    batch_size: int, optional, default=16\n",
    "        No. of training examples utilized in each iteration.\n",
    "    n_classes: int, optional, default=2\n",
    "        No. of unique classes to train the model on. Default assumption is a\n",
    "        binary classifier.\n",
    "    n_epochs: int, optional, default=15\n",
    "        No. of complete passes through the training dataset.\n",
    "    mode: str, optional, default=15\n",
    "        One of \"CV\" or \"full\". Indicates the type of training to perform.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `tf.keras.callbacks.History`\n",
    "        A History object that records several metrics such as training/validation loss/metrics\n",
    "        at successive epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    train_csv_path = os.path.join(csv_path, \"training.csv\")\n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "\n",
    "    if mode == \"CV\":\n",
    "        valid_csv_path = os.path.join(csv_path, \"validation.csv\")\n",
    "        valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "        valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "\n",
    "    weights = class_weight.compute_class_weight(\n",
    "        \"balanced\", np.unique(train_labels), train_labels\n",
    "    )\n",
    "    weights = dict(enumerate(weights))\n",
    "\n",
    "    planes = [\"axial\", \"sagittal\", \"coronal\", \"combined\"]\n",
    "\n",
    "    global_batch_size = batch_size\n",
    "\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    cp_save_path = os.path.join(model_save_path, \"weights\")\n",
    "    logdir_path = os.path.join(model_save_path, \"tb_logs\")\n",
    "    metrics_path = os.path.join(model_save_path, \"metrics\")\n",
    "\n",
    "    os.makedirs(metrics_path, exist_ok=True)\n",
    "\n",
    "    for plane in planes:\n",
    "\n",
    "        logdir = os.path.join(logdir_path, plane)\n",
    "        os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "        tbCallback = TensorBoard(log_dir=logdir)\n",
    "\n",
    "        os.makedirs(os.path.join(cp_save_path, plane), exist_ok=True)\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            os.path.join(cp_save_path, plane, \"best-wts.h5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "\n",
    "        if not plane == \"combined\":\n",
    "            lr = 1e-3\n",
    "            model = _model.Submodel(\n",
    "                root_path=cp_save_path,\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                name=plane,\n",
    "                include_top=True,\n",
    "                weights=None,\n",
    "            )\n",
    "        else:\n",
    "            lr = 5e-4\n",
    "            model = _model.CombinedClassifier(\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                trainable=True,\n",
    "                wts_root=cp_save_path,\n",
    "            )\n",
    "\n",
    "        print(\"Submodel: \", plane)\n",
    "\n",
    "        METRICS = [\n",
    "            metrics.TruePositives(name=\"tp\"),\n",
    "            metrics.FalsePositives(name=\"fp\"),\n",
    "            metrics.TrueNegatives(name=\"tn\"),\n",
    "            metrics.FalseNegatives(name=\"fn\"),\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            metrics.Precision(name=\"precision\"),\n",
    "            metrics.Recall(name=\"recall\"),\n",
    "            metrics.AUC(name=\"auc\"),\n",
    "        ]\n",
    "\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.binary_crossentropy,\n",
    "            optimizer=Adam(learning_rate=lr),\n",
    "            metrics=METRICS,\n",
    "        )\n",
    "\n",
    "        dataset_train = get_dataset(\n",
    "            file_pattern=os.path.join(tfrecords_path, \"data-train_*\"),\n",
    "            n_classes=n_classes,\n",
    "            batch_size=global_batch_size,\n",
    "            volume_shape=volume_shape,\n",
    "            plane=plane,\n",
    "            n_slices=80,\n",
    "            shuffle_buffer_size=global_batch_size,\n",
    "        )\n",
    "\n",
    "        steps_per_epoch = math.ceil(len(train_paths) / batch_size)\n",
    "\n",
    "        if mode == \"CV\":\n",
    "            earlystopping = EarlyStopping(monitor=\"val_accuracy\", patience=3)\n",
    "\n",
    "            dataset_valid = get_dataset(\n",
    "                file_pattern=os.path.join(tfrecords_path, \"data-valid_*\"),\n",
    "                n_classes=n_classes,\n",
    "                batch_size=global_batch_size,\n",
    "                volume_shape=volume_shape,\n",
    "                plane=plane,\n",
    "                n_slices=80,\n",
    "                shuffle_buffer_size=global_batch_size,\n",
    "            )\n",
    "\n",
    "            validation_steps = math.ceil(len(valid_paths) / batch_size)\n",
    "\n",
    "            history = model.fit(\n",
    "                dataset_train,\n",
    "                epochs=n_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                validation_data=dataset_valid,\n",
    "                validation_steps=validation_steps,\n",
    "                callbacks=[tbCallback, model_checkpoint, earlystopping],\n",
    "                class_weight=weights,\n",
    "            )\n",
    "\n",
    "            hist_df = pd.DataFrame(history.history)\n",
    "\n",
    "        else:\n",
    "#             earlystopping = EarlyStopping(monitor=\"loss\", patience=3)\n",
    "            print(model.summary())\n",
    "            print(\"Steps/Epoch: \", steps_per_epoch)\n",
    "            history = model.fit(\n",
    "                dataset_train,\n",
    "                epochs=n_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                callbacks=[tbCallback, model_checkpoint, earlystopping],\n",
    "                class_weight=weights,\n",
    "            )\n",
    "\n",
    "        hist_df = pd.DataFrame(history.history)\n",
    "        jsonfile = os.path.join(metrics_path, plane + \".json\")\n",
    "\n",
    "        with open(jsonfile, mode=\"w\") as f:\n",
    "            hist_df.to_json(f)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc68d20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-19T20:57:47.190740Z",
     "start_time": "2021-09-19T20:57:47.159986Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "root_dir = '/workspace/tf/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_defaced_reproduce/128'\n",
    "\n",
    "for fold in range(1, 12):\n",
    "    csv_path = os.path.join(root_dir, f'csv_F15/train_test_fold_{fold}')\n",
    "    model_save_path = os.path.join(root_dir, f'model_save_dir_F15_shuffled_3/train_test_fold_{fold}')\n",
    "    tfrecords_path = os.path.join(root_dir, f'tfrecords_F15_shuffled_2/train_test_fold_{fold}')\n",
    "    history = train(csv_path, model_save_path, tfrecords_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
