{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert test data to tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "test_root_dir = \"/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi\"\n",
    "csv_path = os.path.join(test_root_dir, \"csv\")\n",
    "tf_records_dir = os.path.join(test_root_dir, \"tfrecords\")\n",
    "\n",
    "os.makedirs(tf_records_dir, exist_ok=True)\n",
    "\n",
    "test_csv_path = os.path.join(csv_path, \"testing.csv\")\n",
    "test_paths = pd.read_csv(test_csv_path)[\"X\"].values\n",
    "test_labels = pd.read_csv(test_csv_path)[\"Y\"].values\n",
    "test_D = list(zip(test_paths, test_labels))\n",
    "test_write_path = os.path.join(tf_records_dir, 'data-test_shard-{shard:03d}.tfrec')\n",
    "\n",
    "nobrainer.tfrecord.write(\n",
    "    features_labels=test_D,\n",
    "    filename_template=test_write_path,\n",
    "    examples_per_shard=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root_dir = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi'\n",
    "model_save_path = os.path.join(ROOTDIR_B, \"model_save_dir_full\")\n",
    "tfrecords_path = os.path.join(test_root_dir, \"tfrecords\")\n",
    "plane = \"axial\"\n",
    "dataset_plane = get_dataset(\n",
    "        file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "        n_classes=2,\n",
    "        batch_size=16,\n",
    "        volume_shape=(128, 128, 128),\n",
    "        plane=plane,\n",
    "        mode='test'\n",
    "    )\n",
    "\n",
    "print(dataset_plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from models.modelN import CombinedClassifier\n",
    "from dataloaders.dataset import get_dataset\n",
    "\n",
    "\n",
    "# Tf packages\n",
    "import tensorflow as tf\n",
    "\n",
    "def inference(tfrecords_path, weights_path):\n",
    "    \n",
    "    model = CombinedClassifier(\n",
    "        input_shape=(128, 128), dropout=0.4, wts_root=None, trainable=True)\n",
    "    \n",
    "    model.load_weights(os.path.abspath(weights_path))\n",
    "    model.trainable = False\n",
    "    \n",
    "    dataset_test = get_dataset(\n",
    "        file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "        n_classes=2,\n",
    "        batch_size=16,\n",
    "        volume_shape=(128, 128, 128),\n",
    "        plane='combined',\n",
    "        mode='test'\n",
    "    )\n",
    "\n",
    "    METRICS = [\n",
    "        metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        metrics.Precision(name=\"precision\"),\n",
    "        metrics.Recall(name=\"recall\"),\n",
    "        metrics.AUC(name=\"auc\"),\n",
    "    ]\n",
    "    \n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.binary_crossentropy,\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "    \n",
    "    results = model.evaluate(dataset_test, batch_size=16)\n",
    "    predictions = (model.predict(dataset_test) > 0.5).astype(int)\n",
    "    \n",
    "    \n",
    "ROOTDIR_B = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_B/128'\n",
    "ROOTDIR_A = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_A/128'\n",
    "test_root_dir = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi'\n",
    "\n",
    "model_save_path = os.path.join(ROOTDIR_B, \"model_save_dir_full\")\n",
    "tfrecords_path = os.path.join(test_root_dir, \"tfrecords\")\n",
    "print(\"TFRECORDS: \", tfrecords_path)\n",
    "weights_path = os.path.join(model_save_path, 'weights/combined/best-wts.h5')\n",
    "    \n",
    "model = CombinedClassifier(\n",
    "    input_shape=(128, 128), dropout=0.4, wts_root=None, trainable=True\n",
    ")\n",
    "model.load_weights(os.path.abspath(weights_path))\n",
    "\n",
    "print(os.path.join(tfrecords_path, \"data-test_*\"))\n",
    "\n",
    "dataset_test = get_dataset(\n",
    "    file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "    n_classes=2,\n",
    "#     n_slices = 24,\n",
    "    batch_size=16,\n",
    "    volume_shape=(128, 128, 128),\n",
    "    plane='combined',\n",
    "    mode='test'\n",
    ")\n",
    "\n",
    "print(dataset_test)\n",
    "\n",
    "METRICS = [\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            metrics.Precision(name=\"precision\"),\n",
    "            metrics.Recall(name=\"recall\"),\n",
    "            metrics.AUC(name=\"auc\"),\n",
    "        ]\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    metrics=METRICS,\n",
    ")\n",
    "\n",
    "    \n",
    "results = model.evaluate(dataset_test, batch_size=16)\n",
    "predictions = (model.predict(dataset_test) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = ['coronal'] #, 'coronal', 'sagittal']\n",
    "\n",
    "for plane in planes:\n",
    "    \n",
    "    model = modelN.Submodel(\n",
    "        input_shape=(128, 128),\n",
    "        dropout=0.2,\n",
    "        name=plane,\n",
    "        include_top=True,\n",
    "        weights=None,\n",
    "        trainable=False,\n",
    "    )\n",
    "    \n",
    "    print(os.path.join(model_save_path, plane, 'best-wts.h5'))\n",
    "    \n",
    "    model.load_weights(os.path.join(model_save_path, 'weights', plane, 'best-wts.h5'))\n",
    "    \n",
    "    dataset_plane = get_dataset(\n",
    "        file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "        n_classes=2,\n",
    "        batch_size=16,\n",
    "        volume_shape=(128, 128, 128),\n",
    "        plane=plane,\n",
    "        mode='test',)\n",
    "    \n",
    "    METRICS = [\n",
    "        metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        metrics.Precision(name=\"precision\"),\n",
    "        metrics.Recall(name=\"recall\"),\n",
    "        metrics.AUC(name=\"auc\"),\n",
    "    ]\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.binary_crossentropy,\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "    \n",
    "#     results = model.evaluate(dataset_plane, batch_size=16)\n",
    "    predictions = (model.predict(dataset_plane) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "corr_pred_map = {}\n",
    "corr = 0\n",
    "incorr = 0\n",
    "for x, y in dataset_plane.as_numpy_iterator():\n",
    "    \n",
    "    batch_predictions = (model.predict(x) > 0.5).astype(int)\n",
    "    all_imgs = []\n",
    "    for i in range(len(batch_predictions)):\n",
    "        if batch_predictions.flatten()[i] != y.flatten()[i].astype(int):\n",
    "            incorr += 1\n",
    "            print(\"Predicted: \",batch_predictions.flatten()[i], \"Actual: \", y.flatten()[i].astype(int))\n",
    "        else:\n",
    "            corr += 1\n",
    "            \n",
    "#             fig = plt.figure(figsize=(25, 8))\n",
    "#             rows, cols = 3, 16\n",
    "            \n",
    "#             for i in range(1, cols*rows + 1):\n",
    "# #                 if i/cols == 1:\n",
    "# #                     use = x['coronal']\n",
    "# #                 if i/cols == 2:\n",
    "# #                     use = x['sagittal']\n",
    "                    \n",
    "#                 fig.add_subplot(rows, cols, i)\n",
    "                \n",
    "#                 plt.imshow(use[(i-1)%cols,:,:, 0])\n",
    "\n",
    "\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr, incorr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
