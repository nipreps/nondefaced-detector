{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert test data to tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "test_root_dir = \"/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi\"\n",
    "csv_path = os.path.join(test_root_dir, \"csv\")\n",
    "tf_records_dir = os.path.join(test_root_dir, \"tfrecords\")\n",
    "\n",
    "os.makedirs(tf_records_dir, exist_ok=True)\n",
    "\n",
    "test_csv_path = os.path.join(csv_path, \"testing.csv\")\n",
    "test_paths = pd.read_csv(test_csv_path)[\"X\"].values\n",
    "test_labels = pd.read_csv(test_csv_path)[\"Y\"].values\n",
    "test_D = list(zip(test_paths, test_labels))\n",
    "test_write_path = os.path.join(tf_records_dir, 'data-test_shard-{shard:03d}.tfrec')\n",
    "\n",
    "nobrainer.tfrecord.write(\n",
    "    features_labels=test_D,\n",
    "    filename_template=test_write_path,\n",
    "    examples_per_shard=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root_dir = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi'\n",
    "model_save_path = os.path.join(ROOTDIR_B, \"model_save_dir_full\")\n",
    "tfrecords_path = os.path.join(test_root_dir, \"tfrecords\")\n",
    "plane = \"axial\"\n",
    "dataset_plane = get_dataset(\n",
    "        file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "        n_classes=2,\n",
    "        batch_size=16,\n",
    "        volume_shape=(128, 128, 128),\n",
    "        plane=plane,\n",
    "        mode='test'\n",
    "    )\n",
    "\n",
    "print(dataset_plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "from models.modelN import CombinedClassifier\n",
    "from dataloaders.dataset import get_dataset\n",
    "\n",
    "\n",
    "# Tf packages\n",
    "import tensorflow as tf\n",
    "\n",
    "def inference(tfrecords_path, weights_path):\n",
    "    \n",
    "    model = CombinedClassifier(\n",
    "        input_shape=(128, 128), dropout=0.4, wts_root=None, trainable=True)\n",
    "    \n",
    "    model.load_weights(os.path.abspath(weights_path))\n",
    "    model.trainable = False\n",
    "    \n",
    "    dataset_test = get_dataset(\n",
    "        file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "        n_classes=2,\n",
    "        batch_size=16,\n",
    "        volume_shape=(128, 128, 128),\n",
    "        plane='combined',\n",
    "        mode='test'\n",
    "    )\n",
    "\n",
    "    METRICS = [\n",
    "        metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        metrics.Precision(name=\"precision\"),\n",
    "        metrics.Recall(name=\"recall\"),\n",
    "        metrics.AUC(name=\"auc\"),\n",
    "    ]\n",
    "    \n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.binary_crossentropy,\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "    \n",
    "    results = model.evaluate(dataset_test, batch_size=16)\n",
    "    predictions = (model.predict(dataset_test) > 0.5).astype(int)\n",
    "    \n",
    "    \n",
    "ROOTDIR_B = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_B/128'\n",
    "ROOTDIR_A = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_A/128'\n",
    "test_root_dir = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi'\n",
    "\n",
    "model_save_path = os.path.join(ROOTDIR_B, \"model_save_dir_full\")\n",
    "tfrecords_path = os.path.join(test_root_dir, \"tfrecords\")\n",
    "print(\"TFRECORDS: \", tfrecords_path)\n",
    "weights_path = os.path.join(model_save_path, 'weights/combined/best-wts.h5')\n",
    "    \n",
    "model = CombinedClassifier(\n",
    "    input_shape=(128, 128), dropout=0.4, wts_root=None, trainable=True\n",
    ")\n",
    "model.load_weights(os.path.abspath(weights_path))\n",
    "\n",
    "print(os.path.join(tfrecords_path, \"data-test_*\"))\n",
    "\n",
    "dataset_test = get_dataset(\n",
    "    file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "    n_classes=2,\n",
    "#     n_slices = 24,\n",
    "    batch_size=16,\n",
    "    volume_shape=(128, 128, 128),\n",
    "    plane='combined',\n",
    "    mode='test'\n",
    ")\n",
    "\n",
    "print(dataset_test)\n",
    "\n",
    "METRICS = [\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            metrics.Precision(name=\"precision\"),\n",
    "            metrics.Recall(name=\"recall\"),\n",
    "            metrics.AUC(name=\"auc\"),\n",
    "        ]\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    metrics=METRICS,\n",
    ")\n",
    "\n",
    "    \n",
    "results = model.evaluate(dataset_test, batch_size=16)\n",
    "predictions = (model.predict(dataset_test) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = ['coronal'] #, 'coronal', 'sagittal']\n",
    "\n",
    "for plane in planes:\n",
    "    \n",
    "    model = modelN.Submodel(\n",
    "        input_shape=(128, 128),\n",
    "        dropout=0.2,\n",
    "        name=plane,\n",
    "        include_top=True,\n",
    "        weights=None,\n",
    "        trainable=False,\n",
    "    )\n",
    "    \n",
    "    print(os.path.join(model_save_path, plane, 'best-wts.h5'))\n",
    "    \n",
    "    model.load_weights(os.path.join(model_save_path, 'weights', plane, 'best-wts.h5'))\n",
    "    \n",
    "    dataset_plane = get_dataset(\n",
    "        file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "        n_classes=2,\n",
    "        batch_size=16,\n",
    "        volume_shape=(128, 128, 128),\n",
    "        plane=plane,\n",
    "        mode='test',)\n",
    "    \n",
    "    METRICS = [\n",
    "        metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        metrics.Precision(name=\"precision\"),\n",
    "        metrics.Recall(name=\"recall\"),\n",
    "        metrics.AUC(name=\"auc\"),\n",
    "    ]\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.binary_crossentropy,\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "    \n",
    "#     results = model.evaluate(dataset_plane, batch_size=16)\n",
    "    predictions = (model.predict(dataset_plane) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing 6 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/shank/Stanford/nondefaced-detector/examples/sample_vols/faced/preprocessed/example1.nii.gz', '/home/shank/Stanford/nondefaced-detector/examples/sample_vols/faced/preprocessed/example2.nii.gz', '/home/shank/Stanford/nondefaced-detector/examples/sample_vols/faced/preprocessed/example3.nii.gz', '/home/shank/Stanford/nondefaced-detector/examples/sample_vols/defaced/preprocessed/example1.nii.gz', '/home/shank/Stanford/nondefaced-detector/examples/sample_vols/defaced/preprocessed/example2.nii.gz', '/home/shank/Stanford/nondefaced-detector/examples/sample_vols/defaced/preprocessed/example3.nii.gz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "path = '/home/shank/Stanford/nondefaced-detector/examples/sample_vols/example.csv'\n",
    "\n",
    "              \n",
    "if path.endswith('csv'):\n",
    "    filepaths = []\n",
    "    skip_header =True\n",
    "    with open(path, newline=\"\") as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=\",\")\n",
    "        if skip_header:\n",
    "            next(reader)\n",
    "            \n",
    "        for row in reader:\n",
    "            filepaths.append(row[0])\n",
    "\n",
    "from nondefaced_detector.preprocess import preprocess, cleanup_files\n",
    "from nondefaced_detector.preprocess import preprocess_parallel\n",
    "\n",
    "num_parallel_calls = None\n",
    "if num_parallel_calls is None:\n",
    "    # Get number of processes allocated to the current process.\n",
    "    # Note the difference from `os.cpu_count()`.\n",
    "    num_parallel_calls = len(os.sched_getaffinity(0))\n",
    "\n",
    "outputs = preprocess_parallel(\n",
    "        filepaths,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "        with_label=False,\n",
    ")\n",
    "\n",
    "print(outputs)\n",
    "\n",
    "# cleanup_files(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:02<00:00,  2.81it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Methods to predict using trained models\"\"\"\n",
    "\n",
    "import functools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import multiprocessing as mp\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nondefaced_detector.helpers       import utils\n",
    "from nondefaced_detector.models.modelN import CombinedClassifier\n",
    "\n",
    "\n",
    "def _predict(volume, model):\n",
    "    \"\"\"Return predictions from `inputs`.\n",
    "\n",
    "    This is a general prediction method.\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(volume, (np.ndarray)):\n",
    "        raise ValueError(\"volume is not a numpy ndarray\")\n",
    "        \n",
    "    ds = _structural_slice(volume, plane=\"combined\", n_slices=n_slices)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(ds)\n",
    "    ds = ds.batch(batch_size=1, drop_remainder=False)\n",
    "\n",
    "    predicted = model.predict(ds)\n",
    "\n",
    "    return predicted\n",
    "\n",
    "\n",
    "def predict(volumes, model_path, n_slices=32):\n",
    "    \n",
    "    if not isinstance(volumes, list):\n",
    "        raise ValueError('Volumes need to be a list of paths to preprocessed MRI volumes.')\n",
    "    \n",
    "    outputs = []\n",
    "    model = _get_model(model_path)\n",
    "    \n",
    "    for path in tqdm(volumes, total=len(volumes)):\n",
    "        vol,_,_ = utils.load_vol(path)\n",
    "        predicted = _predict(vol, model)\n",
    "        \n",
    "        outputs.append((path, predicted[0][0]))\n",
    "        \n",
    "    return outputs\n",
    "    \n",
    "    \n",
    "def _structural_slice(x, plane, n_slices=16):\n",
    "\n",
    "    \"\"\"Transpose dataset based on the plane\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "\n",
    "    plane:\n",
    "\n",
    "    n_slices:\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "\n",
    "    options = [\"sagittal\", \"coronal\", \"axial\", \"combined\"]\n",
    "\n",
    "    if isinstance(plane, str) and plane in options:\n",
    "        idxs = np.random.randint(x.shape[0], size=(n_slices, 3))\n",
    "        if plane == \"sagittal\":\n",
    "            midx = idxs[:, 0]\n",
    "            x = x\n",
    "\n",
    "        if plane == \"coronal\":\n",
    "            midx = idxs[:, 1]\n",
    "            x = tf.transpose(x, perm=[1, 2, 0])\n",
    "\n",
    "        if plane == \"axial\":\n",
    "            midx = idxs[:, 2]\n",
    "            x = tf.transpose(x, perm=[2, 0, 1])\n",
    "\n",
    "        if plane == \"combined\":\n",
    "            temp = {}\n",
    "            for op in options[:-1]:\n",
    "                temp[op] = _structural_slice(x, op, n_slices)\n",
    "            x = temp\n",
    "\n",
    "        if not plane == \"combined\":\n",
    "            x = tf.squeeze(tf.gather_nd(x, midx.reshape(n_slices, 1, 1)), axis=1)\n",
    "            x = tf.math.reduce_mean(x, axis=0, keepdims=True)\n",
    "            x = tf.expand_dims(x, axis=-1)\n",
    "            x = tf.convert_to_tensor(x)\n",
    "\n",
    "        return x\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Expected plane to be one of [sagittal, coronal, axial, combined]\"\n",
    "        )\n",
    "\n",
    "\n",
    "def _get_model(model_path):\n",
    "\n",
    "    \"\"\"Return `tf.keras.Model` object from a filepath.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str, path to HDF5 or SavedModel file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Instance of `tf.keras.Model`.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    `ValueError` if cannot load model.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        p = Path(model_path).resolve()\n",
    "\n",
    "        model = CombinedClassifier(input_shape=(128, 128), wts_root=p, trainable=False)\n",
    "\n",
    "        combined_weights = list(Path(os.path.join(p, \"combined\")).glob(\"*.h5\"))[\n",
    "            0\n",
    "        ].resolve()\n",
    "\n",
    "        model.load_weights(combined_weights)\n",
    "        model.trainable = False\n",
    "\n",
    "        return model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "    raise ValueError(\"Failed to load model.\")\n",
    "    \n",
    "preds = predict(outputs, model_path='/home/shank/Stanford/nondefaced-detector/nondefaced_detector/models/pretrained_weights')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/faced/preprocessed/example1.nii.gz', 0.99998486), ('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/faced/preprocessed/example2.nii.gz', 0.9999981), ('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/faced/preprocessed/example3.nii.gz', 0.9970654), ('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/defaced/preprocessed/example1.nii.gz', 0.016103715), ('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/defaced/preprocessed/example2.nii.gz', 0.9974597), ('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/defaced/preprocessed/example3.nii.gz', 0.0201056)]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
