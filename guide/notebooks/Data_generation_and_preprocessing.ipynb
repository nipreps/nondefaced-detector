{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Details\n",
    "\n",
    "TBA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/shank/miniconda3/envs/fitlins38/lib/python3.8/site-packages (4.57.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nobrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  16\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}\n",
      "[('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/faced/example1.nii.gz', '1'), ('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/faced/example2.nii.gz', '1'), ('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/faced/example3.nii.gz', '1'), ('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/defaced/example1.nii.gz', '0'), ('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/defaced/example2.nii.gz', '0'), ('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/defaced/example3.nii.gz', '0')]\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "import tempfile\n",
    "import sys, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nondefaced_detector.preprocessing.normalization import clip, normalize, standardize\n",
    "from nondefaced_detector.preprocessing.conform       import conform_data\n",
    "from nondefaced_detector.helpers                     import utils\n",
    "\n",
    "print(\"Number of processors: \", mp.cpu_count())\n",
    "print(os.sched_getaffinity(0))\n",
    "\n",
    "from nobrainer.io import read_csv, verify_features_labels\n",
    "\n",
    "\n",
    "# verify_features_labels(temp)\n",
    "\n",
    "def preprocess(\n",
    "    vol_path,\n",
    "    conform_volume_to=(128, 128, 128),\n",
    "    conform_zooms=(2.0, 2.0, 2.0),\n",
    "    save_path=None,\n",
    "    with_label=False,\n",
    "):\n",
    "    \n",
    "    try:\n",
    "        vpath = vol_path\n",
    "        if with_label:\n",
    "            if len(vol_path) != 2:\n",
    "                raise ValueError(\n",
    "                    \"The vol_path must have length of 2 when with_label=True\"\n",
    "                )\n",
    "            \n",
    "            vpath, label = vol_path\n",
    "        \n",
    "        spath = os.path.join(os.path.dirname(vpath), 'preprocessed')\n",
    "        if save_path:\n",
    "            spath = os.path.join(save_path, 'preprocessed')\n",
    "        \n",
    "        os.makedirs(spath, exist_ok=True)\n",
    "\n",
    "        volume, affine, _ = utils.load_vol(vpath)\n",
    "\n",
    "        # Prepocessing\n",
    "        volume = clip(volume, q=90)\n",
    "        volume = normalize(volume)\n",
    "        volume = standardize(volume)\n",
    "        \n",
    "        \n",
    "        tmp_preprocess_vol = tempfile.NamedTemporaryFile(\n",
    "            suffix=\".nii.gz\",\n",
    "            delete=True,\n",
    "            dir=spath,\n",
    "        )\n",
    "        \n",
    "        utils.save_vol(tmp_preprocess_vol.name, volume, affine)\n",
    "    \n",
    "    \n",
    "        tmp_conform_vol = os.path.join(spath, os.path.basename(vpath))\n",
    "        \n",
    "        conform_data(\n",
    "            tmp_preprocess_vol.name,\n",
    "            out_file=tmp_conform_vol,\n",
    "            out_size=conform_volume_to,\n",
    "            out_zooms=conform_zooms)\n",
    "        \n",
    "        tmp_preprocess_vol.close()\n",
    "        \n",
    "        if with_label:\n",
    "            return (tmp_conform_vol, label)\n",
    "        return tmp_conform_vol\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "    \n",
    "def cleanup_files(*args):\n",
    "    for p in args:\n",
    "        if os.path.exists(p):\n",
    "            os.remove(p)\n",
    "            \n",
    "def preprocess_csv(\n",
    "    volume_filepaths,\n",
    "    num_parallel_calls=None,\n",
    "    conform_volume_to=(128, 128, 128),\n",
    "    conform_zooms=(2.0, 2.0, 2.0),\n",
    "    save_path=None,\n",
    "    with_label=True,\n",
    "):\n",
    "\n",
    "    try:\n",
    "        map_fn = functools.partial(\n",
    "            preprocess,\n",
    "            conform_volume_to=conform_volume_to,\n",
    "            conform_zooms=conform_zooms,\n",
    "            save_path=save_path,\n",
    "            with_label=with_label\n",
    "        )\n",
    "        \n",
    "        if num_parallel_calls is None:\n",
    "            # Get number of eligible CPUs.\n",
    "            num_parallel_calls = len(os.sched_getaffinity(0))\n",
    "        \n",
    "        print(\"Preprocessing {} examples\".format(len(volume_filepaths)))\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        if num_parallel_calls == 1:\n",
    "            for vf in tqdm(volume_filepaths, total=len(volume_filepaths)):\n",
    "                result = map_fn(vf)\n",
    "                outputs.append(result)      \n",
    "        else:\n",
    "            pool = mp.Pool(num_parallel_calls)\n",
    "            for result in tqdm(pool.imap(func=map_fn, iterable=volume_filepaths), total=len(volume_filepaths)):\n",
    "                outputs.append(result)\n",
    "                \n",
    "        return outputs\n",
    "                 \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "# import csv\n",
    "# temp = []\n",
    "# with open('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/example.csv', 'r') as file:\n",
    "#     reader = csv.reader(file)\n",
    "#     for row in reader:\n",
    "#         temp.append(row[0])\n",
    "    \n",
    "\n",
    "temp = read_csv('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/example.csv', skip_header=False)\n",
    "\n",
    "# vpaths = list(zip(*temp))[0]\n",
    "# print(preprocess(temp[0], with_label=True))\n",
    "# outputs = preprocess_csv(temp)\n",
    "# print(outputs)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  16\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}\n",
      "Verifying 6 examples\n",
      "6/6 [==============================] - 0s 7ms/step\n",
      "Preprocessing 6 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying 6 examples\n",
      "\r",
      "0/6 [..............................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "6/6 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import functools\n",
    "import tempfile\n",
    "import sys, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nondefaced_detector.preprocessing.normalization import clip, normalize, standardize\n",
    "from nondefaced_detector.preprocessing.conform       import conform_data\n",
    "from nondefaced_detector.helpers                     import utils\n",
    "from nondefaced_detector.preprocess import preprocess_parallel\n",
    "\n",
    "print(\"Number of processors: \", mp.cpu_count())\n",
    "print(os.sched_getaffinity(0))\n",
    "\n",
    "from nobrainer.io import read_csv, verify_features_labels\n",
    "\n",
    "num_parallel_calls=-1\n",
    "volume_shape=(128,128,128)\n",
    "preprocess_path=None\n",
    "volume_filepaths = read_csv('/home/shank/Stanford/nondefaced-detector/examples/sample_vols/example.csv', skip_header=False)\n",
    "\n",
    "num_parallel_calls = None if num_parallel_calls == -1 else num_parallel_calls\n",
    "if num_parallel_calls is None:\n",
    "    # Get number of processes allocated to the current process.\n",
    "    # Note the difference from `os.cpu_count()`.\n",
    "    num_parallel_calls = len(os.sched_getaffinity(0))\n",
    "\n",
    "invalid_pairs = verify_features_labels(\n",
    "    volume_filepaths,\n",
    "    check_labels_int=True,\n",
    "    num_parallel_calls=num_parallel_calls,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "## UNCOMMENT the following when https://github.com/neuronets/nobrainer/pull/125\n",
    "## is merged\n",
    "# if not invalid_pairs:\n",
    "#     click.echo(click.style(\"Passed verification.\", fg=\"green\"))\n",
    "# else:\n",
    "#     click.echo(click.style(\"Failed verification.\", fg=\"red\"))\n",
    "#     for pair in invalid_pairs:\n",
    "#         click.echo(pair[0])\n",
    "#         click.echo(pair[1])\n",
    "#     sys.exit(-1)\n",
    "\n",
    "ppaths = preprocess_parallel(\n",
    "    volume_filepaths,\n",
    "    conform_volume_to=volume_shape,\n",
    "    num_parallel_calls=num_parallel_calls,\n",
    "    save_path=preprocess_path,\n",
    ")\n",
    "\n",
    "invalid_pairs = verify_features_labels(\n",
    "    ppaths,\n",
    "    volume_shape=volume_shape,\n",
    "    check_labels_int=True,\n",
    "    num_parallel_calls=num_parallel_calls,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shank/Stanford/nondefaced-detector/examples/sample_vols/faced/preprocessed/tfrecords/data-train_shard-{shard:03d}.tfrec\n",
      "2/2 [==============================] - 1s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "import nobrainer\n",
    "\n",
    "\n",
    "tfrecords_template = 'tfrecords/data-train_shard-{shard:03d}.tfrec'\n",
    "\n",
    "os.makedirs(os.path.dirname(tfrecords_template), exist_ok=True)\n",
    "\n",
    "print(tfrecords_path)\n",
    "\n",
    "nobrainer.tfrecord.write(\n",
    "        features_labels=ppaths,\n",
    "        filename_template=tfrecords_template,\n",
    "        examples_per_shard=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-train_shard-000.tfrec  data-train_shard-001.tfrec\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "# Define paths\n",
    "ROOT_DIR = '/home/shank/HDDLinux/Stanford/data/mriqc-shared/conformed'\n",
    "\n",
    "face_path = os.path.join(ROOT_DIR, 'face/128')\n",
    "defaced_path = os.path.join(ROOT_DIR, 'face_defaced/128')\n",
    "refaced_path = os.path.join(ROOT_DIR, 'face_refaced/128')\n",
    "\n",
    "paths_d = []\n",
    "paths_f = []\n",
    "paths_r = []\n",
    "\n",
    "for path in glob(defaced_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_d.append(path)\n",
    "    \n",
    "for path in glob(refaced_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_r.append(path)\n",
    "    \n",
    "for path in glob(face_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_f.append(path)\n",
    "    \n",
    "\n",
    "def generate_datasets(fpaths, dpaths, size, typ ='faced'):\n",
    "    \n",
    "    if typ not in ['faced', 'refaced']:\n",
    "        print(\"Incorrect value for t. Choose from [faced, refaced]\")\n",
    "        return\n",
    "    \n",
    "    random.shuffle(fpaths)\n",
    "    test_f = fpaths[:size]\n",
    "    main_f = fpaths[size:]\n",
    "\n",
    "    test_d = []\n",
    "    for t in test_f:\n",
    "        if typ == 'faced':\n",
    "            test_d.append(t.replace('face', 'face_defaced'))\n",
    "        \n",
    "        if typ == 'refaced':\n",
    "            DS = t.split('/')[-2]\n",
    "            sub = t.split('/')[-1].replace('_defaced_refaced', '').split('.nii.gz')[0]\n",
    "            search_pattern = os.path.join(DS, sub)\n",
    "            \n",
    "            # match pattern from defaced dataset\n",
    "            for _d in dpaths:\n",
    "                if search_pattern in _d:\n",
    "                    test_d.append(_d)\n",
    "                \n",
    "\n",
    "    test = test_f + test_d\n",
    "    labels_test = [1]*len(test_f) + [0]*len(test_d)\n",
    "    \n",
    "    # remove T_A_D from defaced volume set\n",
    "    main_d = list(set(dpaths) - set(test_d))\n",
    "    \n",
    "    labels_main = [1]*len(main_f) + [0]*len(main_d)\n",
    "    main = main_f + main_d\n",
    "    \n",
    "    return main, labels_main, test, labels_test\n",
    "\n",
    "A_2, L_A_2, T_A, L_T_A = generate_datasets(paths_f, paths_d, 49, typ='faced')\n",
    "B_2, L_B_2, T_B, L_T_B = generate_datasets(paths_r, paths_d, 49, typ='refaced')\n",
    "\n",
    "print(len(A_2), len(T_A))\n",
    "print(len(B_2), len(T_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nondefaced_detector import preprocess\n",
    "vol_path = '../../examples/sample_vols/IXI002-Guys-0828-T1.nii.gz'\n",
    "save_path = ''\n",
    "ppath, cpath = preprocess.preprocess(vol_path, save_path=save_path)\n",
    "print(ppath, cpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate n-fold CV Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "\n",
    "def generate_CSV(paths, labels, save_path, test_paths=None, test_labels=None, n=15, mode='CV'):\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"X\"] = paths\n",
    "    df[\"Y\"] = labels\n",
    "    df.to_csv(os.path.join(save_path, \"all.csv\"))\n",
    "    \n",
    "    if mode == 'CV':\n",
    "        SPLITS = n\n",
    "        skf = StratifiedKFold(n_splits=SPLITS)\n",
    "        fold_no = 1\n",
    "\n",
    "        for train_index, test_index in skf.split(paths, labels):\n",
    "            out_path = os.path.join(save_path, \"train_test_fold_{}/csv/\".format(fold_no))\n",
    "\n",
    "            if not os.path.exists(out_path):\n",
    "                os.makedirs(out_path)\n",
    "\n",
    "            image_train, image_test = (\n",
    "                itemgetter(*train_index)(paths),\n",
    "                itemgetter(*test_index)(paths),\n",
    "            )\n",
    "\n",
    "            label_train, label_test = (\n",
    "                itemgetter(*train_index)(labels),\n",
    "                itemgetter(*test_index)(labels),\n",
    "            )\n",
    "\n",
    "            train_data = {\"X\": image_train , \"Y\": label_train}\n",
    "            df_train = pd.DataFrame(train_data)\n",
    "            df_train.to_csv(os.path.join(out_path, \"training.csv\"), index=False)\n",
    "\n",
    "            validation_data = {\"X\": image_test, \"Y\": label_test}\n",
    "            df_validation = pd.DataFrame(validation_data)\n",
    "            df_validation.to_csv(os.path.join(out_path, \"validation.csv\"), index=False)\n",
    "\n",
    "            fold_no += 1\n",
    "    else:\n",
    "        train_data = {\"X\": paths , \"Y\": labels}\n",
    "        df_train = pd.DataFrame(train_data)\n",
    "        df_train.to_csv(os.path.join(save_path, \"training.csv\"), index=False)\n",
    "        \n",
    "        test_data = {\"X\": test_paths , \"Y\": test_labels}\n",
    "        df_test = pd.DataFrame(test_data)\n",
    "        df_test.to_csv(os.path.join(save_path, \"testing.csv\"), index=False)\n",
    "        \n",
    "ROOTDIR = '/home/shank/HDDLinux/Stanford/data/mriqc-shared/experiments'\n",
    "\n",
    "## CROSS VALIDATION\n",
    "# generate_CSV(A_2, L_A_2, \"experiments/experiment_A/csv_F15\")\n",
    "generate_CSV(B_2, L_B_2, os.path.join(ROOTDIR, \"experiment_B/128/csv_F15\"), mode='CV')\n",
    "\n",
    "\n",
    "## DEFINE A ROOT DIR where all the data will be stored <<<<<\n",
    "# ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/experiments' \n",
    "\n",
    "## FULL DATASET\n",
    "# generate_CSV(A_2,\n",
    "#              L_A_2,\n",
    "#              os.path.join(ROOTDIR, 'experiment_A/128/csv_full'),\n",
    "#              test_paths=T_A,\n",
    "#              test_labels=L_T_A,\n",
    "#              mode='full')\n",
    "\n",
    "# generate_CSV(B_2,\n",
    "#              L_B_2,\n",
    "#              os.path.join(ROOTDIR, 'experiment_B/128/csv_full'),\n",
    "#              test_paths=T_B,\n",
    "#              test_labels=L_T_B,\n",
    "#              mode='full')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tfrecords for n-fold CV datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_tfrecords(csv_path, records_save_path, mode='CV'):\n",
    "    \n",
    "    os.makedirs(records_save_path, exist_ok=True)\n",
    "    train_csv_path = os.path.join(csv_path, \"training.csv\")\n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "    train_D = list(zip(train_paths, train_labels))\n",
    "    \n",
    "    random.shuffle(train_D)\n",
    "    train_write_path = os.path.join(records_save_path, 'data-train_shard-{shard:03d}.tfrec')\n",
    "    \n",
    "    nobrainer.tfrecord.write(\n",
    "        features_labels=train_D,\n",
    "        filename_template=train_write_path,\n",
    "        examples_per_shard=3)\n",
    "    \n",
    "    if mode =='CV':\n",
    "        vt_csv_path = os.path.join(csv_path, \"validation.csv\")\n",
    "        namefill = 'valid'\n",
    "    else:\n",
    "        vt_csv_path = os.path.join(csv_path, \"testing.csv\")\n",
    "        namefill = 'test'\n",
    "        \n",
    "    vt_paths = pd.read_csv(vt_csv_path)[\"X\"].values\n",
    "    vt_labels = pd.read_csv(vt_csv_path)[\"Y\"].values\n",
    "    vt_D = list(zip(vt_paths, vt_labels))\n",
    "    random.shuffle(vt_D)\n",
    "    vt_write_path = os.path.join(records_save_path, 'data-{}_shard-{shard:03d}.tfrec'.format(namefill))\n",
    "\n",
    "    nobrainer.tfrecord.write(\n",
    "        features_labels=vt_D,\n",
    "        filename_template=vt_write_path,\n",
    "        examples_per_shard=1)\n",
    "        \n",
    "\n",
    "ROOTDIR = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments'\n",
    "\n",
    "# Cross-Validation \n",
    "# SPLITS = 15\n",
    "# for fold in range(1, SPLITS+1):\n",
    "#     print(\"FOLD: \", fold)\n",
    "#     csv_path = os.path.join(\n",
    "#         ROOTDIR, \"experiment_B/128/csv_F15/train_test_fold_{}/csv/\".format(fold)\n",
    "#     )\n",
    "    \n",
    "#     tf_records_dir = os.path.join(\n",
    "#         ROOTDIR, \"experiment_B/128/tfrecords_F15/tfrecords_fold_{}/\".format(fold)\n",
    "#     )\n",
    "#     generate_tfrecords(csv_path, tf_records_dir)\n",
    "\n",
    "\n",
    "# Test (full dataset)\n",
    "# experiment_A\n",
    "# csv_path = os.path.join(ROOT_DIR, \"experiment_A/128/csv_full\")\n",
    "# tf_records_dir = os.path.join(ROOT_DIR, \"experiment_A/128/tfrecords_full\")\n",
    "# generate_tfrecords(csv_path, tf_records_dir, mode='test')\n",
    "\n",
    "# experiment_B\n",
    "# csv_path = os.path.join(ROOT_DIR, \"experiment_B/128/csv_full\")\n",
    "# tf_records_dir = os.path.join(ROOT_DIR, \"experiment_B/128/tfrecords_full\")\n",
    "# generate_tfrecords(csv_path, tf_records_dir, mode='test')\n",
    "\n",
    "## Main held-out Test Dataset\n",
    "csv_path = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi/csv/testing.csv'\n",
    "records_save_path = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi/tfrecords_new'\n",
    "paths = pd.read_csv(csv_path)[\"X\"].values\n",
    "labels = pd.read_csv(csv_path)[\"Y\"].values\n",
    "\n",
    "vt_D = list(zip(paths, labels))\n",
    "random.shuffle(vt_D)\n",
    "\n",
    "write_path = os.path.join(records_save_path, 'data-test_shard-{shard:03d}.tfrec')\n",
    "\n",
    "nobrainer.tfrecord.write(\n",
    "    features_labels=vt_D,\n",
    "    filename_template=write_path,\n",
    "    examples_per_shard=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
