{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import nobrainer\n",
    "from nobrainer.io import _is_gzipped\n",
    "from nobrainer.volume import to_blocks\n",
    "\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    \n",
    "# function to apply augmentations to tf dataset\n",
    "def apply_augmentations(features, labels):\n",
    "\n",
    "    \"\"\" Apply <TYPE_OF> augmentation to the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "#     iaa.SomeOf(\n",
    "#             (0, 3),\n",
    "#             [\n",
    "#                 iaa.Fliplr(0.5),\n",
    "#                 iaa.Flipud(0.5),\n",
    "#                 iaa.Noop(),\n",
    "#                 iaa.OneOf(\n",
    "#                     [\n",
    "#                         iaa.Affine(rotate=90),\n",
    "#                         iaa.Affine(rotate=180),\n",
    "#                         iaa.Affine(rotate=270),\n",
    "#                     ]\n",
    "#                 ),\n",
    "#                 # iaa.GaussianBlur(sigma=(0.0, 0.2)),\n",
    "#             ],\n",
    "#         )\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern,\n",
    "                n_classes,\n",
    "                batch_size,\n",
    "                volume_shape,\n",
    "                plane,\n",
    "                block_shape=None,\n",
    "                n_epochs=None,\n",
    "                mapping=None,\n",
    "                augment=False,\n",
    "                shuffle_buffer_size=None,\n",
    "                num_parallel_calls=AUTOTUNE):\n",
    "    \n",
    "    \"\"\" Returns tf.data.Dataset after preprocessing from \n",
    "    tfrecords for training and validation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_pattern:\n",
    "    \n",
    "    n_classes:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if not files:\n",
    "        raise ValueError(\"no files found for pattern '{}'\".format(file_pattern))\n",
    "    \n",
    "    \n",
    "    compressed = _is_gzipped(files[0])\n",
    "    shuffle = bool(shuffle_buffer_size)\n",
    "    \n",
    "    \n",
    "    ds = nobrainer.dataset.tfrecord_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        volume_shape=volume_shape,\n",
    "        shuffle=shuffle,\n",
    "        scalar_label=True,\n",
    "        compressed=compressed,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "    )\n",
    "    \n",
    "    if augment:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: tf.cond(\n",
    "                tf.random.uniform((1,)) > 0.5,\n",
    "                    true_fn=lambda: apply_augmentations(x, y),\n",
    "                    false_fn=lambda: (x, y),\n",
    "            ),\n",
    "            num_parallel_calls=num_parallel_calls,\n",
    "        )\n",
    "    \n",
    "    def _ss(x, y):\n",
    "        x, y = structural_slice(x, y, plane)\n",
    "        return (x, y)\n",
    "    \n",
    "    ds = ds.map(_ss, num_parallel_calls)\n",
    "    \n",
    "    \n",
    "#     def _f(x, y):\n",
    "#         x = to_blocks(x, block_shape)\n",
    "#         n_blocks = x.shape[0]\n",
    "#         y = tf.repeat(y, n_blocks)\n",
    "#         return (x, y)\n",
    "#     ds = ds.map(_f, num_parallel_calls=num_parallel_calls)\n",
    "    \n",
    "    # This step is necessary because it reduces the extra dimension.\n",
    "    ds = ds.unbatch()\n",
    "    \n",
    "    # add a single dimension at the end\n",
    "    ds = ds.map(lambda x, y: (tf.expand_dims(x, -1), y))\n",
    "    \n",
    "    ds = ds.prefetch(buffer_size=batch_size)\n",
    "    \n",
    "    if batch_size is not None:\n",
    "        ds = ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        \n",
    "    if shuffle_buffer_size:\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    \n",
    "    # Repeat the dataset n_epochs times\n",
    "    ds = ds.repeat(n_epochs)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "def structural_slice(x, y, plane):\n",
    "    \n",
    "    \"\"\" Transpose dataset based on the plane\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "    \n",
    "    y:\n",
    "    \n",
    "    plane:\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    options = ['axial', 'coronal', 'sagittal']\n",
    "    \n",
    "    x = tf.convert_to_tensor(x)\n",
    "    volume_shape = np.array(x.shape)\n",
    "    \n",
    "    \n",
    "    if isinstance(plane, str) and plane in options:\n",
    "        if plane == 'axial':\n",
    "            x = x\n",
    "            y = tf.repeat(y, volume_shape[0])\n",
    "        \n",
    "        if plane == 'coronal':\n",
    "            x = tf.transpose(x, perm=[1,0,2])\n",
    "            y = tf.repeat(y, volume_shape[1])\n",
    "            \n",
    "        if plane == 'sagittal':\n",
    "            x = tf.transpose(x, perm=[2,0,1])\n",
    "            y = tf.repeat(y, volume_shape[2])\n",
    "        return x, y\n",
    "    else:\n",
    "        raise ValueError(\"expected plane to be one of ['axial', 'coronal', 'sagittal']\")\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    n_classes=2\n",
    "    global_batch_size=4\n",
    "    volume_shape=(64,64,64)\n",
    "    dataset_train_axial = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "                            n_classes=n_classes,\n",
    "                            batch_size=global_batch_size,\n",
    "                            volume_shape=volume_shape,\n",
    "                            plane='axial',\n",
    "                            shuffle_buffer_size=3)\n",
    "    \n",
    "    print(dataset_train_axial)\n",
    "# dataset_train_coronal = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "#                             n_classes=n_classes,\n",
    "#                             batch_size=global_batch_size,\n",
    "#                             volume_shape=volume_shape,\n",
    "#                             block_shape=block_shape,\n",
    "#                             plane='coronal',\n",
    "#                             shuffle_buffer_size=3)\n",
    "\n",
    "# dataset_train_sagittal = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "#                             n_classes=n_classes,\n",
    "#                             batch_size=global_batch_size,\n",
    "#                             volume_shape=volume_shape,\n",
    "#                             block_shape=block_shape,\n",
    "#                             plane='sagittal',\n",
    "#                             shuffle_buffer_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import nobrainer\n",
    "from nobrainer import dataset, volume\n",
    "\n",
    "dir_path = os.path.abspath(\"csv/faced_defaced/train_test_fold_1/csv/\")\n",
    "csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "# print(dir_path, csv_path)\n",
    "\n",
    "\n",
    "labels = pd.read_csv(csv_path)[\"Y\"].values\n",
    "paths = pd.read_csv(csv_path)[\"X\"].values\n",
    "\n",
    "\n",
    "# print(labels)\n",
    "\n",
    "n_classes=2\n",
    "volume_shape = (256, 256, 256)\n",
    "block_shape = (128, 128, 128)\n",
    "\n",
    "\n",
    "training_paths = zip(paths, labels)\n",
    "\n",
    "print(training_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import binascii\n",
    "from helpers.utils import load_vol, save_vol\n",
    "from preprocessing.normalization import standardize_volume, normalize_volume\n",
    "from preprocessing.conform import conform_data\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "\n",
    "\n",
    "orig_data_face = \"/work/01329/poldrack/data/mriqc-net/data/face/T1w\"\n",
    "orig_data_deface = \"/work/01329/poldrack/data/mriqc-net/data/defaced\"\n",
    "\n",
    "save_data_face = \"/work/06850/sbansal6/maverick2/mriqc-shared/face\"\n",
    "save_data_deface = \"/work/06850/sbansal6/maverick2/mriqc-shared/deface\"\n",
    "\n",
    "os.makedirs(save_data_face, exist_ok=True)\n",
    "os.makedirs(save_data_deface, exist_ok=True)\n",
    "\n",
    "\n",
    "conform_size = (64, 64, 64)\n",
    "\n",
    "def is_gz_file(filepath):\n",
    "    if os.path.splitext(filepath)[1] == '.gz':\n",
    "        with open(filepath, 'rb') as test_f:\n",
    "            return binascii.hexlify(test_f.read(2)) == b'1f8b'\n",
    "    return False\n",
    "\n",
    "def preprocess(pth, conform_size, save_data_path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    filename = pth.split(\"/\")[-1]\n",
    "    print('Confirmation step')\n",
    "    volume = conform_data(pth, out_size=conform_size)\n",
    "    \n",
    "    print(\"Normalize/Standardize step\")\n",
    "    volume = normalize_volume(standardize_volume(volume))\n",
    "    save_path = os.path.join(save_data_path, filename)\n",
    "\n",
    "    newaffine = np.eye(4)\n",
    "    newaffine[:3, 3] = -0.5 * (np.array(conform_size) - 1)\n",
    "    nii = nb.Nifti1Image(volume, newaffine, None)\n",
    "    \n",
    "    print(\"Save new affine\")\n",
    "    nii.to_filename(save_path)\n",
    "    return save_path\n",
    "\n",
    "        \n",
    "# print(list.count(deface_orig))\n",
    "\n",
    "#     if tempfile not in deface_C:\n",
    "#         print(tempfile)\n",
    "#         deface_NC.append(tempfile)\n",
    "        \n",
    "#     if not is_gz_file(path):\n",
    "#         tempname = path.split(\"/\")[-1]\n",
    "#         rename_file = os.path.splitext(tempname)[0]\n",
    "#         dst = os.path.join(save_data_face, rename_file)\n",
    "#         print(dst)\n",
    "#         subprocess.call(['cp', path, dst])\n",
    "#         print(preprocess(dst, conform_size))\n",
    "#     else:\n",
    "#         print(preprocess(path, conform_size))\n",
    "\n",
    "\n",
    "for path in glob(orig_data_deface + \"/*/*.nii*\"):\n",
    "#     try:\n",
    "    print(\"Orig Path: \", path)\n",
    "    if not is_gz_file(path) and os.path.splitext(path)[1] == '.gz':\n",
    "        tempname = path.split(\"/\")[-1]\n",
    "        ds = path.split(\"/\")[-2]\n",
    "        rename_file = os.path.splitext(tempname)[0]\n",
    "        dst = os.path.join(save_data_deface, rename_file)\n",
    "        print(dst)\n",
    "        subprocess.call(['cp', path, dst])\n",
    "        ds_save_path = os.path.join(save_data_deface, ds)\n",
    "        if not os.path.exists(ds_save_path):\n",
    "            os.makedirs(ds_save_path)   \n",
    "        print(preprocess(dst, conform_size, save_data_path=ds_save_path)) \n",
    "    else:\n",
    "        ds = path.split(\"/\")[-2]\n",
    "        ds_save_path = os.path.join(save_data_deface, ds)\n",
    "        if not os.path.exists(ds_save_path):\n",
    "            os.makedirs(ds_save_path)\n",
    "        print(preprocess(path, conform_size, save_data_path=ds_save_path))\n",
    "#     except:\n",
    "#         print(\"Preprocessing incomplete. Exception occurred.\")\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nb\n",
    "\n",
    "in_file = '/work/06850/sbansal6/maverick2/mriqc-shared/deface/ds001912_anat/sub-01_ses-01_run-01_T1w.nii.gz'\n",
    "# dst_path = '/work/06850/sbansal6/maverick2/mriqc-shared/deface'\n",
    "\n",
    "# print(is_gz_file(in_file))\n",
    "\n",
    "# if not is_gz_file(in_file):\n",
    "#     filename = in_file.split(\"/\")[-1]\n",
    "#     print(filename)\n",
    "#     rename_file = os.path.splitext(filename)[0]\n",
    "#     dst = os.path.join(dst_path, rename_file)\n",
    "    \n",
    "#     subprocess.call(['cp', in_file, dst])\n",
    "    \n",
    "    \n",
    "# if isinstance(in_file, (str, Path)):\n",
    "i_file = nb.load(in_file)\n",
    "print(i_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_C = []\n",
    "face_O = []\n",
    "\n",
    "for path in glob(save_data_face + \"/*/*.nii*\"):\n",
    "    tempname = path.split(\"/\")[-1]\n",
    "    ds = path.split(\"/\")[-2]\n",
    "    face_C.append(ds + '/' + tempname)\n",
    "\n",
    "print(len(face_C))\n",
    "# print(face_C)\n",
    "\n",
    "\n",
    "for path in glob(orig_data_face + \"/*/*.nii*\"):\n",
    "    tempname = path.split(\"/\")[-1]\n",
    "    ds = path.split(\"/\")[-2]\n",
    "    face_O.append(ds + '/' + tempname)\n",
    "\n",
    "print(len(face_O))\n",
    "# print(face_O)\n",
    "\n",
    "count = 0\n",
    "for f in face_O:\n",
    "    exists = False\n",
    "    for fc in face_C:\n",
    "        if fc in f:\n",
    "            exists = True\n",
    "    if not exists:\n",
    "        count += 1\n",
    "        print(f)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import binascii\n",
    "from helpers.utils import load_vol, save_vol\n",
    "from preprocessing.normalization import standardize_volume, normalize_volume\n",
    "from preprocessing.conform import conform_data\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "\n",
    "face_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/face\"\n",
    "deface_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/deface\"\n",
    "\n",
    "paths = []\n",
    "labels = []\n",
    "\n",
    "for path in glob(deface_path + \"/*/*.nii*\"):\n",
    "    paths.append(path)\n",
    "    labels.append(0)\n",
    "\n",
    "for path in glob(face_path + \"/*/*.nii*\"):\n",
    "    paths.append(path)\n",
    "    labels.append(1)\n",
    "    \n",
    "print(len(paths))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "save_path = \"./csv/\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"X\"] = paths\n",
    "df[\"Y\"] = labels\n",
    "df.to_csv(os.path.join(save_path, \"all.csv\"))\n",
    "\n",
    "SPLITS = 10\n",
    "skf = StratifiedKFold(n_splits=SPLITS)\n",
    "fold_no = 1\n",
    "\n",
    "for train_index, test_index in skf.split(paths, labels):\n",
    "    out_path = save_path + \"/train_test_fold_{}/csv/\".format(fold_no)\n",
    "\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "    image_train, image_test = (\n",
    "        itemgetter(*train_index)(paths),\n",
    "        itemgetter(*test_index)(paths),\n",
    "    )\n",
    "    label_train, label_test = (\n",
    "        itemgetter(*train_index)(labels),\n",
    "        itemgetter(*test_index)(labels),\n",
    "    )\n",
    "\n",
    "    # image_train = [os.path.join(data_path, 'sub-' + str(pth) + '_T1w.nii.gz') for pth in image_train]\n",
    "    train_data = {\"X\": image_train, \"Y\": label_train}\n",
    "    df_train = pd.DataFrame(train_data)\n",
    "    df_train.to_csv(os.path.join(out_path, \"training.csv\"), index=False)\n",
    "\n",
    "    # image_test = [os.path.join(data_path, 'sub-' + str(pth) + '_T1w.nii.gz') for pth in image_test]\n",
    "    validation_data = {\"X\": image_test, \"Y\": label_test}\n",
    "    df_validation = pd.DataFrame(validation_data)\n",
    "    df_validation.to_csv(os.path.join(out_path, \"validation.csv\"), index=False)\n",
    "\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "\n",
    "for fold in range(1, SPLITS+1):\n",
    "    \n",
    "    dir_path = \"./csv/train_test_fold_{}/csv/\".format(fold)\n",
    "    \n",
    "    tf_records_dir = \"./tfrecords/tfrecords_fold_{}/\".format(fold)\n",
    "    os.makedirs(tf_records_dir, exist_ok=True)\n",
    "    \n",
    "    train_csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "    valid_csv_path = os.path.join(dir_path, \"validation.csv\")\n",
    "    \n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "    train_D = list(zip(train_paths, train_labels))\n",
    "    random.shuffle(train_D)\n",
    "#     print(train_D[0])\n",
    "    \n",
    "    valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "    valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "    valid_D = list(zip(valid_paths, valid_labels))\n",
    "    random.shuffle(valid_D)\n",
    "    \n",
    "    train_write_path = os.path.join(tf_records_dir, 'data-train_shard-{shard:03d}.tfrec')\n",
    "    valid_write_path = os.path.join(tf_records_dir, 'data-valid_shard-{shard:03d}.tfrec')\n",
    "    \n",
    "    nobrainer.tfrecord.write(\n",
    "        features_labels=train_D,\n",
    "        filename_template=train_write_path,\n",
    "        examples_per_shard=3)\n",
    "    \n",
    "    nobrainer.tfrecord.write(\n",
    "        features_labels=valid_D,\n",
    "        filename_template=valid_write_path,\n",
    "        examples_per_shard=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nobrainer import dataset, volume\n",
    "from nobrainer.io import _is_gzipped\n",
    "from nobrainer.volume import to_blocks\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "n_classes = 1\n",
    "batch_size = 4\n",
    "volume_shape = (64, 64, 64)\n",
    "block_shape = (32, 32, 32)\n",
    "n_epochs = 20\n",
    "num_parallel_calls = 4\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "BATCH_SIZE_PER_REPLICA = batch_size\n",
    "global_batch_size = BATCH_SIZE_PER_REPLICA*strategy.num_replicas_in_sync\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# function to apply augmentations to tf dataset\n",
    "def apply_augmentations(x, y):\n",
    "    \n",
    "#     iaa.SomeOf(\n",
    "#             (0, 3),\n",
    "#             [\n",
    "#                 iaa.Fliplr(0.5),\n",
    "#                 iaa.Flipud(0.5),\n",
    "#                 iaa.Noop(),\n",
    "#                 iaa.OneOf(\n",
    "#                     [\n",
    "#                         iaa.Affine(rotate=90),\n",
    "#                         iaa.Affine(rotate=180),\n",
    "#                         iaa.Affine(rotate=270),\n",
    "#                     ]\n",
    "#                 ),\n",
    "#                 # iaa.GaussianBlur(sigma=(0.0, 0.2)),\n",
    "#             ],\n",
    "#         )\n",
    "    \n",
    "    return\n",
    "    \n",
    "def structural_slice(x, y, plane):\n",
    "    \n",
    "    options = ['axial', 'coronal', 'sagittal']\n",
    "    \n",
    "    x = tf.convert_to_tensor(x)\n",
    "    volume_shape = np.array(x.shape)\n",
    "    \n",
    "    \n",
    "    if isinstance(plane, str) and plane in options:\n",
    "        if plane == 'axial':\n",
    "            x = x\n",
    "            y = tf.repeat(y, volume_shape[0])\n",
    "        \n",
    "        if plane == 'coronal':\n",
    "            x = tf.transpose(x, perm=[1,0,2])\n",
    "            y = tf.repeat(y, volume_shape[1])\n",
    "            \n",
    "        if plane == 'sagittal':\n",
    "            x = tf.transpose(x, perm=[2,0,1])\n",
    "            y = tf.repeat(y, volume_shape[2])\n",
    "            \n",
    "        return x, y\n",
    "    else:\n",
    "        raise ValueError(\"expected plane to be one of ['axial', 'coronal', 'sagittal']\")\n",
    "    \n",
    "    \n",
    "\n",
    "def get_dataset(file_pattern,\n",
    "                n_classes,\n",
    "                batch_size,\n",
    "                volume_shape,\n",
    "                plane,\n",
    "                block_shape=None,\n",
    "                n_epochs=None,\n",
    "                mapping=None,\n",
    "                augment=False,\n",
    "                shuffle_buffer_size=None,\n",
    "                num_parallel_calls=AUTOTUNE):\n",
    "    \n",
    "    files = glob.glob(file_pattern)\n",
    "    \n",
    "    if not files:\n",
    "        raise ValueError(\"no files found for pattern '{}'\".format(file_pattern))\n",
    "    \n",
    "    compressed = _is_gzipped(files[0])\n",
    "    shuffle = bool(shuffle_buffer_size)\n",
    "    \n",
    "    \n",
    "    ds = dataset.tfrecord_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        volume_shape=volume_shape,\n",
    "        shuffle=shuffle,\n",
    "        scalar_label=True,\n",
    "        compressed=compressed,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "    )\n",
    "    \n",
    "    if augment:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: tf.cond(\n",
    "                tf.random.uniform((1,)) > 0.5,\n",
    "                    true_fn=lambda: apply_augmentations(x, y),\n",
    "                    false_fn=lambda: (x, y),\n",
    "            ),\n",
    "            num_parallel_calls=num_parallel_calls,\n",
    "        )\n",
    "    \n",
    "    def _sp(x, y):\n",
    "        x, y = structural_slice(x, y, plane)\n",
    "        return (x, y)\n",
    "    \n",
    "    ds = ds.map(_sp, num_parallel_calls)\n",
    "    \n",
    "#     print(ds)\n",
    "#     temp = list(ds.as_numpy_iterator())\n",
    "\n",
    "#     def _f(x, y):\n",
    "#         x = to_blocks(x, block_shape)\n",
    "#         n_blocks = x.shape[0]\n",
    "#         y = tf.repeat(y, n_blocks)\n",
    "#         return (x, y)\n",
    "#     ds = ds.map(_f, num_parallel_calls=num_parallel_calls)\n",
    "    \n",
    "    # This step is necessary because separating into blocks adds a dimension.\n",
    "    ds = ds.unbatch()\n",
    "    \n",
    "    # add a single dimension at the end\n",
    "    ds = ds.map(lambda x, y: (tf.expand_dims(x, -1), y))\n",
    "    \n",
    "    ds = ds.prefetch(buffer_size=batch_size)\n",
    "    \n",
    "    if batch_size is not None:\n",
    "        ds = ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        \n",
    "#     ds = ds.repeat(n_epochs)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "# dataset_train_axial = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "#                             n_classes=n_classes,\n",
    "#                             batch_size=global_batch_size,\n",
    "#                             volume_shape=volume_shape,\n",
    "#                             block_shape=block_shape,\n",
    "#                             plane='axial',\n",
    "#                             shuffle_buffer_size=3)\n",
    "\n",
    "# dataset_train_coronal = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "#                             n_classes=n_classes,\n",
    "#                             batch_size=global_batch_size,\n",
    "#                             volume_shape=volume_shape,\n",
    "#                             block_shape=block_shape,\n",
    "#                             plane='coronal',\n",
    "#                             shuffle_buffer_size=3)\n",
    "\n",
    "# dataset_train_sagittal = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "#                             n_classes=n_classes,\n",
    "#                             batch_size=global_batch_size,\n",
    "#                             volume_shape=volume_shape,\n",
    "#                             block_shape=block_shape,\n",
    "#                             plane='sagittal',\n",
    "#                             shuffle_buffer_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train_axial)\n",
    "print(dataset_train_coronal)\n",
    "print(dataset_train_sagittal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "tpaths = glob.glob('tfrecords/tfrecords_fold_1/data-train_*')\n",
    "vpaths = glob.glob('tfrecords/tfrecords_fold_1/data-valid_*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "from models import modelN\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "def train(\n",
    "    image_size=(64, 64),\n",
    "    dropout=0.4,\n",
    "    batch_size=8,\n",
    "    n_classes=2,\n",
    "    n_epochs=30\n",
    "):\n",
    "    \n",
    "    planes = ['axial', 'coronal', 'sagittal']\n",
    "    \n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    BATCH_SIZE_PER_REPLICA = batch_size\n",
    "    global_batch_size = BATCH_SIZE_PER_REPLICA*strategy.num_replicas_in_sync\n",
    "    \n",
    "    model_save_path = './model_save_dir'\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "        \n",
    "    cp_save_path = os.path.join(model_save_path, 'weights')\n",
    "    \n",
    "    logdir_path = os.path.join(model_save_path, \"tb_logs\")\n",
    "    if not os.path.exists(logdir_path):\n",
    "        os.makedirs(logdir_path)\n",
    "        \n",
    "        \n",
    "    for plane in planes:\n",
    "        \n",
    "        logdir = os.path.join(logdir_path, plane)\n",
    "        os.makedirs(logdir, exist_ok=True)\n",
    "        \n",
    "        tbCallback = TensorBoard(\n",
    "            log_dir=logdir,\n",
    "            histogram_freq=0,\n",
    "            write_graph=True,\n",
    "            write_images=False,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        os.makedirs(os.path.join(cp_save_path, plane), exist_ok=True)\n",
    "        \n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            os.path.join(cp_save_path, plane, \"best-wts.h5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "        \n",
    "        \n",
    "        with strategy.scope():\n",
    "            \n",
    "            lr = 1e-4\n",
    "            model = modelN.Submodel(\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                name=plane,\n",
    "                include_top=True,\n",
    "                weights=None,\n",
    "            )\n",
    "            \n",
    "            print(\"Submodel: \", plane)\n",
    "            print(model.summary())\n",
    "    \n",
    "            METRICS = [\n",
    "                metrics.TruePositives(name='tp'),\n",
    "                metrics.FalsePositives(name='fp'),\n",
    "                metrics.TrueNegatives(name='tn'),\n",
    "                metrics.FalseNegatives(name='fn'),\n",
    "                metrics.BinaryAccuracy(name='accuracy'),\n",
    "                metrics.Precision(name='precision'),\n",
    "                metrics.Recall(name='recall'),\n",
    "                metrics.AUC(name='auc'),\n",
    "            ]\n",
    "\n",
    "    \n",
    "            model.compile(\n",
    "                loss=tf.keras.losses.binary_crossentropy,\n",
    "                optimizer=\"adam\",\n",
    "                metrics=METRICS\n",
    "            )\n",
    "        \n",
    "        \n",
    "        print(\"GLOBAL BATCH SIZE: \", global_batch_size)\n",
    "        dataset_train = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "                                    n_classes=n_classes,\n",
    "                                    batch_size=global_batch_size,\n",
    "                                    volume_shape=volume_shape,\n",
    "                                    block_shape=block_shape,\n",
    "                                    plane=plane,\n",
    "                                    shuffle_buffer_size=global_batch_size)\n",
    "        \n",
    "        dataset_valid = get_dataset(\"tfrecords/tfrecords_fold_1/data-valid_*\",\n",
    "                                    n_classes=n_classes,\n",
    "                                    batch_size=global_batch_size,\n",
    "                                    volume_shape=volume_shape,\n",
    "                                    block_shape=block_shape,\n",
    "                                    plane=plane,\n",
    "                                    shuffle_buffer_size=global_batch_size)\n",
    "        \n",
    "        steps_per_epoch = dataset.get_steps_per_epoch(\n",
    "            n_volumes = len(tpaths),\n",
    "            volume_shape = volume_shape,\n",
    "            block_shape = block_shape,\n",
    "            batch_size = global_batch_size)\n",
    "\n",
    "        validation_steps = dataset.get_steps_per_epoch(\n",
    "            n_volumes = len(vpaths),\n",
    "            volume_shape = volume_shape,\n",
    "            block_shape = block_shape,\n",
    "            batch_size = global_batch_size)\n",
    "\n",
    "        model.fit(\n",
    "            dataset_train,\n",
    "            epochs=n_epochs,\n",
    "            steps_per_epoch = steps_per_epoch,\n",
    "            validation_data = dataset_valid,\n",
    "            validation_steps = validation_steps,\n",
    "            callbacks=[tbCallback,\n",
    "                       model_checkpoint]\n",
    "        )\n",
    "        \n",
    "        del model\n",
    "        K.clear_session()\n",
    "    \n",
    "    lr = 5e-5\n",
    "    model = modelN.CombinedClassifier(input_shape=image_size, dropout=dropout, wts_root=cp_save_path)\n",
    "    \n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
