{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.path)\n",
    "print(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python -W ignore train.py -jn split-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import datetime\n",
    "import numpy as np\n",
    "import random\n",
    "import timeit\n",
    "import argparse\n",
    "import getpass\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../defacing\")\n",
    "from training.training import trainer\n",
    "from helpers.utils import get_available_gpus\n",
    "from distutils.dir_util import copy_tree\n",
    "import tensorflow as tf\n",
    "\n",
    "list_gpu = get_available_gpus()\n",
    "n_gpu = len(list_gpu)\n",
    "print(\"Available GPUs: \", list_gpu)\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Training DefacingNet\")\n",
    "parser.add_argument(\"--GPU\", default=\"0\", type=str, help=\"which GPU to use\")\n",
    "parser.add_argument(\n",
    "    \"-jn\",\n",
    "    \"--job_name\",\n",
    "    required=True,\n",
    "    type=str,\n",
    "    help=\"The job name is required. All the training will be saved here.\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "t0 = timeit.default_timer()\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.GPU\n",
    "print(\n",
    "    \"GPU Availability: \",\n",
    "    tf.test.is_gpu_available(cuda_only=True, min_cuda_compute_capability=None),\n",
    ")\n",
    "\n",
    "\n",
    "Kfolds = 10\n",
    "nfolds = list(range(1, Kfolds + 1))\n",
    "\n",
    "for fold in nfolds:\n",
    "    root_dir = \"./Logs/\" + args.job_name + \"/train_test_fold_{}\".format(fold)\n",
    "    dir_path = \"./Logs/\" + args.job_name + \"/train_test_fold_{}/csv/\".format(fold)\n",
    "\n",
    "    # currently a very hacky way of doing this -- will need to fix later\n",
    "    from_dir = os.path.abspath(\n",
    "        \"./csv/faced_defaced/train_test_fold_{}/csv/\".format(fold)\n",
    "    )\n",
    "    to_dir = dir_path\n",
    "    copy_tree(from_dir, to_dir)\n",
    "\n",
    "    train_csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "    valid_csv_path = os.path.join(dir_path, \"validation.csv\")\n",
    "\n",
    "    # Model Path\n",
    "    model_path = root_dir + \"/\" + args.job_name\n",
    "\n",
    "    # create a path to where the model will be saved\n",
    "    if not os.path.exists(root_dir):\n",
    "        os.makedirs(root_dir)\n",
    "\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    # basic job info text file to identify jobs\n",
    "    basic_job_info = os.path.join(os.path.abspath(root_dir), \"job_info.txt\")\n",
    "    with open(basic_job_info, \"w\") as f:\n",
    "        f.write(\"Jobname: %s\\n\" % args.job_name)\n",
    "        f.write(\"Created on: %s\\n\" % str(datetime.datetime.now()))\n",
    "        f.write(\"Created by: %s\\n\" % str(getpass.getuser()))\n",
    "        f.write(\"Model store path: %s\\n\" % os.path.abspath(model_path))\n",
    "        f.write(\n",
    "            \"GPU Availability: %s\\n\"\n",
    "            % str(\n",
    "                tf.test.is_gpu_available(\n",
    "                    cuda_only=True, min_cuda_compute_capability=None\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        f.write(\"Available GPUs: %s\\n\" % (\",\".join(list_gpu)))\n",
    "\n",
    "    train = trainer(\n",
    "        train_csv_path,\n",
    "        valid_csv_path,\n",
    "        basic_job_info,\n",
    "        model_path,\n",
    "        image_size=32,\n",
    "        batch_size=8,\n",
    "        initial_epoch=0,\n",
    "        nepochs=25,\n",
    "        dropout=0.4,\n",
    "        nclasses=2,\n",
    "        nchannels=1,\n",
    "        gpus=4,\n",
    "    )\n",
    "    train.train()\n",
    "\n",
    "    elapsed = timeit.default_timer() - t0\n",
    "    print(\"Time: {:.3f} min\".format(elapsed / 60))\n",
    "    del train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "import os, sys\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import imgaug\n",
    "from imgaug import augmenters as iaa\n",
    "import nibabel as nib\n",
    "import SimpleITK as sitk\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "\n",
    "sys.path.append('..')\n",
    "from helpers.utils import *\n",
    "from skimage.restoration import denoise_wavelet\n",
    "\n",
    "\n",
    "class DataGeneratoronFly(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "\t\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_csv,\n",
    "        nclasses=2,\n",
    "        image_size=128,\n",
    "        batch_size=32,\n",
    "        nchannels=1,\n",
    "        mode=\"Train\",\n",
    "        name=None,\n",
    "        samples_per_epoch=None,\n",
    "        transform=None,\n",
    "    ):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.nchannels = nchannels\n",
    "        self.nclasses = nclasses\n",
    "        self.transform = transform\n",
    "        self.name = name\n",
    "        self.paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        labels = pd.read_csv(data_csv)[\"Y\"].values\n",
    "        paths = pd.read_csv(data_csv)[\"X\"].values\n",
    "\n",
    "        index = np.arange(len(paths))\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        labels = labels[index]\n",
    "        paths = paths[index]\n",
    "        \n",
    "        if mode.lower() in [\"train\", \"valid\", \"test\"]:\n",
    "            self.mode = mode.lower()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"mode should be one among ['Train', 'Valid', 'Test'], given argument: {}\".format(\n",
    "                    mode\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        if mode == \"Train\":\n",
    "            minarr = [np.sum(labels == i) for i in range(nclasses)]\n",
    "            mincount = np.min(minarr)\n",
    "            for i in range(nclasses):\n",
    "                self.paths.extend(paths[labels == i][:mincount])\n",
    "                self.labels.extend(labels[labels == i][:mincount])\n",
    "\n",
    "            self.paths = np.array(self.paths)\n",
    "            self.labels = np.array(self.labels)\n",
    "\n",
    "        elif mode == \"Valid\":\n",
    "            self.paths = np.array(paths)\n",
    "            self.labels = np.array(labels)\n",
    "\n",
    "        assert len(np.unique(self.labels)) == nclasses\n",
    "        self.len_arr = [sum(self.labels == arr) for arr in np.unique(self.labels)]\n",
    "\n",
    "        index = np.arange(len(self.paths))\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        self.paths = self.paths[index]\n",
    "        self.labels = self.labels[index]\n",
    "        \n",
    "        print(\n",
    "            \"============== paths: {}, labels: {} ================\".format(\n",
    "                len(self.paths), len(self.labels)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if samples_per_epoch is None:\n",
    "            if mode == \"Train\":\n",
    "                self.samples_per_epoch = 4 * len(self.paths)\n",
    "            else:\n",
    "                self.samples_per_epoch = len(self.paths)\n",
    "        else:\n",
    "            self.samples_per_epoch = samples_per_epoch\n",
    "            \n",
    "        print(\n",
    "            \"============== Samples/Epoch: {} ================\".format(\n",
    "                self.samples_per_epoch\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "            Denotes the number of batches per epoch\n",
    "        \"\"\"\n",
    "        return int(np.floor(self.samples_per_epoch / self.batch_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "        \n",
    "# #         print(\"getitem index: \", index)\n",
    "#         # Generate indexes of the batch\n",
    "#         X1, X2, X3, y = self.__data_generation(index)\n",
    "#         if self.name == \"combined\":\n",
    "#             return [X1, X2, X3], y\n",
    "#         elif self.name == \"axial\":\n",
    "# #             tf_dataset = tf.data.Dataset.from_tensor_slices((X1, y))\n",
    "#             return X1, y\n",
    "#         elif self.name == \"coronal\":\n",
    "#             return X2, y\n",
    "#         elif self.name == \"sagittal\":\n",
    "#             return X3, y\n",
    "\n",
    "    def _standardize_volume(self, volume, mask=None):\n",
    "        \"\"\"\n",
    "\t\t\tvolume: volume which needs to be normalized\n",
    "\t\t\tmask: brain mask, only required if you prefer not to\n",
    "\t\t\t\tconsider the effect of air in normalization\n",
    "\t\t\"\"\"\n",
    "        if mask != None:\n",
    "            volume = volume * mask\n",
    "\n",
    "        mean = np.mean(volume[volume != 0])\n",
    "        std = np.std(volume[volume != 0])\n",
    "\n",
    "        return (volume - mean) / std\n",
    "\n",
    "    def _normalize_volume(self, volume, mask=None, _type=\"MinMax\"):\n",
    "        \"\"\"\n",
    "\t\t\tvolume: volume which needs to be normalized\n",
    "\t\t\tmask: brain mask, only required if you prefer not to\n",
    "\t\t\t\tconsider the effect of air in normalization\n",
    "\t\t\t_type: {'Max', 'MinMax', 'Sum'}\n",
    "\t\t\"\"\"\n",
    "        if mask != None:\n",
    "            volume = mask * volume\n",
    "\n",
    "        min_vol = np.min(volume)\n",
    "        max_vol = np.max(volume)\n",
    "        sum_vol = np.sum(volume)\n",
    "\n",
    "        if _type == \"MinMax\":\n",
    "            return (volume - min_vol) / (max_vol - min_vol)\n",
    "        elif _type == \"Max\":\n",
    "            return volume / max_vol\n",
    "        elif _type == \"Sum\":\n",
    "            return volume / sum_vol\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid _type, allowed values are: {}\".format(\"Max, MinMax, Sum\")\n",
    "            )\n",
    "\n",
    "    def _augmentation(self, volume):\n",
    "        \"\"\"\n",
    "\t\t\tAugmenters that are safe to apply to masks\n",
    "\t\t\tSome, such as Affine, have settings that make them unsafe, so always\n",
    "\t\t\ttest your augmentation on masks\n",
    "\t\t\"\"\"\n",
    "        volume_shape = volume.shape\n",
    "        det = self.transform.to_deterministic()\n",
    "        volume = det.augment_image(volume)\n",
    "\n",
    "        assert volume.shape == volume_shape, \"Augmentation shouldn't change volume size\"\n",
    "        return volume\n",
    "\n",
    "    def _resizeVolume(self, volume):\n",
    "        \"\"\"\n",
    "\t\t\tresizes the original volume such that every patch is\n",
    "\t\t\t75% of original volume\n",
    "\n",
    "\t\t\tvolume: numpy 3d tensor\n",
    "\t\t\"\"\"\n",
    "        ratio = 1.0\n",
    "\n",
    "        orig_size = (\n",
    "            int(self.image_size / ratio),\n",
    "            int(self.image_size / ratio),\n",
    "            int(self.image_size / ratio),\n",
    "        )\n",
    "        resized_volume = resize_sitk(volume, orig_size)\n",
    "        return resized_volume\n",
    "\n",
    "    def _get_random_slices(self, volume):\n",
    "        \"\"\"\n",
    "\t\t\"\"\"\n",
    "        dimensions = volume.shape\n",
    "        img = np.zeros((dimensions[0], dimensions[1], 3))\n",
    "        x = np.random.randint(dimensions[0] // 4, 3 * dimensions[0] // 4)\n",
    "        z = np.random.randint(dimensions[1] // 4, 3 * dimensions[1] // 4)\n",
    "        y = np.random.randint(dimensions[2] // 4, 3 * dimensions[2] // 4)\n",
    "        slice_x = volume[x, :, :]\n",
    "        slice_y = volume[:, y, :]\n",
    "        slice_z = volume[:, :, z]\n",
    "\n",
    "        return slice_x[..., None], slice_y[..., None], slice_z[..., None]\n",
    "\n",
    "    def _center_align(self, volume):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return volume\n",
    "\n",
    "    def _axis_align(self, volume):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return volume\n",
    "\n",
    "    def __data_generation(self, index):\n",
    "        \"\"\"\n",
    "        balanced data loader\n",
    "        \"\"\"\n",
    "        X1, X2, X3 = [], [], []\n",
    "        Y = []\n",
    "        nclass_batch = self.batch_size // self.nclasses\n",
    "        \n",
    "#         print(\"Nclass batch: \", nclass_batch)\n",
    "        \n",
    "        for i in range(nclass_batch):\n",
    "            for ii in np.unique(self.labels):\n",
    "                # try:\n",
    "                pid_path = self.paths[self.labels == ii][\n",
    "                    int(index * nclass_batch + i) % self.len_arr[ii]\n",
    "                ]\n",
    "                label = ii  # np.eye(self.nclasses)[ii]\n",
    "\n",
    "                volume, affine, size = load_vol(pid_path)\n",
    "                volume = self._axis_align(volume)\n",
    "                volume = self._center_align(volume)\n",
    "                volume = self._resizeVolume(volume)\n",
    "                volume = self._standardize_volume(volume)\n",
    "                volume = self._normalize_volume(volume)\n",
    "\n",
    "                if (self.mode.lower() == \"train\") and self.transform:\n",
    "                    volume = self._augmentation(volume)\n",
    "\n",
    "                ax, sg, co = self._get_random_slices(volume)\n",
    "\n",
    "                if ax.shape == sg.shape == co.shape:\n",
    "                    X1.append(ax)\n",
    "                    X2.append(sg)\n",
    "                    X3.append(co)\n",
    "                    Y.append(label)\n",
    "                # except:\n",
    "                # \tcontinue\n",
    "\n",
    "        X1, X2, X3, Y = np.array(X1), np.array(X2), np.array(X3), np.array(Y)\n",
    "        \n",
    "        \n",
    "        index = np.arange(len(X1))\n",
    "        np.random.shuffle(index)\n",
    "        \n",
    "        X1, X2, X3, Y = X1[index], X2[index], X3[index], Y[index]\n",
    "        \n",
    "#         print(\"X1.shape: \", X1.shape)\n",
    "#         print(Y)\n",
    "        \n",
    "        return X1, X2, X3, Y\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dir_path = os.path.abspath(\"csv/faced_defaced/train_test_fold_1/csv/\")\n",
    "\n",
    "    csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "    \n",
    "    print(dir_path, csv_path)\n",
    "    \n",
    "    augmentation = iaa.SomeOf(\n",
    "        (0, 3),\n",
    "        [\n",
    "            iaa.Fliplr(0.5),\n",
    "            iaa.Flipud(0.5),\n",
    "            iaa.Noop(),\n",
    "            iaa.OneOf(\n",
    "                [iaa.Affine(rotate=90), iaa.Affine(rotate=180), iaa.Affine(rotate=270)]\n",
    "            ),\n",
    "            # iaa.GaussianBlur(sigma=(0.0, 0.2)),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Parameters\n",
    "    train_transform_params = {\n",
    "        \"image_size\": 128,\n",
    "        \"batch_size\": 32,\n",
    "        \"nclasses\": 2,\n",
    "        \"nchannels\": 1,\n",
    "        \"name\": \"axial\",\n",
    "        \"samples_per_epoch\": None,\n",
    "        \"transform\": augmentation,\n",
    "    }\n",
    "\n",
    "    valid_transform_params = {\n",
    "        \"image_size\": 128,\n",
    "        \"batch_size\": 32,\n",
    "        \"nclasses\": 2,\n",
    "        \"nchannels\": 1,\n",
    "        \"name\": \"axial\",\n",
    "        \"samples_per_epoch\": None,\n",
    "        \"transform\": None,\n",
    "    }\n",
    "\n",
    "    # Generators\n",
    "    training_generator = DataGeneratoronFly(data_csv=csv_path, **train_transform_params)\n",
    "    \n",
    "#     X, Y - (133*32, 128, 128, 1) (133*32)\n",
    "    \n",
    "    # print (training_generator.__len__())\n",
    "\n",
    "#     validation_generator = DataGeneratoronFly(data_csv=csv_path, **valid_transform_params)\n",
    "#     print(validation_generator.__len__())\n",
    "    \n",
    "#     print(training_generator[0][0].shape)\n",
    "#     print(training_generator[0][1].shape)\n",
    "\n",
    "    \n",
    "#     train_dataset = tf.data.Dataset.from_tensor_slices(training_generator)\n",
    "    \n",
    "#     for X, y in training_generator:\n",
    "#         print (X.shape, y.shape)\n",
    "#         print (y[:4])\n",
    "#         imshow(X[0,:,:,64, 0], X[1,:,:,64, 0], X[2,:,:,64, 0], X[3,:,:,64, 0])\n",
    "        \n",
    "        \n",
    "#     for ep in range(5):\n",
    "#         print (\"============================\")\n",
    "#         for X, y in validation_generator:\n",
    "#             print (X.shape, y.shape)\n",
    "#             print (y[:4])\n",
    "\t\t# imshow(X[0,:,:,64, 0], X[1,:,:,64, 0], X[2,:,:,64, 0], X[3,:,:,64, 0])\n",
    "        \n",
    "\n",
    "#     import time\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     for i, (X, y) in enumerate(validation_generator):\n",
    "#         elapsed_time = time.time() - start_time\n",
    "#         start_time = time.time()\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.imshow(X[0][0][:, :, 0])\n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(X[1][0][:, :, 0])\n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.imshow(X[2][0][:, :, 0])\n",
    "#         plt.title(str(y[0]))\n",
    "#         plt.savefig(str(i) + \"_.png\")\n",
    "#         print(y, type(X))\n",
    "#         print(X[0].shape, X[1].shape, X[2].shape)\n",
    "#         print(i, \"Elapsed Time\", np.round(elapsed_time, decimals=2), \"seconds\")\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import nobrainer\n",
    "from nobrainer import dataset, volume\n",
    "\n",
    "dir_path = os.path.abspath(\"csv/faced_defaced/train_test_fold_1/csv/\")\n",
    "csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "# print(dir_path, csv_path)\n",
    "\n",
    "\n",
    "labels = pd.read_csv(csv_path)[\"Y\"].values\n",
    "paths = pd.read_csv(csv_path)[\"X\"].values\n",
    "\n",
    "\n",
    "# print(labels)\n",
    "\n",
    "n_classes=2\n",
    "volume_shape = (256, 256, 256)\n",
    "block_shape = (128, 128, 128)\n",
    "\n",
    "\n",
    "training_paths = zip(paths, labels)\n",
    "\n",
    "print(training_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nibabel.nifti1.Nifti1Image'>\n",
      "data shape (64, 64, 64)\n",
      "affine: \n",
      "[[  1.    0.    0.  -31.5]\n",
      " [  0.    1.    0.  -31.5]\n",
      " [  0.    0.    1.  -31.5]\n",
      " [  0.    0.    0.    1. ]]\n",
      "metadata:\n",
      "<class 'nibabel.nifti1.Nifti1Header'> object, endian='<'\n",
      "sizeof_hdr      : 348\n",
      "data_type       : b''\n",
      "db_name         : b''\n",
      "extents         : 0\n",
      "session_error   : 0\n",
      "regular         : b''\n",
      "dim_info        : 0\n",
      "dim             : [ 3 64 64 64  1  1  1  1]\n",
      "intent_p1       : 0.0\n",
      "intent_p2       : 0.0\n",
      "intent_p3       : 0.0\n",
      "intent_code     : none\n",
      "datatype        : float64\n",
      "bitpix          : 64\n",
      "slice_start     : 0\n",
      "pixdim          : [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "vox_offset      : 0.0\n",
      "scl_slope       : nan\n",
      "scl_inter       : nan\n",
      "slice_end       : 0\n",
      "slice_code      : unknown\n",
      "xyzt_units      : 0\n",
      "cal_max         : 0.0\n",
      "cal_min         : 0.0\n",
      "slice_duration  : 0.0\n",
      "toffset         : 0.0\n",
      "glmax           : 0\n",
      "glmin           : 0\n",
      "descrip         : b''\n",
      "aux_file        : b''\n",
      "qform_code      : unknown\n",
      "sform_code      : aligned\n",
      "quatern_b       : 0.0\n",
      "quatern_c       : 0.0\n",
      "quatern_d       : 0.0\n",
      "qoffset_x       : -31.5\n",
      "qoffset_y       : -31.5\n",
      "qoffset_z       : -31.5\n",
      "srow_x          : [  1.    0.    0.  -31.5]\n",
      "srow_y          : [  0.    1.    0.  -31.5]\n",
      "srow_z          : [  0.    0.    1.  -31.5]\n",
      "intent_name     : b''\n",
      "magic           : b'n+1'\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import binascii\n",
    "from helpers.utils import load_vol, save_vol\n",
    "from preprocessing.normalization import standardize_volume, normalize_volume\n",
    "from preprocessing.conform import conform_data\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import nibabel as nib\n",
    "from shutil import *\n",
    "import subprocess\n",
    "\n",
    "\n",
    "orig_data_face = \"/work/01329/poldrack/data/mriqc-net/data/face/T1w\"\n",
    "orig_data_deface = \"/work/01329/poldrack/data/mriqc-net/data/defaced\"\n",
    "\n",
    "save_data_face = \"/work/06850/sbansal6/maverick2/mriqc-shared/face\"\n",
    "save_data_deface = \"/work/06850/sbansal6/maverick2/mriqc-shared/deface\"\n",
    "\n",
    "os.makedirs(save_data_face, exist_ok=True)\n",
    "os.makedirs(save_data_deface, exist_ok=True)\n",
    "\n",
    "\n",
    "conform_size = (64, 64, 64)\n",
    "\n",
    "def is_gz_file(filepath):\n",
    "    if os.path.splitext(filepath)[1] == '.gz':\n",
    "        with open(filepath, 'rb') as test_f:\n",
    "            return binascii.hexlify(test_f.read(2)) == b'1f8b'\n",
    "    \n",
    "\n",
    "# in_file = '/work/01329/poldrack/data/mriqc-net/data/face/T1w/ds000009_anat/sub-01_T1w.nii.gz'\n",
    "# dst_path = '/work/06850/sbansal6/maverick2/mriqc-shared/face'\n",
    "\n",
    "# print(is_gz_file(in_file))\n",
    "\n",
    "# if not is_gz_file(in_file):\n",
    "#     filename = in_file.split(\"/\")[-1]\n",
    "#     print(filename)\n",
    "#     rename_file = os.path.splitext(filename)[0]\n",
    "#     dst = os.path.join(dst_path, rename_file)\n",
    "    \n",
    "#     subprocess.call(['cp', in_file, dst])\n",
    "    \n",
    "# in_file = '/work/06850/sbansal6/maverick2/mriqc-shared/face/conformed/sub-04_T1w.nii'\n",
    "# if isinstance(in_file, (str, Path)):\n",
    "#     in_file = nib.load(in_file)\n",
    "    \n",
    "    \n",
    "# print(in_file)\n",
    "\n",
    "# volume = conform_data(in_file, out_size=conform_size)\n",
    "\n",
    "# print(type(volume))\n",
    "\n",
    "def preprocess(pth, conform_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(pth)\n",
    "    filename = pth.split(\"/\")[-1]\n",
    "    print('Confirmation step')\n",
    "    volume = conform_data(pth, out_size=conform_size)\n",
    "    \n",
    "    print(\"Normalize/Standardize step\")\n",
    "    volume = normalize_volume(standardize_volume(volume))\n",
    "    save_path = os.path.join(save_data_face, 'conformed', filename)\n",
    "\n",
    "    newaffine = np.eye(4)\n",
    "    newaffine[:3, 3] = -0.5 * (np.array(conform_size) - 1)\n",
    "    nii = nib.Nifti1Image(volume, newaffine, None)\n",
    "    \n",
    "    print(\"Save new affine\")\n",
    "    nii.to_filename(save_path)\n",
    "    return save_path\n",
    "\n",
    "\n",
    "for path in glob(orig_data_face + \"/*/*.nii.gz\"):\n",
    "    print(path)\n",
    "    if not is_gz_file(path):\n",
    "        tempname = path.split(\"/\")[-1]\n",
    "        rename_file = os.path.splitext(tempname)[0]\n",
    "        dst = os.path.join(save_data_face, rename_file)\n",
    "        print(dst)\n",
    "        subprocess.call(['cp', path, dst])\n",
    "        \n",
    "        \n",
    "        print(preprocess(dst, conform_size))\n",
    "    else:\n",
    "        print(preprocess(path, conform_size))\n",
    "\n",
    "\n",
    "# for path in glob(orig_data_deface + \"/*/*.nii.gz\"):\n",
    "#     try:\n",
    "#         print(preprocess(path))\n",
    "#     except:\n",
    "#         pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
