{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import nobrainer\n",
    "from nobrainer import dataset, volume\n",
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "dir_path = os.path.abspath(\"csv/faced_defaced/train_test_fold_1/csv/\")\n",
    "csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "# print(dir_path, csv_path)\n",
    "\n",
    "\n",
    "labels = pd.read_csv(csv_path)[\"Y\"].values\n",
    "paths = pd.read_csv(csv_path)[\"X\"].values\n",
    "\n",
    "\n",
    "# print(labels)\n",
    "\n",
    "n_classes=2\n",
    "volume_shape = (256, 256, 256)\n",
    "block_shape = (128, 128, 128)\n",
    "\n",
    "\n",
    "training_paths = zip(paths, labels)\n",
    "\n",
    "print(training_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "# matplotlib.use('Agg')\n",
    "import os, sys\n",
    "sys.path.append('../defacing')\n",
    "\n",
    "from preprocessing.normalization import clip, standardize, normalize\n",
    "from preprocessing.conform import conform_data\n",
    "from helpers.utils import load_vol, save_vol, is_gz_file\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "# matplotlib.use('Agg')\n",
    "import os, sys\n",
    "sys.path.append('../defacing')\n",
    "\n",
    "from preprocessing.normalization import clip, standardize, normalize\n",
    "from preprocessing.conform import conform_data\n",
    "from helpers.utils import load_vol, save_vol, is_gz_file\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "\n",
    "orig_root_dir = '/work/01329/poldrack/data/mriqc-net/data'\n",
    "orig_data_face = os.path.join(orig_root_dir, 'face/T1w')\n",
    "orig_data_mask = os.path.join(orig_root_dir, 'masks')\n",
    "\n",
    "save_root_dir = '/work/06850/sbansal6/maverick2/mriqc-shared/'\n",
    "\n",
    "save_preprocessing_face = os.path.join(save_root_dir, 'preprocessing/face')\n",
    "save_conformed_face = os.path.join(save_root_dir, 'conformed/face')\n",
    "\n",
    "save_preprocessing_deface = os.path.join(save_root_dir, 'preprocessing/deface')\n",
    "save_conformed_deface = os.path.join(save_root_dir, 'conformed/deface')\n",
    "\n",
    "os.makedirs(save_preprocessing_face, exist_ok=True)\n",
    "os.makedirs(save_preprocessing_deface, exist_ok=True)\n",
    "os.makedirs(save_conformed_face, exist_ok=True)\n",
    "os.makedirs(save_conformed_deface, exist_ok=True)\n",
    "\n",
    "conform_size = (64, 64, 64)\n",
    "conform_zoom = (4., 4., 4.)\n",
    "\n",
    "\n",
    "def preprocess(orig_vol_pth, conform_pth, preprocess_pth, DS=None, mask_path=None, debug=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    filename = orig_vol_pth.split('/')[-1]\n",
    "    volume, affine, _ = load_vol(orig_vol_pth)\n",
    "    \n",
    "    # Preprocessing\n",
    "    volume = clip(volume, q=90)\n",
    "    volume = normalize(volume)\n",
    "    volume = standardize(volume)\n",
    "    \n",
    "    # \n",
    "    save_preprocessing_path = os.path.join(preprocess_pth, filename)\n",
    "    save_conformed_path = os.path.join(conform_pth, filename)\n",
    "    \n",
    "    print(\"save_preprocessing_path: \", save_preprocessing_path)\n",
    "    print(\"save_conformed_path: \", save_conformed_path)\n",
    "    \n",
    "    save_vol(save_preprocessing_path, volume, affine)\n",
    "\n",
    "    def _plot(data):\n",
    "        f, axarr = plt.subplots(8, 8, figsize=(12, 12))\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                axarr[i, j].imshow(np.rot90(data[:, :, j + 8*i], 1))\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "#         \"\"\"\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.imshow(np.rot90(np.mean(data, axis=0)))\n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(np.rot90(np.mean(data, axis=1)))\n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.imshow(np.rot90(np.mean(data, axis=2)))\n",
    "#         plt.show()\n",
    "#         \"\"\"\n",
    "    conform_data(save_preprocessing_path, \n",
    "                 out_file=save_conformed_path, \n",
    "                 out_size=conform_size, \n",
    "                 out_zooms=conform_zoom)\n",
    "\n",
    "#     if debug: _plot(load_vol(save_conformed_path)[0])\n",
    "    \n",
    "    if mask_path and DS:\n",
    "        mask = np.array(nib.load(mask_path).dataobj)\n",
    "        masked_volume = volume*mask\n",
    "\n",
    "        save_mpreprocessing_path = os.path.join(save_preprocessing_deface, DS, filename)\n",
    "        save_mconformed_path = os.path.join(save_conformed_deface, DS, filename)\n",
    "        \n",
    "        print(\"save_deface_preprocessing_path: \", save_mpreprocessing_path)\n",
    "        print(\"save_deface_conformed_path: \", save_mconformed_path)\n",
    "        \n",
    "#         os.makedirs(save_mpreprocessing_path, exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(save_mconformed_path), exist_ok=True)\n",
    "    \n",
    "#         save_mpreprocessing_path = os\n",
    "        \n",
    "        save_vol(save_mpreprocessing_path, masked_volume, affine)\n",
    "\n",
    "        conform_data(save_mpreprocessing_path, \n",
    "                 out_file=save_mconformed_path, \n",
    "                 out_size=conform_size, \n",
    "                 out_zooms=conform_zoom)        \n",
    "        \n",
    "        return save_conformed_path, save_mconformed_path\n",
    "\n",
    "    return save_conformed_path\n",
    "\n",
    "\n",
    "def checkNonConformed(orig_path, save_path):\n",
    "\n",
    "    conform = []\n",
    "    orig = []\n",
    "\n",
    "    for path in glob(save_path + \"/*/*.nii*\"):\n",
    "        tempname = path.split(\"/\")[-1]\n",
    "        ds = path.split(\"/\")[-2]\n",
    "        conform.append(ds + \"/\" + tempname)\n",
    "\n",
    "    print(\"Total Conformed: \", len(conform))\n",
    "\n",
    "    for path in glob(orig_path + \"/*/*.nii*\"):\n",
    "        tempname = path.split(\"/\")[-1]\n",
    "        ds = path.split(\"/\")[-2]\n",
    "        orig.append(ds + \"/\" + tempname)\n",
    "\n",
    "    print(\"Total Original: \", len(orig))\n",
    "\n",
    "    print(\"Total not conformed: \", len(orig) - len(conform))\n",
    "\n",
    "    count = 0\n",
    "    for f in orig:\n",
    "        exists = False\n",
    "        for fc in conform:\n",
    "            if fc in f:\n",
    "                exists = True\n",
    "        if not exists:\n",
    "            count += 1\n",
    "            print(\"Not conformed file: \", f)\n",
    "\n",
    "\n",
    "for path in glob(orig_data_face + \"/*/*.nii*\"):\n",
    "    try:\n",
    "        if 'ds000140_anat' not in path:\n",
    "            print(\"Orig Path: \", path)\n",
    "            # Example: \n",
    "            #        vol_name - sub-22_T1w.nii.gz\n",
    "            #        DS - ds000140_anat\n",
    "            vol_name = path.split(\"/\")[-1]\n",
    "            DS = path.split(\"/\")[-2]\n",
    "            \n",
    "            \n",
    "                \n",
    "            # directories for saving preprocessed and conformed volumes\n",
    "            ds_save_conform_path = os.path.join(save_conformed_face, DS)\n",
    "            ds_save_preprocess_path = os.path.join(save_preprocessing_face, DS)\n",
    "\n",
    "            # Get the mask path\n",
    "            mask_path = glob(os.path.join(orig_data_mask, DS, vol_name.split('.')[0] + \"*_mask*\"))[0]\n",
    "\n",
    "            print(\"Mask_path\", mask_path)\n",
    "\n",
    "            if not os.path.exists(ds_save_conform_path):\n",
    "                os.makedirs(ds_save_conform_path)\n",
    "\n",
    "            if not os.path.exists(ds_save_preprocess_path):\n",
    "                os.makedirs(ds_save_preprocess_path)\n",
    "\n",
    "            # Check if volume is a proper gunzipped\n",
    "            if not os.path.splitext(path)[1] == \".gz\" and is_gz_file(path):\n",
    "                rename_file = os.path.splitext(vol_name)[0]\n",
    "                fixed_gz_tmp = os.path.join(save_conformed_face, rename_file)\n",
    "                print(fixed_gz_tmp)\n",
    "                subprocess.call([\"cp\", path, fixed_gz_tmp])\n",
    "\n",
    "                print(preprocess(fixed_gz_tmp,\n",
    "                                 ds_save_conform_path,\n",
    "                                 ds_save_preprocess_path,\n",
    "                                 mask_path = mask_path,\n",
    "                                 DS=DS\n",
    "                                ))\n",
    "                os.remove(fixed_gz_tmp)\n",
    "\n",
    "            else:\n",
    "                print(preprocess(path,\n",
    "                                 ds_save_conform_path,\n",
    "                                 ds_save_preprocess_path,\n",
    "                                 mask_path = mask_path,\n",
    "                                 DS=DS\n",
    "                                ))\n",
    "    except:\n",
    "        print(\"Preprocessing incomplete. Exception occurred.\")\n",
    "        pass\n",
    "\n",
    "\n",
    "# for path in glob(orig_data_deface + \"/*/*.nii*\"):\n",
    "#     try:\n",
    "#         print(\"Orig Path: \", path)\n",
    "#         if not is_gz_file(path) and os.path.splitext(path)[1] == \".gz\":\n",
    "#             tempname = path.split(\"/\")[-1]\n",
    "#             ds = path.split(\"/\")[-2]\n",
    "#             rename_file = os.path.splitext(tempname)[0]\n",
    "#             dst = os.path.join(save_data_deface, rename_file)\n",
    "# #             print(dst)\n",
    "#             subprocess.call([\"cp\", path, dst])\n",
    "#             ds_save_path = os.path.join(save_data_deface, ds)\n",
    "#             if not os.path.exists(ds_save_path):\n",
    "#                 os.makedirs(ds_save_path)\n",
    "#             preprocess(dst, conform_size, save_data_path=ds_save_path))\n",
    "#         else:\n",
    "#             ds = path.split(\"/\")[-2]\n",
    "#             ds_save_path = os.path.join(save_data_deface, ds)\n",
    "#             if not os.path.exists(ds_save_path):\n",
    "#                 os.makedirs(ds_save_path)\n",
    "#             preprocess(path, conform_size, save_data_path=ds_save_path)\n",
    "#     except:\n",
    "#         print(\"Preprocessing incomplete. Exception occurred.\")\n",
    "#         pass\n",
    "    \n",
    "\n",
    "# checkNonConformed(orig_data_face, save_data_face)\n",
    "# checkNonConformed(orig_data_deface, save_data_deface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkNonConformed(orig_data_deface, save_conformed_deface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_C = []\n",
    "face_O = []\n",
    "\n",
    "for path in glob(save_data_face + \"/*/*.nii*\"):\n",
    "    tempname = path.split(\"/\")[-1]\n",
    "    ds = path.split(\"/\")[-2]\n",
    "    face_C.append(ds + '/' + tempname)\n",
    "\n",
    "print(len(face_C))\n",
    "# print(face_C)\n",
    "\n",
    "\n",
    "for path in glob(orig_data_face + \"/*/*.nii*\"):\n",
    "    tempname = path.split(\"/\")[-1]\n",
    "    ds = path.split(\"/\")[-2]\n",
    "    face_O.append(ds + '/' + tempname)\n",
    "\n",
    "print(len(face_O))\n",
    "# print(face_O)\n",
    "\n",
    "count = 0\n",
    "for f in face_O:\n",
    "    exists = False\n",
    "    for fc in face_C:\n",
    "        if fc in f:\n",
    "            exists = True\n",
    "    if not exists:\n",
    "        count += 1\n",
    "        print(f)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate n-fold CV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "face_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face\"\n",
    "deface_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/deface\"\n",
    "\n",
    "paths = []\n",
    "labels = []\n",
    "\n",
    "for path in glob(deface_path + \"/*/*.nii*\"):\n",
    "    paths.append(path)\n",
    "    labels.append(0)\n",
    "\n",
    "for path in glob(face_path + \"/*/*.nii*\"):\n",
    "    paths.append(path)\n",
    "    labels.append(1)\n",
    "    \n",
    "print(len(paths))\n",
    "print(len(labels))\n",
    "\n",
    "save_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/csv\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"X\"] = paths\n",
    "df[\"Y\"] = labels\n",
    "df.to_csv(os.path.join(save_path, \"all.csv\"))\n",
    "\n",
    "SPLITS = 10\n",
    "skf = StratifiedKFold(n_splits=SPLITS)\n",
    "fold_no = 1\n",
    "\n",
    "for train_index, test_index in skf.split(paths, labels):\n",
    "    out_path = save_path + \"/train_test_fold_{}/csv/\".format(fold_no)\n",
    "\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "    image_train, image_test = (\n",
    "        itemgetter(*train_index)(paths),\n",
    "        itemgetter(*test_index)(paths),\n",
    "    )\n",
    "    label_train, label_test = (\n",
    "        itemgetter(*train_index)(labels),\n",
    "        itemgetter(*test_index)(labels),\n",
    "    )\n",
    "\n",
    "    # image_train = [os.path.join(data_path, 'sub-' + str(pth) + '_T1w.nii.gz') for pth in image_train]\n",
    "    train_data = {\"X\": image_train, \"Y\": label_train}\n",
    "    df_train = pd.DataFrame(train_data)\n",
    "    df_train.to_csv(os.path.join(out_path, \"training.csv\"), index=False)\n",
    "\n",
    "    # image_test = [os.path.join(data_path, 'sub-' + str(pth) + '_T1w.nii.gz') for pth in image_test]\n",
    "    validation_data = {\"X\": image_test, \"Y\": label_test}\n",
    "    df_validation = pd.DataFrame(validation_data)\n",
    "    df_validation.to_csv(os.path.join(out_path, \"validation.csv\"), index=False)\n",
    "\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tfrecords for n-fold CV Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "SPLITS = 10\n",
    "\n",
    "for fold in range(1, SPLITS+1):\n",
    "    \n",
    "    dir_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/csv/train_test_fold_{}/csv/\".format(fold)\n",
    "    tf_records_dir = \"/work/06850/sbansal6/maverick2/mriqc-shared/tfrecords/tfrecords_fold_{}/\".format(fold)\n",
    "    os.makedirs(tf_records_dir, exist_ok=True)\n",
    "    \n",
    "    train_csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "    valid_csv_path = os.path.join(dir_path, \"validation.csv\")\n",
    "    \n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "    train_D = list(zip(train_paths, train_labels))\n",
    "    random.shuffle(train_D)\n",
    "#     print(train_D[0])\n",
    "    \n",
    "    valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "    valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "    valid_D = list(zip(valid_paths, valid_labels))\n",
    "    random.shuffle(valid_D)\n",
    "    \n",
    "    train_write_path = os.path.join(tf_records_dir, 'data-train_shard-{shard:03d}.tfrec')\n",
    "    valid_write_path = os.path.join(tf_records_dir, 'data-valid_shard-{shard:03d}.tfrec')\n",
    "    \n",
    "    nobrainer.tfrecord.write(\n",
    "        features_labels=train_D,\n",
    "        filename_template=train_write_path,\n",
    "        examples_per_shard=3)\n",
    "    \n",
    "    nobrainer.tfrecord.write(\n",
    "        features_labels=valid_D,\n",
    "        filename_template=valid_write_path,\n",
    "        examples_per_shard=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "face_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face\"\n",
    "deface_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/deface\"\n",
    "\n",
    "paths = []\n",
    "labels = []\n",
    "\n",
    "for path in glob(deface_path + \"/*/*.nii*\"):\n",
    "    paths.append(path)\n",
    "    labels.append(0)\n",
    "\n",
    "for path in glob(face_path + \"/*/*.nii*\"):\n",
    "    paths.append(path)\n",
    "    labels.append(1)\n",
    "    \n",
    "print(len(paths))\n",
    "print(len(labels))\n",
    "\n",
    "\n",
    "# print(paths)\n",
    "X_train, X_test, y_train, y_test = train_test_split(paths, labels, test_size=0.2, random_state=123)\n",
    "\n",
    "# print(X_train, y_train, stratify=True)\n",
    "\n",
    "save_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/csv_final\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "train_data = {\"X\": X_train, \"Y\": y_train}\n",
    "df_train = pd.DataFrame(train_data)\n",
    "df_train.to_csv(os.path.join(save_path, \"training.csv\"), index=False)\n",
    "\n",
    "validation_data = {\"X\": X_test, \"Y\": y_test}\n",
    "df_validation = pd.DataFrame(validation_data)\n",
    "df_validation.to_csv(os.path.join(save_path, \"validation.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "dir_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/csv_final\"\n",
    "tf_records_dir = \"/work/06850/sbansal6/maverick2/mriqc-shared/tfrecords_final\"\n",
    "os.makedirs(tf_records_dir, exist_ok=True)\n",
    "\n",
    "train_csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "valid_csv_path = os.path.join(dir_path, \"validation.csv\")\n",
    "\n",
    "train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "train_D = list(zip(train_paths, train_labels))\n",
    "random.shuffle(train_D)\n",
    "#     print(train_D[0])\n",
    "\n",
    "valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "valid_D = list(zip(valid_paths, valid_labels))\n",
    "random.shuffle(valid_D)\n",
    "\n",
    "train_write_path = os.path.join(tf_records_dir, 'data-train_shard-{shard:03d}.tfrec')\n",
    "valid_write_path = os.path.join(tf_records_dir, 'data-valid_shard-{shard:03d}.tfrec')\n",
    "\n",
    "nobrainer.tfrecord.write(\n",
    "    features_labels=train_D,\n",
    "    filename_template=train_write_path,\n",
    "    examples_per_shard=3)\n",
    "\n",
    "nobrainer.tfrecord.write(\n",
    "    features_labels=valid_D,\n",
    "    filename_template=valid_write_path,\n",
    "    examples_per_shard=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nobrainer\n",
    "from nobrainer.io import _is_gzipped\n",
    "from nobrainer.volume import to_blocks\n",
    "import sys\n",
    "sys.path.append('../defacing')\n",
    "from helpers.utils import load_vol\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/'\n",
    "DISTRIBUTION = load_vol('../defacing/helpers/distribution.nii.gz')[0]\n",
    "DISTRIBUTION /= DISTRIBUTION.sum()\n",
    "\n",
    "sampler = lambda n: np.array([ np.unravel_index(\n",
    "          np.random.choice(np.arange(np.prod(DISTRIBUTION.shape)),\n",
    "                                     p = DISTRIBUTION.ravel()),\n",
    "          DISTRIBUTION.shape) for _ in range(n)]) \n",
    "\n",
    "\n",
    "# function to apply augmentations to tf dataset\n",
    "def apply_augmentations(features, labels):\n",
    "\n",
    "    \"\"\" Apply <TYPE_OF> augmentation to the dataset\n",
    "\n",
    "    \"\"\"\n",
    "    #     iaa.SomeOf(\n",
    "    #             (0, 3),\n",
    "    #             [\n",
    "    #                 iaa.Fliplr(0.5),\n",
    "    #                 iaa.Flipud(0.5),\n",
    "    #                 iaa.Noop(),\n",
    "    #                 iaa.OneOf(\n",
    "    #                     [\n",
    "    #                         iaa.Affine(rotate=90),\n",
    "    #                         iaa.Affine(rotate=180),\n",
    "    #                         iaa.Affine(rotate=270),\n",
    "    #                     ]\n",
    "    #                 ),\n",
    "    #                 # iaa.GaussianBlur(sigma=(0.0, 0.2)),\n",
    "    #             ],\n",
    "    #         )\n",
    "\n",
    "    return\n",
    "\n",
    "def _magic_slicing_(shape):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    idx = []\n",
    "    for ii in np.arange(shape[0]):\n",
    "        if (ii % shape[0]**0.5) == 0:\n",
    "            idx.append(ii)\n",
    "    idx = np.array(idx)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    file_pattern,\n",
    "    n_classes,\n",
    "    batch_size,\n",
    "    volume_shape,\n",
    "    plane,\n",
    "    n = 24,\n",
    "    block_shape=None,\n",
    "    n_epochs=None,\n",
    "    mapping=None,\n",
    "    augment=False,\n",
    "    shuffle_buffer_size=None,\n",
    "    num_parallel_calls=AUTOTUNE,\n",
    "):\n",
    "\n",
    "    \"\"\" Returns tf.data.Dataset after preprocessing from\n",
    "    tfrecords for training and validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_pattern:\n",
    "\n",
    "    n_classes:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    files = glob.glob(file_pattern)\n",
    "\n",
    "    if not files:\n",
    "        raise ValueError(\"no files found for pattern '{}'\".format(file_pattern))\n",
    "\n",
    "    compressed = _is_gzipped(files[0])\n",
    "    shuffle = bool(shuffle_buffer_size)\n",
    "\n",
    "    ds = nobrainer.dataset.tfrecord_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        volume_shape=volume_shape,\n",
    "        shuffle=shuffle,\n",
    "        scalar_label=True,\n",
    "        compressed=compressed,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "    )\n",
    "    \n",
    "    print(ds)\n",
    "\n",
    "    if augment:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: tf.cond(\n",
    "                tf.random.uniform((1,)) > 0.5,\n",
    "                true_fn=lambda: apply_augmentations(x, y),\n",
    "                false_fn=lambda: (x, y),\n",
    "            ),\n",
    "            num_parallel_calls=num_parallel_calls,\n",
    "        )\n",
    "\n",
    "    def _ss(x, y):\n",
    "        x, y = structural_slice(x, y, plane, n)\n",
    "        return (x, y)\n",
    "\n",
    "    ds = ds.map(_ss, num_parallel_calls)\n",
    "\n",
    "    #     def _f(x, y):\n",
    "    #         x = to_blocks(x, block_shape)\n",
    "    #         n_blocks = x.shape[0]\n",
    "    #         y = tf.repeat(y, n_blocks)\n",
    "    #         return (x, y)\n",
    "    #     ds = ds.map(_f, num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # This step is necessary because it reduces the extra dimension.\n",
    "    # ds = ds.unbatch()\n",
    "\n",
    "\n",
    "    ds = ds.prefetch(buffer_size=batch_size)\n",
    "    def reshape(x,y):\n",
    "        if plane == \"combined\":\n",
    "            for _ in 3:\n",
    "                pass\n",
    "        return (x, y)\n",
    "    if batch_size is not None:\n",
    "        ds = ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "    if shuffle_buffer_size:\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat the dataset n_epochs times\n",
    "    ds = ds.repeat(n_epochs)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def structural_slice(x, y, plane, n=4):\n",
    "\n",
    "    \"\"\" Transpose dataset based on the plane\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "\n",
    "    y:\n",
    "\n",
    "    plane:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    options = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "    shape = np.array(x.shape)\n",
    "\n",
    "    if isinstance(plane, str) and plane in options:\n",
    "        if plane == \"axial\":\n",
    "            idx = np.random.randint(shape[0]**0.5)\n",
    "            midx = sampler(n)[:, 0]\n",
    "            x = x\n",
    "            k = 3\n",
    "\n",
    "        if plane == \"coronal\":\n",
    "            idx = np.random.randint(shape[1]**0.5)\n",
    "            midx = sampler(n)[:, 1]\n",
    "            x = tf.transpose(x, perm=[1, 2, 0])\n",
    "            k = 2\n",
    "\n",
    "        if plane == \"sagittal\":\n",
    "            idx = np.random.randint(shape[2]**0.5)\n",
    "            midx = sampler(n)[:, 2]\n",
    "            x = tf.transpose(x, perm=[2, 0, 1])\n",
    "            k = 1\n",
    "\n",
    "        if plane == \"combined\":\n",
    "            temp = {}\n",
    "            for op in options[:-1]:\n",
    "                temp[op] = structural_slice(x, y, op, n)[0]\n",
    "            x = temp\n",
    "\n",
    "        if not plane == \"combined\":\n",
    "            x = tf.squeeze(tf.gather_nd(x, midx.reshape(n, 1, 1)), axis=1)\n",
    "            x = tf.math.reduce_mean(x, axis=0)\n",
    "            x = tf.convert_to_tensor(tf.expand_dims(x, axis=-1))\n",
    "            x =  tf.image.rot90(\n",
    "                   x, k, name=None\n",
    "                )\n",
    "\n",
    "        # y = tf.repeat(y, n)\n",
    "\n",
    "        return x, y\n",
    "    else:\n",
    "        raise ValueError(\"expected plane to be one of ['axial', 'coronal', 'sagittal']\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    n_classes = 2\n",
    "    global_batch_size = 8\n",
    "    volume_shape = (64, 64, 64)\n",
    "    ds = get_dataset(\n",
    "        ROOTDIR + \"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "        n_classes=n_classes,\n",
    "        batch_size=global_batch_size,\n",
    "        volume_shape=volume_shape,\n",
    "        plane=\"combined\",\n",
    "        shuffle_buffer_size=3,\n",
    "    )\n",
    "    \n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(ds)\n",
    "    \n",
    "#     for ii in range(100):\n",
    "#         x,y=next(ds.as_numpy_iterator())\n",
    "#         # print (np.min(x), np.max(x), np.unique(y))\n",
    "#         count = 1\n",
    "#         for i in range(global_batch_size):\n",
    "#             for key in x.keys():\n",
    "#                 plt.subplot(global_batch_size, 3, count)\n",
    "#                 plt.imshow(x[key][i, :, :, 0])\n",
    "#                 plt.title(str(y[i]))\n",
    "#                 plt.xticks([],\" \")\n",
    "#                 plt.yticks([], \" \")\n",
    "#                 count += 1\n",
    "#         plt.savefig(\"processed_image_combined_{}.png\".format(ii))\n",
    "\n",
    "\n",
    "# dataset_train_coronal = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "#                             n_classes=n_classes,\n",
    "#                             batch_size=global_batch_size,\n",
    "#                             volume_shape=volume_shape,\n",
    "#                             block_shape=block_shape,\n",
    "#                             plane='coronal',\n",
    "#                             shuffle_buffer_size=3)\n",
    "\n",
    "# dataset_train_sagittal = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "#                             n_classes=n_classes,\n",
    "#                             batch_size=global_batch_size,\n",
    "#                             volume_shape=volume_shape,\n",
    "#                             block_shape=block_shape,\n",
    "#                             plane='sagittal',\n",
    "#                             shuffle_buffer_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fold = 1\n",
    "dir_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/csv/train_test_fold_{}/csv/\".format(fold)\n",
    "\n",
    "train_csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "valid_csv_path = os.path.join(dir_path, \"validation.csv\")\n",
    "\n",
    "train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "\n",
    "\n",
    "print(len(train_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Std packages\n",
    "import sys, os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "sys.path.append(\"../defacing\")\n",
    "\n",
    "# Custom packages\n",
    "from models import modelN\n",
    "# from dataloaders.dataset import get_dataset\n",
    "\n",
    "# Tf packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler,\n",
    "    TensorBoard,\n",
    ")\n",
    "# import nobrainer\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "ROOTDIR = \"/work/06850/sbansal6/maverick2/mriqc-shared/\"\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 3:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "\n",
    "def train(\n",
    "    volume_shape=(64, 64, 64),\n",
    "    image_size=(64, 64),\n",
    "    dropout=0.2,\n",
    "    batch_size=8,\n",
    "    n_classes=2,\n",
    "    n_epochs=15,\n",
    "    fold=1\n",
    "):\n",
    "    \n",
    "    dir_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/csv/train_test_fold_{}/csv/\".format(fold)\n",
    "\n",
    "    train_csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "    valid_csv_path = os.path.join(dir_path, \"validation.csv\")\n",
    "\n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "\n",
    "    planes = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "\n",
    "#     strategy = tf.distribute.MirroredStrategy()\n",
    "#     BATCH_SIZE_PER_REPLICA = batch_size\n",
    "#     global_batch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "    global_batch_size = batch_size\n",
    "    \n",
    "    model_save_path = os.path.join(ROOTDIR, \"model_save_dir_final2\")\n",
    "    \n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "    cp_save_path = os.path.join(model_save_path, \"weights\")\n",
    "\n",
    "    logdir_path = os.path.join(model_save_path, \"tb_logs\")\n",
    "    if not os.path.exists(logdir_path):\n",
    "        os.makedirs(logdir_path)\n",
    "\n",
    "    for plane in planes:\n",
    "\n",
    "        logdir = os.path.join(logdir_path, plane)\n",
    "        os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "        tbCallback = TensorBoard(\n",
    "            log_dir=logdir, histogram_freq=2, write_graph=True, write_images=True,\n",
    "        )\n",
    "\n",
    "        os.makedirs(os.path.join(cp_save_path, plane), exist_ok=True)\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            os.path.join(cp_save_path, plane, \"best-wts.h5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "\n",
    "#         with strategy.scope():\n",
    "\n",
    "        if not plane == \"combined\": \n",
    "            lr = 1e-3\n",
    "            model = modelN.Submodel(\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                name=plane,\n",
    "                include_top=True,\n",
    "                weights=None,\n",
    "            )\n",
    "        else:\n",
    "            lr = 5e-4\n",
    "            model = modelN.CombinedClassifier(\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                trainable=True,\n",
    "                wts_root=cp_save_path,\n",
    "            )\n",
    "\n",
    "        print(\"Submodel: \", plane)\n",
    "        print(model.summary())\n",
    "\n",
    "        METRICS = [\n",
    "            metrics.TruePositives(name=\"tp\"),\n",
    "            metrics.FalsePositives(name=\"fp\"),\n",
    "            metrics.TrueNegatives(name=\"tn\"),\n",
    "            metrics.FalseNegatives(name=\"fn\"),\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            metrics.Precision(name=\"precision\"),\n",
    "            metrics.Recall(name=\"recall\"),\n",
    "            metrics.AUC(name=\"auc\"),\n",
    "        ]\n",
    "\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.binary_crossentropy,\n",
    "            optimizer=Adam(learning_rate=lr),\n",
    "            metrics=METRICS,\n",
    "        )\n",
    "\n",
    "        print(\"GLOBAL BATCH SIZE: \", global_batch_size)\n",
    "\n",
    "        dataset_train = get_dataset(\n",
    "            file_pattern=os.path.join(ROOTDIR, \"tfrecords_final/data-train_*\".format(fold)),\n",
    "            n_classes=n_classes,\n",
    "            batch_size=global_batch_size,\n",
    "            volume_shape=volume_shape,\n",
    "            plane=plane,\n",
    "            shuffle_buffer_size=global_batch_size,\n",
    "        )\n",
    "\n",
    "        dataset_valid = get_dataset(\n",
    "            file_pattern=os.path.join(ROOTDIR, \"tfrecords_final/data-valid_*\".format(fold)),\n",
    "            n_classes=n_classes,\n",
    "            batch_size=global_batch_size,\n",
    "            volume_shape=volume_shape,\n",
    "            plane=plane,\n",
    "            shuffle_buffer_size=global_batch_size,\n",
    "        )\n",
    "        \n",
    "        steps_per_epoch = math.ceil(len(train_paths)/batch_size)\n",
    "\n",
    "        validation_steps = math.ceil(len(valid_paths)/batch_size)\n",
    "\n",
    "        print(steps_per_epoch, validation_steps)\n",
    "\n",
    "        lrcallback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        \n",
    "        model.fit(\n",
    "            dataset_train,\n",
    "            epochs=n_epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=dataset_valid,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=[tbCallback, model_checkpoint],\n",
    "        )\n",
    "\n",
    "        del model\n",
    "        K.clear_session()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import imgaug\n",
    "import shutil\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../defacing')\n",
    "from helpers.utils import save_vol, load_vol\n",
    "from preprocessing.conform import conform_data\n",
    "from preprocessing.normalization import clip, standardize, normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DataGeneratoronFly(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conform_size=(64, 64, 64),\n",
    "                        conform_zoom=(4., 4., 4.), \n",
    "                        nchannels=1, \n",
    "                        nruns=8,\n",
    "                        nsamples=20,\n",
    "                        save=False, \n",
    "                        transform=None):\n",
    "\n",
    "        self.conform_size=conform_size\n",
    "        self.conform_zoom=conform_zoom\n",
    "        self.nchannels=nchannels\n",
    "        self.transform=transform\n",
    "        self.nsamples=nsamples\n",
    "        self.nruns=nruns\n",
    "        self.save=save\n",
    "        DISTRIBUTION = load_vol('../defacing/helpers/distribution.nii.gz')[0]\n",
    "        assert DISTRIBUTION.shape == conform_size, \"Invalid conform_size needs to regenerate face distribution\"\n",
    "\n",
    "        DISTRIBUTION /= DISTRIBUTION.sum()\n",
    "        self.sampler = lambda n: np.array([ np.unravel_index(\n",
    "                  np.random.choice(np.arange(np.prod(DISTRIBUTION.shape)),\n",
    "                                             p = DISTRIBUTION.ravel()),\n",
    "                  DISTRIBUTION.shape) for _ in range(n)]) \n",
    "\n",
    "\n",
    "\n",
    "    def _augmentation(self, volume):\n",
    "        r\"\"\"\n",
    "                Augmenters that are safe to apply to masks\n",
    "                Some, such as Affine, have settings that make them unsafe, so always\n",
    "                test your augmentation on masks\n",
    "        \"\"\"\n",
    "        volume_shape = volume.shape\n",
    "        det = self.transform.to_deterministic()\n",
    "        volume = det.augment_image(volume)\n",
    "\n",
    "        assert volume.shape == volume_shape, \"Augmentation shouldn't change volume size\"\n",
    "        return volume\n",
    "\n",
    "\n",
    "    def _sample_slices(self, volume, plane=None):\n",
    "\n",
    "        options = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "        assert plane in options, \"expected plane to be one of ['axial', 'coronal', 'sagittal']\"\n",
    "        samples = self.sampler(self.nsamples)\n",
    "\n",
    "        if plane == \"axial\":\n",
    "            midx = samples[:, 0]\n",
    "            volume = volume\n",
    "            k = 3\n",
    "\n",
    "        if plane == \"coronal\":\n",
    "            midx = samples[:, 1]\n",
    "            volume = np.transpose(volume, axes=[1, 2, 0])\n",
    "            k = 2\n",
    "\n",
    "        if plane == \"sagittal\":\n",
    "            midx = samples[:, 2]\n",
    "            volume = np.transpose(volume, axes=[2, 0, 1])\n",
    "            k = 1\n",
    "\n",
    "        if plane == \"combined\":\n",
    "            temp = []\n",
    "            for op in options[:-1]:\n",
    "                temp.append(self._sample_slices(volume, op))\n",
    "            volume = temp\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(volume[0][:,:,0])\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(volume[1][:,:,0])\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(volume[2][:,:,0])\n",
    "            plt.show()\n",
    "\n",
    "        if not plane == \"combined\":\n",
    "            volume = np.squeeze(volume[midx,:,:])\n",
    "            volume = np.mean(volume, axis=0)\n",
    "            volume = np.rot90(volume, k)\n",
    "            volume = volume[..., None]\n",
    "        return volume\n",
    "\n",
    "\n",
    "    def get_data(self, volume):\n",
    "        # Generate indexes of the batch\n",
    "        volume = clip(volume, q=90)\n",
    "        volume = normalize(volume)\n",
    "        volume = standardize(volume)\n",
    "        newaffine = np.eye(4)\n",
    "        newaffine[:3, 3] = -0.5 * (np.array(self.conform_size) - 1)\n",
    "        os.makedirs('./tmp', exist_ok=True)\n",
    "        save_vol('./tmp/Pre-processing.nii.gz', volume, newaffine)\n",
    "        conform_data('./tmp/Pre-processing.nii.gz',\n",
    "                        './tmp/Conformed.nii.gz',\n",
    "                        self.conform_size,\n",
    "                        self.conform_zoom)\n",
    "\n",
    "        volume = load_vol('./tmp/Conformed.nii.gz')[0]\n",
    "\n",
    "        if self.transform:\n",
    "            volume = self._augmentation(volume)\n",
    "\n",
    "        slices = []\n",
    "        for _ in range(self.nruns):\n",
    "            slices.append(self._sample_slices(volume, \n",
    "                                    plane=\"combined\"))\n",
    "\n",
    "        if not self.save: \n",
    "            shutil.rmtree('./tmp')\n",
    "        return slices\n",
    "\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    path = '../sample_vols/defaced/example2.nii.gz'\n",
    "    \n",
    "    vol, _, _ = load_vol(path)\n",
    "    \n",
    "    \n",
    "    inference_transform_params = {\n",
    "            \"conform_size\": (64, 64, 64),\n",
    "            \"conform_zoom\": (4., 4., 4.), \n",
    "            \"nchannels\": 1, \n",
    "            \"nruns\": 8,\n",
    "            \"nsamples\": 24,\n",
    "            \"save\": False, \n",
    "            \"transform\": None\n",
    "        }\n",
    "    \n",
    "    inference_generator = DataGeneratoronFly(**inference_transform_params)\n",
    "    \n",
    "    slices = inference_generator.get_data(vol)\n",
    "    \n",
    "    slices = np.transpose(np.array(slices),axes=[1, 0, 2, 3, 4])\n",
    "    ds = {}\n",
    "    ds['axial'] = slices[0]\n",
    "    ds['coronal'] = slices[1]\n",
    "    ds['sagittal'] = slices[2]\n",
    "    \n",
    "#     print(ds)\n",
    "    print(slices.shape)\n",
    "#         return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "sys.path.append('../defacing')\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from models.modelN import CombinedClassifier\n",
    "# from dataloaders.inference_dataloader import DataGeneratoronFly\n",
    "\n",
    "ROOTDIR = \"/work/06850/sbansal6/maverick2/mriqc-shared/\"\n",
    "\n",
    "\n",
    "class inferer(object):\n",
    "    \"\"\"\n",
    "       nMontecarlo: for multiple exp for same model\n",
    "       quick: checks for all 3 fold models\n",
    "       mode: method to merge predictions\n",
    "             allowed ['avg', 'max_vote']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nMontecarlo=8, mode=\"avg\"):\n",
    "        r\"\"\"\n",
    "        \"\"\"\n",
    "        inference_transform_params = {\n",
    "            \"conform_size\": (64, 64, 64),\n",
    "            \"conform_zoom\": (4., 4., 4.), \n",
    "            \"nchannels\": 1, \n",
    "            \"nruns\": 8,\n",
    "            \"nsamples\": 20,\n",
    "            \"save\": False, \n",
    "            \"transform\": None\n",
    "        }\n",
    "\n",
    "        self.mode = mode\n",
    "        assert self.mode.lower() in [\n",
    "            \"avg\",\n",
    "            \"max_vote\",\n",
    "        ], \"unknown mode, allowed mode are ['avg', 'max_vote']\"\n",
    "\n",
    "        self.inference_generator = DataGeneratoronFly(**inference_transform_params)\n",
    "        self.model = CombinedClassifier(\n",
    "            input_shape=(64, 64), dropout=0.4, wts_root=None, trainable=True\n",
    "        )\n",
    "        self.model.load_weights(\n",
    "\n",
    "            os.path.abspath(os.path.join(ROOTDIR, \"model_save_dir_final/weights/combined/best-wts.h5\"))\n",
    "        )\n",
    "\n",
    "    def infer(self, vol):\n",
    "        \"\"\"\n",
    "        vol : can be numpy ndarray or path to volume\n",
    "        \"\"\"\n",
    "        slices = self.inference_generator.get_data(vol)\n",
    "        \n",
    "        slices = np.transpose(np.array(slices),axes=[1, 0, 2, 3, 4])\n",
    "        ds = {}\n",
    "        ds['axial'] = slices[0]\n",
    "        ds['coronal'] = slices[1]\n",
    "        ds['sagittal'] = slices[2]\n",
    "    \n",
    "        predictions = self.model.predict(ds)\n",
    "\n",
    "        if self.mode.lower() == \"max_vote\":\n",
    "            predictions = np.round(predictions)\n",
    "            unique_elements = np.unique(predictions)\n",
    "            count_array = np.array(\n",
    "                [\n",
    "                    sum(predictions == unique_element)\n",
    "                    for unique_element in unique_elements\n",
    "                ]\n",
    "            )\n",
    "            pred = (\n",
    "                np.argmax(count_array) if len(count_array) > 1 else unique_elements[0]\n",
    "            )\n",
    "            conf = (\n",
    "                1\n",
    "                if len(count_array) == 1\n",
    "                else count_array[pred] * 1.0 / np.sum(count_array)\n",
    "            )\n",
    "        elif self.mode.lower() == \"avg\":\n",
    "            conf = np.mean(predictions)\n",
    "            pred = np.round(conf)\n",
    "\n",
    "        pred_str = \"faced\" if pred == 1 else \"defaced\"\n",
    "        conf = conf if pred == 1 else 1.0 - conf\n",
    "\n",
    "        print(\"[INFO] Given volume is \" + pred_str + \" with confidence of: {}\".format(conf))\n",
    "        \n",
    "        del self.model\n",
    "        K.clear_session()\n",
    "        \n",
    "        return pred, conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../defacing')\n",
    "from helpers.utils import load_vol\n",
    "# from defacing.inference import inferer\n",
    "\n",
    "_inferer = inferer()\n",
    "path = '../sample_vols/faced/example4.nii.gz'\n",
    "vol, _, _ = load_vol(path)\n",
    "label, conf = _inferer.infer(vol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!python example.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
