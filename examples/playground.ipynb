{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import nobrainer\n",
    "from nobrainer import dataset, volume\n",
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "dir_path = os.path.abspath(\"csv/faced_defaced/train_test_fold_1/csv/\")\n",
    "csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "# print(dir_path, csv_path)\n",
    "\n",
    "\n",
    "labels = pd.read_csv(csv_path)[\"Y\"].values\n",
    "paths = pd.read_csv(csv_path)[\"X\"].values\n",
    "\n",
    "\n",
    "# print(labels)\n",
    "\n",
    "n_classes=2\n",
    "volume_shape = (256, 256, 256)\n",
    "block_shape = (128, 128, 128)\n",
    "\n",
    "\n",
    "training_paths = zip(paths, labels)\n",
    "\n",
    "print(training_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orig Path:  /work/01329/poldrack/data/mriqc-net/data/face/T1w/ds000119_anat/sub-10_T1w.nii.gz\n",
      "Mask_path /work/01329/poldrack/data/mriqc-net/data/masks/ds000119_anat/sub-10_T1w_pydeface_mask.nii.gz\n",
      "save_preprocessing_path:  /work/06850/sbansal6/maverick2/mriqc-shared/preprocessing/face/ds000119_anat/sub-10_T1w.nii.gz\n",
      "save_conformed_path:  /work/06850/sbansal6/maverick2/mriqc-shared/conformed/face/ds000119_anat/sub-10_T1w.nii.gz\n"
     ]
    },
    {
     "ename": "ImageFileError",
     "evalue": "Cannot work out file type of \"/work/01329/poldrack/data/mriqc-net/data/masks/ds000119_anat/sub-10_T1w_pydeface_mask.nii.gz\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImageFileError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a1ac81991cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    174\u001b[0m                              \u001b[0mds_save_preprocess_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                              \u001b[0mmask_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                              \u001b[0mDS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                             ))\n\u001b[1;32m    178\u001b[0m \u001b[0;31m#     except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a1ac81991cac>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(orig_vol_pth, conform_pth, preprocess_pth, DS, mask_path, debug)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask_path\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mmasked_volume\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvolume\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/06850/sbansal6/anaconda3/lib/python3.7/site-packages/nibabel/loadsave.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     raise ImageFileError('Cannot work out file type of \"%s\"' %\n\u001b[0;32m---> 56\u001b[0;31m                          filename)\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImageFileError\u001b[0m: Cannot work out file type of \"/work/01329/poldrack/data/mriqc-net/data/masks/ds000119_anat/sub-10_T1w_pydeface_mask.nii.gz\""
     ]
    }
   ],
   "source": [
    "import matplotlib \n",
    "# matplotlib.use('Agg')\n",
    "import os, sys\n",
    "sys.path.append('../defacing')\n",
    "\n",
    "from preprocessing.normalization import clip, standardize, normalize\n",
    "from preprocessing.conform import conform_data\n",
    "from helpers.utils import load_vol, save_vol, is_gz_file\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "\n",
    "orig_root_dir = '/work/01329/poldrack/data/mriqc-net/data'\n",
    "orig_data_face = os.path.join(orig_root_dir, 'face/T1w')\n",
    "orig_data_mask = os.path.join(orig_root_dir, 'masks')\n",
    "\n",
    "save_root_dir = '/work/06850/sbansal6/maverick2/mriqc-shared/'\n",
    "\n",
    "save_preprocessing_face = os.path.join(save_root_dir, 'preprocessing/face')\n",
    "save_conformed_face = os.path.join(save_root_dir, 'conformed/face')\n",
    "\n",
    "save_preprocessing_deface = os.path.join(save_root_dir, 'preprocessing/deface')\n",
    "save_conformed_deface = os.path.join(save_root_dir, 'conformed/deface')\n",
    "\n",
    "os.makedirs(save_preprocessing_face, exist_ok=True)\n",
    "os.makedirs(save_preprocessing_deface, exist_ok=True)\n",
    "os.makedirs(save_conformed_face, exist_ok=True)\n",
    "os.makedirs(save_conformed_deface, exist_ok=True)\n",
    "\n",
    "conform_size = (64, 64, 64)\n",
    "conform_zoom = (4., 4., 4.)\n",
    "\n",
    "\n",
    "def preprocess(orig_vol_pth, conform_pth, preprocess_pth, DS=None, mask_path=None, debug=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    filename = orig_vol_pth.split('/')[-1]\n",
    "    volume, affine, _ = load_vol(orig_vol_pth)\n",
    "    \n",
    "    # Preprocessing\n",
    "    volume = clip(volume, q=90)\n",
    "    volume = normalize(volume)\n",
    "    volume = standardize(volume)\n",
    "    \n",
    "    # \n",
    "    save_preprocessing_path = os.path.join(preprocess_pth, filename)\n",
    "    save_conformed_path = os.path.join(conform_pth, filename)\n",
    "    \n",
    "    print(\"save_preprocessing_path: \", save_preprocessing_path)\n",
    "    print(\"save_conformed_path: \", save_conformed_path)\n",
    "    \n",
    "    save_vol(save_preprocessing_path, volume, affine)\n",
    "\n",
    "    def _plot(data):\n",
    "        f, axarr = plt.subplots(8, 8, figsize=(12, 12))\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                axarr[i, j].imshow(np.rot90(data[:, :, j + 8*i], 1))\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "#         \"\"\"\n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.imshow(np.rot90(np.mean(data, axis=0)))\n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(np.rot90(np.mean(data, axis=1)))\n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.imshow(np.rot90(np.mean(data, axis=2)))\n",
    "#         plt.show()\n",
    "#         \"\"\"\n",
    "    conform_data(save_preprocessing_path, \n",
    "                 out_file=save_conformed_path, \n",
    "                 out_size=conform_size, \n",
    "                 out_zooms=conform_zoom)\n",
    "\n",
    "#     if debug: _plot(load_vol(save_conformed_path)[0])\n",
    "    \n",
    "    if mask_path and DS:\n",
    "        mask = np.array(nib.load(mask_path).dataobj)\n",
    "        masked_volume = volume*mask\n",
    "\n",
    "        save_mpreprocessing_path = os.path.join(save_preprocessing_deface, DS, filename)\n",
    "        save_mconformed_path = os.path.join(save_conformed_deface, DS, filename)    \n",
    "        save_vol(save_mpreprocessing_path, masked_volume, affine)\n",
    "\n",
    "        conform_data(save_mpreprocessing_path, \n",
    "                 out_file=save_mconformed_path, \n",
    "                 out_size=conform_size, \n",
    "                 out_zooms=conform_zoom)        \n",
    "        return save_conformed_path, save_mconformed_path\n",
    "\n",
    "    return save_conformed_path\n",
    "\n",
    "\n",
    "def checkNonConformed(orig_path, save_path):\n",
    "\n",
    "    conform = []\n",
    "    orig = []\n",
    "\n",
    "    for path in glob(save_path + \"/*/*.nii*\"):\n",
    "        tempname = path.split(\"/\")[-1]\n",
    "        ds = path.split(\"/\")[-2]\n",
    "        conform.append(ds + \"/\" + tempname)\n",
    "\n",
    "    print(\"Total Conformed: \", len(conform))\n",
    "\n",
    "    for path in glob(orig_path + \"/*/*.nii*\"):\n",
    "        tempname = path.split(\"/\")[-1]\n",
    "        ds = path.split(\"/\")[-2]\n",
    "        orig.append(ds + \"/\" + tempname)\n",
    "\n",
    "    print(\"Total Original: \", len(orig))\n",
    "\n",
    "    print(\"Total not conformed: \", len(orig) - len(conform))\n",
    "\n",
    "    count = 0\n",
    "    for f in orig:\n",
    "        exists = False\n",
    "        for fc in conform:\n",
    "            if fc in f:\n",
    "                exists = True\n",
    "        if not exists:\n",
    "            count += 1\n",
    "            print(\"Not conformed file: \", f)\n",
    "\n",
    "\n",
    "for path in glob(orig_data_face + \"/*/*.nii*\"):\n",
    "#     try:\n",
    "    if 'ds000140_anat' not in path:\n",
    "        print(\"Orig Path: \", path)\n",
    "        # Example: \n",
    "        #        vol_name - sub-22_T1w.nii.gz\n",
    "        #        DS - ds000140_anat\n",
    "        vol_name = path.split(\"/\")[-1]\n",
    "        DS = path.split(\"/\")[-2]\n",
    "\n",
    "        # directories for saving preprocessed and conformed volumes\n",
    "        ds_save_conform_path = os.path.join(save_conformed_face, DS)\n",
    "        ds_save_preprocess_path = os.path.join(save_preprocessing_face, DS)\n",
    "\n",
    "        # Get the mask path\n",
    "        mask_path = glob(os.path.join(orig_data_mask, DS, vol_name.split('.')[0] + \"*_mask*\"))[0]\n",
    "        \n",
    "        print(\"Mask_path\", mask_path)\n",
    "\n",
    "        if not os.path.exists(ds_save_conform_path):\n",
    "            os.makedirs(ds_save_conform_path)\n",
    "\n",
    "        if not os.path.exists(ds_save_preprocess_path):\n",
    "            os.makedirs(ds_save_preprocess_path)\n",
    "\n",
    "        # Check if volume is a proper gunzipped\n",
    "        if not os.path.splitext(path)[1] == \".gz\" and is_gz_file(path):\n",
    "            rename_file = os.path.splitext(vol_name)[0]\n",
    "            fixed_gz_tmp = os.path.join(save_conformed_face, rename_file)\n",
    "            print(fixed_gz_tmp)\n",
    "            subprocess.call([\"cp\", path, fixed_gz_tmp])\n",
    "\n",
    "            print(preprocess(fixed_gz_tmp,\n",
    "                             ds_save_conform_path,\n",
    "                             ds_save_preprocess_path,\n",
    "                             mask_path = mask_path,\n",
    "                             DS=DS\n",
    "                            ))\n",
    "            os.remove(fixed_gz_tmp)\n",
    "\n",
    "        else:\n",
    "            print(preprocess(path,\n",
    "                             ds_save_conform_path,\n",
    "                             ds_save_preprocess_path,\n",
    "                             mask_path = mask_path,\n",
    "                             DS=DS\n",
    "                            ))\n",
    "#     except:\n",
    "#         print(\"Preprocessing incomplete. Exception occurred.\")\n",
    "#         pass\n",
    "\n",
    "\n",
    "# for path in glob(orig_data_deface + \"/*/*.nii*\"):\n",
    "#     try:\n",
    "#         print(\"Orig Path: \", path)\n",
    "#         if not is_gz_file(path) and os.path.splitext(path)[1] == \".gz\":\n",
    "#             tempname = path.split(\"/\")[-1]\n",
    "#             ds = path.split(\"/\")[-2]\n",
    "#             rename_file = os.path.splitext(tempname)[0]\n",
    "#             dst = os.path.join(save_data_deface, rename_file)\n",
    "# #             print(dst)\n",
    "#             subprocess.call([\"cp\", path, dst])\n",
    "#             ds_save_path = os.path.join(save_data_deface, ds)\n",
    "#             if not os.path.exists(ds_save_path):\n",
    "#                 os.makedirs(ds_save_path)\n",
    "#             preprocess(dst, conform_size, save_data_path=ds_save_path))\n",
    "#         else:\n",
    "#             ds = path.split(\"/\")[-2]\n",
    "#             ds_save_path = os.path.join(save_data_deface, ds)\n",
    "#             if not os.path.exists(ds_save_path):\n",
    "#                 os.makedirs(ds_save_path)\n",
    "#             preprocess(path, conform_size, save_data_path=ds_save_path)\n",
    "#     except:\n",
    "#         print(\"Preprocessing incomplete. Exception occurred.\")\n",
    "#         pass\n",
    "    \n",
    "\n",
    "# checkNonConformed(orig_data_face, save_data_face)\n",
    "# checkNonConformed(orig_data_deface, save_data_deface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImageFileError",
     "evalue": "Cannot work out file type of \"/work/01329/poldrack/data/mriqc-net/data/masks/ds000119_anat/sub-10_T1w_pydeface_mask.nii.gz\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImageFileError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c8a0734b9daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmask_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/work/01329/poldrack/data/mriqc-net/data/masks/ds000119_anat/sub-10_T1w_pydeface_mask.nii.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/06850/sbansal6/anaconda3/lib/python3.7/site-packages/nibabel/loadsave.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     raise ImageFileError('Cannot work out file type of \"%s\"' %\n\u001b[0;32m---> 56\u001b[0;31m                          filename)\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImageFileError\u001b[0m: Cannot work out file type of \"/work/01329/poldrack/data/mriqc-net/data/masks/ds000119_anat/sub-10_T1w_pydeface_mask.nii.gz\""
     ]
    }
   ],
   "source": [
    "mask_path = '/work/01329/poldrack/data/mriqc-net/data/masks/ds000119_anat/sub-10_T1w_pydeface_mask.nii.gz'\n",
    "\n",
    "mask = np.array(nib.load(mask_path).dataobj)\n",
    "print(mask)\n",
    "\n",
    "# print(\"is_gz_file: \", is_gz_file(mask_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_C = []\n",
    "face_O = []\n",
    "\n",
    "for path in glob(save_data_face + \"/*/*.nii*\"):\n",
    "    tempname = path.split(\"/\")[-1]\n",
    "    ds = path.split(\"/\")[-2]\n",
    "    face_C.append(ds + '/' + tempname)\n",
    "\n",
    "print(len(face_C))\n",
    "# print(face_C)\n",
    "\n",
    "\n",
    "for path in glob(orig_data_face + \"/*/*.nii*\"):\n",
    "    tempname = path.split(\"/\")[-1]\n",
    "    ds = path.split(\"/\")[-2]\n",
    "    face_O.append(ds + '/' + tempname)\n",
    "\n",
    "print(len(face_O))\n",
    "# print(face_O)\n",
    "\n",
    "count = 0\n",
    "for f in face_O:\n",
    "    exists = False\n",
    "    for fc in face_C:\n",
    "        if fc in f:\n",
    "            exists = True\n",
    "    if not exists:\n",
    "        count += 1\n",
    "        print(f)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "face_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face\"\n",
    "deface_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/deface\"\n",
    "\n",
    "paths = []\n",
    "labels = []\n",
    "\n",
    "for path in glob(deface_path + \"/*/*.nii*\"):\n",
    "    paths.append(path)\n",
    "    labels.append(0)\n",
    "\n",
    "for path in glob(face_path + \"/*/*.nii*\"):\n",
    "    paths.append(path)\n",
    "    labels.append(1)\n",
    "    \n",
    "print(len(paths))\n",
    "print(len(labels))\n",
    "\n",
    "save_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/csv\"\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"X\"] = paths\n",
    "df[\"Y\"] = labels\n",
    "df.to_csv(os.path.join(save_path, \"all.csv\"))\n",
    "\n",
    "SPLITS = 10\n",
    "skf = StratifiedKFold(n_splits=SPLITS)\n",
    "fold_no = 1\n",
    "\n",
    "for train_index, test_index in skf.split(paths, labels):\n",
    "    out_path = save_path + \"/train_test_fold_{}/csv/\".format(fold_no)\n",
    "\n",
    "    if not os.path.exists(out_path):\n",
    "        os.makedirs(out_path)\n",
    "\n",
    "    image_train, image_test = (\n",
    "        itemgetter(*train_index)(paths),\n",
    "        itemgetter(*test_index)(paths),\n",
    "    )\n",
    "    label_train, label_test = (\n",
    "        itemgetter(*train_index)(labels),\n",
    "        itemgetter(*test_index)(labels),\n",
    "    )\n",
    "\n",
    "    # image_train = [os.path.join(data_path, 'sub-' + str(pth) + '_T1w.nii.gz') for pth in image_train]\n",
    "    train_data = {\"X\": image_train, \"Y\": label_train}\n",
    "    df_train = pd.DataFrame(train_data)\n",
    "    df_train.to_csv(os.path.join(out_path, \"training.csv\"), index=False)\n",
    "\n",
    "    # image_test = [os.path.join(data_path, 'sub-' + str(pth) + '_T1w.nii.gz') for pth in image_test]\n",
    "    validation_data = {\"X\": image_test, \"Y\": label_test}\n",
    "    df_validation = pd.DataFrame(validation_data)\n",
    "    df_validation.to_csv(os.path.join(out_path, \"validation.csv\"), index=False)\n",
    "\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "SPLITS = 10\n",
    "\n",
    "for fold in range(1, SPLITS+1):\n",
    "    \n",
    "    dir_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/csv/train_test_fold_{}/csv/\".format(fold)\n",
    "    \n",
    "    tf_records_dir = \"/work/06850/sbansal6/maverick2/mriqc-shared/tfrecords/tfrecords_fold_{}/\".format(fold)\n",
    "    os.makedirs(tf_records_dir, exist_ok=True)\n",
    "    \n",
    "    train_csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "    valid_csv_path = os.path.join(dir_path, \"validation.csv\")\n",
    "    \n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "    train_D = list(zip(train_paths, train_labels))\n",
    "    random.shuffle(train_D)\n",
    "#     print(train_D[0])\n",
    "    \n",
    "    valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "    valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "    valid_D = list(zip(valid_paths, valid_labels))\n",
    "    random.shuffle(valid_D)\n",
    "    \n",
    "    train_write_path = os.path.join(tf_records_dir, 'data-train_shard-{shard:03d}.tfrec')\n",
    "    valid_write_path = os.path.join(tf_records_dir, 'data-valid_shard-{shard:03d}.tfrec')\n",
    "    \n",
    "    nobrainer.tfrecord.write(\n",
    "        features_labels=train_D,\n",
    "        filename_template=train_write_path,\n",
    "        examples_per_shard=3)\n",
    "    \n",
    "    nobrainer.tfrecord.write(\n",
    "        features_labels=valid_D,\n",
    "        filename_template=valid_write_path,\n",
    "        examples_per_shard=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nobrainer\n",
    "from nobrainer.io import _is_gzipped\n",
    "from nobrainer.volume import to_blocks\n",
    "\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/'\n",
    "\n",
    "# function to apply augmentations to tf dataset\n",
    "def apply_augmentations(features, labels):\n",
    "\n",
    "    \"\"\" Apply <TYPE_OF> augmentation to the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    #     iaa.SomeOf(\n",
    "    #             (0, 3),\n",
    "    #             [\n",
    "    #                 iaa.Fliplr(0.5),\n",
    "    #                 iaa.Flipud(0.5),\n",
    "    #                 iaa.Noop(),\n",
    "    #                 iaa.OneOf(\n",
    "    #                     [\n",
    "    #                         iaa.Affine(rotate=90),\n",
    "    #                         iaa.Affine(rotate=180),\n",
    "    #                         iaa.Affine(rotate=270),\n",
    "    #                     ]\n",
    "    #                 ),\n",
    "    #                 # iaa.GaussianBlur(sigma=(0.0, 0.2)),\n",
    "    #             ],\n",
    "    #         )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# from nobrainer.dataset\n",
    "def tfrecord_dataset(\n",
    "    file_pattern,\n",
    "    volume_shape,\n",
    "    shuffle,\n",
    "    scalar_label,\n",
    "    compressed=True,\n",
    "    num_parallel_calls=None,\n",
    "):\n",
    "    \"\"\"Return `tf.data.Dataset` from TFRecord files.\"\"\"\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=shuffle)\n",
    "    # Read each of these files as a TFRecordDataset.\n",
    "    # Assume all files have same compression type as the first file.\n",
    "    compression_type = \"GZIP\" if compressed else None\n",
    "    cycle_length = 1 if num_parallel_calls is None else num_parallel_calls\n",
    "    dataset = dataset.interleave(\n",
    "        map_func=lambda x: tf.data.TFRecordDataset(\n",
    "            x, compression_type=compression_type\n",
    "        ),\n",
    "        cycle_length=cycle_length,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "    )\n",
    "    parse_fn = parse_example_fn(volume_shape=volume_shape, scalar_label=scalar_label)\n",
    "    dataset = dataset.map(map_func=parse_fn, num_parallel_calls=num_parallel_calls)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# from nobrainer.tfrecord\n",
    "def parse_example_fn(volume_shape, scalar_label=False):\n",
    "    \"\"\"Return function that can be used to read TFRecord file into tensors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    volume_shape: sequence, the shape of the feature data. If `scalar_label` is `False`,\n",
    "        this also corresponds to the shape of the label data.\n",
    "    scalar_label: boolean, if `True`, label is a scalar. If `False`, label must be the\n",
    "        same shape as feature data.\n",
    "    Returns\n",
    "    -------\n",
    "    Function with which a TFRecord file can be parsed.\n",
    "    \"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def parse_example(serialized):\n",
    "        \"\"\"Parse one example from a TFRecord file made with Nobrainer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        serialized: str, serialized proto message.\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of two tensors. If `scalar_label` is `False`, both tensors have shape\n",
    "        `volume_shape`. Otherwise, the first tensor has shape `volume_shape`, and the\n",
    "        second is a scalar tensor.\n",
    "        \"\"\"\n",
    "        features = {\n",
    "            \"feature/shape\": tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "            \"feature/value\": tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "            \"label/value\": tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "            \"label/rank\": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "        }\n",
    "        e = tf.io.parse_single_example(serialized=serialized, features=features)\n",
    "        x = tf.io.decode_raw(e[\"feature/value\"], _TFRECORDS_DTYPE)\n",
    "        y = tf.io.decode_raw(e[\"label/value\"], _TFRECORDS_DTYPE)\n",
    "        # TODO: this line does not work. The shape cannot be determined\n",
    "        # dynamically... for now.\n",
    "        # xshape = tf.cast(\n",
    "        #     tf.io.decode_raw(e[\"feature/shape\"], _TFRECORDS_DTYPE), tf.int32)\n",
    "        x = tf.reshape(x, shape=volume_shape)\n",
    "        if not scalar_label:\n",
    "            y = tf.reshape(y, shape=volume_shape)\n",
    "        else:\n",
    "            y = tf.reshape(y, shape=[1])\n",
    "        return x, y\n",
    "\n",
    "    return parse_example\n",
    "\n",
    "\n",
    "def standardize(x, clip_value_min=-5000, clip_value_max=5000):\n",
    "    \"\"\"Standard score input tensor.\n",
    "    Implements `(x - mean(x)) / stdev(x)`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: tensor, values to standardize.\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of standardized values. Output has mean 0 and standard deviation 1.\n",
    "    \"\"\"\n",
    "    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "    x = tf.where(tf.math.is_inf(x), tf.zeros_like(x), x)\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    if x.dtype != tf.float32:\n",
    "        x = tf.cast(x, tf.float32)\n",
    "    # x = tf.clip_by_value(\n",
    "    #     x, \n",
    "    #    clip_value_min, \n",
    "    #    clip_value_max, name=None\n",
    "    #    )\n",
    "    \n",
    "    mean, var = tf.nn.moments(x, axes=None)\n",
    "    std = tf.sqrt(var)\n",
    "    return (x - mean) / std\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"Standard score input tensor.\n",
    "    Implements `(x - mean(x)) / stdev(x)`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: tensor, values to standardize.\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of standardized values. Output has mean 0 and standard deviation 1.\n",
    "    \"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    if x.dtype != tf.float32:\n",
    "        x = tf.cast(x, tf.float32)\n",
    "\n",
    "    max_value = tf.math.reduce_max(\n",
    "                x, \n",
    "                axis=None, \n",
    "                keepdims=False, name=None\n",
    "                )\n",
    "\n",
    "    min_value = tf.math.reduce_min(\n",
    "                x, \n",
    "                axis=None, \n",
    "                keepdims=False, name=None\n",
    "                )\n",
    "    return (x - min_value) / (max_value - min_value + 1e-3)\n",
    "\n",
    "def get_dataset(\n",
    "    file_pattern,\n",
    "    n_classes,\n",
    "    batch_size,\n",
    "    volume_shape,\n",
    "    plane,\n",
    "    n = 4,\n",
    "    block_shape=None,\n",
    "    n_epochs=None,\n",
    "    mapping=None,\n",
    "    augment=False,\n",
    "    shuffle_buffer_size=None,\n",
    "    num_parallel_calls=AUTOTUNE,\n",
    "):\n",
    "\n",
    "    \"\"\" Returns tf.data.Dataset after preprocessing from \n",
    "    tfrecords for training and validation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_pattern:\n",
    "    \n",
    "    n_classes:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    files = glob.glob(file_pattern)\n",
    "\n",
    "    if not files:\n",
    "        raise ValueError(\"no files found for pattern '{}'\".format(file_pattern))\n",
    "\n",
    "    compressed = _is_gzipped(files[0])\n",
    "    shuffle = bool(shuffle_buffer_size)\n",
    "\n",
    "    ds = nobrainer.dataset.tfrecord_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        volume_shape=volume_shape,\n",
    "        shuffle=shuffle,\n",
    "        scalar_label=True,\n",
    "        compressed=compressed,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "    )\n",
    "\n",
    "    if augment:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: tf.cond(\n",
    "                tf.random.uniform((1,)) > 0.5,\n",
    "                true_fn=lambda: apply_augmentations(x, y),\n",
    "                false_fn=lambda: (x, y),\n",
    "            ),\n",
    "            num_parallel_calls=num_parallel_calls,\n",
    "        )\n",
    "\n",
    "    def _ss(x, y):\n",
    "        x, y = structural_slice(x, y, plane, n)\n",
    "        return (x, y)\n",
    "\n",
    "    ds = ds.map(_ss, num_parallel_calls)\n",
    "\n",
    "    #     def _f(x, y):\n",
    "    #         x = to_blocks(x, block_shape)\n",
    "    #         n_blocks = x.shape[0]\n",
    "    #         y = tf.repeat(y, n_blocks)\n",
    "    #         return (x, y)\n",
    "    #     ds = ds.map(_f, num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # This step is necessary because it reduces the extra dimension.\n",
    "    # ds = ds.unbatch()\n",
    "\n",
    "    # add a single dimension at the end\n",
    "    # ds = ds.map(lambda x, y: (tf.expand_dims(x, -1), y))\n",
    "\n",
    "    ds = ds.prefetch(buffer_size=batch_size)\n",
    "    \n",
    "    if batch_size is not None:\n",
    "        ds = ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        ds = ds.map(lambda x,y: (tf.reshape\n",
    "                                 (x,\n",
    "                                  ((3, batch_size*n,)\n",
    "                                   if plane == \"combined\"\n",
    "                                   else (batch_size*n,)\n",
    "                                  ) + volume_shape[:2] +(1,)), \n",
    "                                 tf.reshape(y, (batch_size*n,))))\n",
    "\n",
    "    if shuffle_buffer_size:\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat the dataset n_epochs times\n",
    "#     ds = ds.repeat(n_epochs)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def structural_slice(x, y, plane, n=4):\n",
    "\n",
    "    \"\"\" Transpose dataset based on the plane\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "    \n",
    "    y:\n",
    "    \n",
    "    plane:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    options = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "    shape = np.array(x.shape)\n",
    "    x = normalize(standardize(x))\n",
    "    \n",
    "#     print(tf.math.reduce_max(\n",
    "#                 x,\n",
    "#                 axis=None,\n",
    "#                 keepdims=False, name=None\n",
    "#                 ), \n",
    "#           tf.math.reduce_min(\n",
    "#                 x,\n",
    "#                 axis=None,\n",
    "#                 keepdims=False, name=None\n",
    "#                 )\n",
    "#          )\n",
    "    \n",
    "    \n",
    "    if isinstance(plane, str) and plane in options:\n",
    "        if plane == \"axial\":\n",
    "            idx = np.random.randint(2*shape[0]//5, 3*shape[0]//5, n)\n",
    "            x = x\n",
    "\n",
    "        if plane == \"coronal\":\n",
    "            idx = np.random.randint(2*shape[1]//4, 3*shape[1]//4, n)\n",
    "            x = tf.transpose(x, perm=[1, 2, 0])\n",
    "\n",
    "        if plane == \"sagittal\":\n",
    "            idx = np.random.randint(shape[2]//3, 2*shape[2]//3, n)\n",
    "            x = tf.transpose(x, perm=[2, 0, 1])\n",
    "\n",
    "        if plane == \"combined\":\n",
    "            temp = []\n",
    "            for op in options[:-1]:\n",
    "                temp.append(tf.expand_dims(structural_slice(x, y, op, n)[0], axis=1))\n",
    "            x = tf.concat(temp, axis = 1)\n",
    "\n",
    "        if not plane == \"combined\":\n",
    "            x = tf.squeeze(tf.gather_nd(x, idx.reshape(n, 1, 1)), axis=1)\n",
    "        \n",
    "#         print(x)\n",
    "        \n",
    "        x = tf.convert_to_tensor(x)\n",
    "        y = tf.repeat(y, n)\n",
    "        \n",
    "        return x, y\n",
    "    else:\n",
    "        raise ValueError(\"expected plane to be one of ['axial', 'coronal', 'sagittal']\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    n_classes = 2\n",
    "    global_batch_size = 8\n",
    "    volume_shape = (64, 64, 64)\n",
    "    ds = get_dataset(\n",
    "        ROOTDIR + \"tfrecords_no_ds001985/tfrecords_fold_1/data-train_*\",\n",
    "        n_classes=n_classes,\n",
    "        batch_size=global_batch_size,\n",
    "        volume_shape=volume_shape,\n",
    "        plane=\"combined\",\n",
    "        shuffle_buffer_size=10,\n",
    "    )\n",
    "    \n",
    "    print(ds)\n",
    "    \n",
    "    temp = len(list(ds.as_numpy_iterator()))\n",
    "    \n",
    "    \n",
    "#     for _ in range(100):\n",
    "#         x,y=next(ds.as_numpy_iterator())\n",
    "#         print (np.min(x), np.max(x), np.unique(y))\n",
    "#         if np.max(x) == np.nan:\n",
    "#             print(x)\n",
    "            \n",
    "        \n",
    "#     print (y)\n",
    "#     import matplotlib \n",
    "#     matplotlib.use('Agg')\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "#     rows = 5\n",
    "#     count = 1\n",
    "#     idx = np.random.randint(0, 32, rows)\n",
    "#     for i in range(rows):\n",
    "#         for j in range(3):\n",
    "#             plt.subplot(rows, 3, count)\n",
    "#             plt.imshow(x[idx[i], j, :, :, 0])\n",
    "#             plt.title(str(y[idx[i]]))\n",
    "#             count += 1\n",
    "#     plt.savefig(\"processed_image.png\")\n",
    "    \n",
    "# dataset_train_coronal = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "#                             n_classes=n_classes,\n",
    "#                             batch_size=global_batch_size,\n",
    "#                             volume_shape=volume_shape,\n",
    "#                             block_shape=block_shape,\n",
    "#                             plane='coronal',\n",
    "#                             shuffle_buffer_size=3)\n",
    "\n",
    "# dataset_train_sagittal = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "#                             n_classes=n_classes,\n",
    "#                             batch_size=global_batch_size,\n",
    "#                             volume_shape=volume_shape,\n",
    "#                             block_shape=block_shape,\n",
    "#                             plane='sagittal',\n",
    "#                             shuffle_buffer_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_train_axial)\n",
    "print(dataset_train_coronal)\n",
    "print(dataset_train_sagittal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "def get_steps_per_epoch(n_volumes, volume_shape, block_shape, batch_size):\n",
    "    def get_n(a, k):\n",
    "        return (a - k) / k + 1\n",
    "\n",
    "    n_blocks = tuple(get_n(aa, kk) for aa, kk in zip(volume_shape, block_shape))\n",
    "\n",
    "    for n in n_blocks:\n",
    "        if not n.is_integer() or n < 1:\n",
    "            raise ValueError(\n",
    "                \"cannot create non-overlapping blocks with the given parameters.\"\n",
    "            )\n",
    "    n_blocks_per_volume = np.prod(n_blocks).astype(int)\n",
    "\n",
    "    steps = n_blocks_per_volume * n_volumes / batch_size\n",
    "    steps = math.ceil(steps)\n",
    "    return steps\n",
    "\n",
    "n_volumes / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std packages\n",
    "import sys, os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Custom packages\n",
    "from models import modelN\n",
    "\n",
    "# Tf packages\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler,\n",
    "    TensorBoard,\n",
    ")\n",
    "import nobrainer\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/'\n",
    "\n",
    "def train(\n",
    "    volume_shape=(64, 64, 64),\n",
    "    image_size=(64, 64),\n",
    "    dropout=0.4,\n",
    "    batch_size=4,\n",
    "    n_classes=2,\n",
    "    n_epochs=30,\n",
    "):\n",
    "\n",
    "    tpaths = glob.glob(ROOTDIR+\"tfrecords_no_ds001985/tfrecords_fold_1/data-train_*\")\n",
    "    vpaths = glob.glob(ROOTDIR+\"tfrecords_no_ds001985/tfrecords_fold_1/data-valid_*\")\n",
    "    \n",
    "    print(\"Total Training data: \", len(tpaths))\n",
    "    print(\"Total Validation data: \", len(vpaths))\n",
    "\n",
    "    planes = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "\n",
    "#     strategy = tf.distribute.MirroredStrategy()\n",
    "#     BATCH_SIZE_PER_REPLICA = batch_size\n",
    "#     global_batch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "    \n",
    "    global_batch_size = batch_size\n",
    "\n",
    "    model_save_path = \"./model_save_dir\"\n",
    "\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    cp_save_path = os.path.join(model_save_path, \"weights\")\n",
    "\n",
    "    logdir_path = os.path.join(model_save_path, \"tb_logs\")\n",
    "    if not os.path.exists(logdir_path):\n",
    "        os.makedirs(logdir_path)\n",
    "\n",
    "\n",
    "    for plane in planes:\n",
    "\n",
    "        logdir = os.path.join(logdir_path, plane)\n",
    "        os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "        tbCallback = TensorBoard(\n",
    "            log_dir=logdir, histogram_freq=0, write_graph=True, write_images=False,\n",
    "        )\n",
    "\n",
    "        os.makedirs(os.path.join(cp_save_path, plane), exist_ok=True)\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            os.path.join(cp_save_path, plane, \"best-wts.h5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "\n",
    "#         with strategy.scope():\n",
    "\n",
    "        if not plane == \"combined\": \n",
    "            lr = 1e-4\n",
    "            model = modelN.Submodel(\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                name=plane,\n",
    "                include_top=True,\n",
    "                weights=None,\n",
    "            )\n",
    "        else:\n",
    "            lr = 5e-5\n",
    "            model = modelN.CombinedClassifier(\n",
    "                input_shape=image_size, \n",
    "                dropout=dropout, \n",
    "                wts_root=cp_save_path\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        print(\"Submodel: \", plane)\n",
    "        print(model.summary())\n",
    "\n",
    "        METRICS = [\n",
    "            metrics.TruePositives(name=\"tp\"),\n",
    "            metrics.FalsePositives(name=\"fp\"),\n",
    "            metrics.TrueNegatives(name=\"tn\"),\n",
    "            metrics.FalseNegatives(name=\"fn\"),\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            #metrics.Precision(name=\"precision\"),\n",
    "            #metrics.Recall(name=\"recall\"),\n",
    "            #metrics.AUC(name=\"auc\"),\n",
    "        ]\n",
    "\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.binary_crossentropy,\n",
    "            optimizer=\"adam\",\n",
    "            metrics=METRICS,\n",
    "        )\n",
    "\n",
    "        print(\"GLOBAL BATCH SIZE: \", global_batch_size)\n",
    "        \n",
    "        \n",
    "        dataset_train = get_dataset(\n",
    "            ROOTDIR + \"tfrecords_no_ds001985/tfrecords_fold_1/data-train_*\",\n",
    "            n_classes=n_classes,\n",
    "            batch_size=global_batch_size,\n",
    "            volume_shape=volume_shape,\n",
    "            plane=plane,\n",
    "            shuffle_buffer_size=global_batch_size,\n",
    "        )\n",
    "        \n",
    "        print(dataset_train)\n",
    "        \n",
    "        dataset_valid = get_dataset(\n",
    "            ROOTDIR + \"tfrecords_no_ds001985/tfrecords_fold_1/data-valid_*\",\n",
    "            n_classes=n_classes,\n",
    "            batch_size=global_batch_size,\n",
    "            volume_shape=volume_shape,\n",
    "            plane=plane,\n",
    "            shuffle_buffer_size=global_batch_size,\n",
    "        )\n",
    "        \n",
    "        print(dataset_valid)\n",
    "        \n",
    "        steps_per_epoch = math.ceil(len(tpaths) / batch_size)\n",
    "        \n",
    "        validation_steps = math.ceil(len(vpaths) / batch_size)\n",
    "\n",
    "        print(steps_per_epoch, validation_steps)\n",
    "\n",
    "        model.fit(\n",
    "            dataset_train,\n",
    "            epochs=n_epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=dataset_valid,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=[tbCallback, model_checkpoint],\n",
    "        )\n",
    "\n",
    "        del model\n",
    "        K.clear_session()\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nobrainer\n",
    "from nobrainer.io import _is_gzipped\n",
    "from nobrainer.volume import to_blocks\n",
    "\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/'\n",
    "\n",
    "# function to apply augmentations to tf dataset\n",
    "def apply_augmentations(features, labels):\n",
    "\n",
    "    \"\"\" Apply <TYPE_OF> augmentation to the dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    #     iaa.SomeOf(\n",
    "    #             (0, 3),\n",
    "    #             [\n",
    "    #                 iaa.Fliplr(0.5),\n",
    "    #                 iaa.Flipud(0.5),\n",
    "    #                 iaa.Noop(),\n",
    "    #                 iaa.OneOf(\n",
    "    #                     [\n",
    "    #                         iaa.Affine(rotate=90),\n",
    "    #                         iaa.Affine(rotate=180),\n",
    "    #                         iaa.Affine(rotate=270),\n",
    "    #                     ]\n",
    "    #                 ),\n",
    "    #                 # iaa.GaussianBlur(sigma=(0.0, 0.2)),\n",
    "    #             ],\n",
    "    #         )\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# from nobrainer.dataset\n",
    "def tfrecord_dataset(\n",
    "    file_pattern,\n",
    "    volume_shape,\n",
    "    shuffle,\n",
    "    scalar_label,\n",
    "    compressed=True,\n",
    "    num_parallel_calls=None,\n",
    "):\n",
    "    \"\"\"Return `tf.data.Dataset` from TFRecord files.\"\"\"\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(file_pattern, shuffle=shuffle)\n",
    "    # Read each of these files as a TFRecordDataset.\n",
    "    # Assume all files have same compression type as the first file.\n",
    "    compression_type = \"GZIP\" if compressed else None\n",
    "    cycle_length = 1 if num_parallel_calls is None else num_parallel_calls\n",
    "    dataset = dataset.interleave(\n",
    "        map_func=lambda x: tf.data.TFRecordDataset(\n",
    "            x, compression_type=compression_type\n",
    "        ),\n",
    "        cycle_length=cycle_length,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "    )\n",
    "    parse_fn = parse_example_fn(volume_shape=volume_shape, scalar_label=scalar_label)\n",
    "    dataset = dataset.map(map_func=parse_fn, num_parallel_calls=num_parallel_calls)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# from nobrainer.tfrecord\n",
    "def parse_example_fn(volume_shape, scalar_label=False):\n",
    "    \"\"\"Return function that can be used to read TFRecord file into tensors.\n",
    "    Parameters\n",
    "    ----------\n",
    "    volume_shape: sequence, the shape of the feature data. If `scalar_label` is `False`,\n",
    "        this also corresponds to the shape of the label data.\n",
    "    scalar_label: boolean, if `True`, label is a scalar. If `False`, label must be the\n",
    "        same shape as feature data.\n",
    "    Returns\n",
    "    -------\n",
    "    Function with which a TFRecord file can be parsed.\n",
    "    \"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def parse_example(serialized):\n",
    "        \"\"\"Parse one example from a TFRecord file made with Nobrainer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        serialized: str, serialized proto message.\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple of two tensors. If `scalar_label` is `False`, both tensors have shape\n",
    "        `volume_shape`. Otherwise, the first tensor has shape `volume_shape`, and the\n",
    "        second is a scalar tensor.\n",
    "        \"\"\"\n",
    "        features = {\n",
    "            \"feature/shape\": tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "            \"feature/value\": tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "            \"label/value\": tf.io.FixedLenFeature(shape=[], dtype=tf.string),\n",
    "            \"label/rank\": tf.io.FixedLenFeature(shape=[], dtype=tf.int64),\n",
    "        }\n",
    "        e = tf.io.parse_single_example(serialized=serialized, features=features)\n",
    "        x = tf.io.decode_raw(e[\"feature/value\"], _TFRECORDS_DTYPE)\n",
    "        y = tf.io.decode_raw(e[\"label/value\"], _TFRECORDS_DTYPE)\n",
    "        # TODO: this line does not work. The shape cannot be determined\n",
    "        # dynamically... for now.\n",
    "        # xshape = tf.cast(\n",
    "        #     tf.io.decode_raw(e[\"feature/shape\"], _TFRECORDS_DTYPE), tf.int32)\n",
    "        x = tf.reshape(x, shape=volume_shape)\n",
    "        if not scalar_label:\n",
    "            y = tf.reshape(y, shape=volume_shape)\n",
    "        else:\n",
    "            y = tf.reshape(y, shape=[1])\n",
    "        return x, y\n",
    "\n",
    "    return parse_example\n",
    "\n",
    "\n",
    "def standardize(x, clip_value_min=-5000, clip_value_max=5000):\n",
    "    \"\"\"Standard score input tensor.\n",
    "    Implements `(x - mean(x)) / stdev(x)`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: tensor, values to standardize.\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of standardized values. Output has mean 0 and standard deviation 1.\n",
    "    \"\"\"\n",
    "    x = tf.where(tf.math.is_nan(x), tf.zeros_like(x), x)\n",
    "    x = tf.where(tf.math.is_inf(x), tf.zeros_like(x), x)\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    if x.dtype != tf.float32:\n",
    "        x = tf.cast(x, tf.float32)\n",
    "    # x = tf.clip_by_value(\n",
    "    #     x, \n",
    "    #    clip_value_min, \n",
    "    #    clip_value_max, name=None\n",
    "    #    )\n",
    "    \n",
    "    mean, var = tf.nn.moments(x, axes=None)\n",
    "    std = tf.sqrt(var)\n",
    "    return (x - mean) / std\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"Standard score input tensor.\n",
    "    Implements `(x - mean(x)) / stdev(x)`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: tensor, values to standardize.\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of standardized values. Output has mean 0 and standard deviation 1.\n",
    "    \"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    if x.dtype != tf.float32:\n",
    "        x = tf.cast(x, tf.float32)\n",
    "\n",
    "    max_value = tf.math.reduce_max(\n",
    "                x, \n",
    "                axis=None, \n",
    "                keepdims=False, name=None\n",
    "                )\n",
    "\n",
    "    min_value = tf.math.reduce_min(\n",
    "                x, \n",
    "                axis=None, \n",
    "                keepdims=False, name=None\n",
    "                )\n",
    "    return (x - min_value) / (max_value - min_value + 1e-3)\n",
    "\n",
    "def get_dataset(\n",
    "    file_pattern,\n",
    "    n_classes,\n",
    "    batch_size,\n",
    "    volume_shape,\n",
    "    plane,\n",
    "    n = 4,\n",
    "    block_shape=None,\n",
    "    n_epochs=None,\n",
    "    mapping=None,\n",
    "    augment=False,\n",
    "    shuffle_buffer_size=None,\n",
    "    num_parallel_calls=AUTOTUNE,\n",
    "):\n",
    "\n",
    "    \"\"\" Returns tf.data.Dataset after preprocessing from \n",
    "    tfrecords for training and validation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_pattern:\n",
    "    \n",
    "    n_classes:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    files = glob.glob(file_pattern)\n",
    "\n",
    "    if not files:\n",
    "        raise ValueError(\"no files found for pattern '{}'\".format(file_pattern))\n",
    "\n",
    "    compressed = _is_gzipped(files[0])\n",
    "    shuffle = bool(shuffle_buffer_size)\n",
    "\n",
    "    ds = nobrainer.dataset.tfrecord_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        volume_shape=volume_shape,\n",
    "        shuffle=shuffle,\n",
    "        scalar_label=True,\n",
    "        compressed=compressed,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "    )\n",
    "\n",
    "    if augment:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: tf.cond(\n",
    "                tf.random.uniform((1,)) > 0.5,\n",
    "                true_fn=lambda: apply_augmentations(x, y),\n",
    "                false_fn=lambda: (x, y),\n",
    "            ),\n",
    "            num_parallel_calls=num_parallel_calls,\n",
    "        )\n",
    "\n",
    "    def _ss(x, y):\n",
    "        x, y = structural_slice(x, y, plane, n)\n",
    "        return (x, y)\n",
    "\n",
    "    ds = ds.map(_ss, num_parallel_calls)\n",
    "\n",
    "    #     def _f(x, y):\n",
    "    #         x = to_blocks(x, block_shape)\n",
    "    #         n_blocks = x.shape[0]\n",
    "    #         y = tf.repeat(y, n_blocks)\n",
    "    #         return (x, y)\n",
    "    #     ds = ds.map(_f, num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # This step is necessary because it reduces the extra dimension.\n",
    "    # ds = ds.unbatch()\n",
    "\n",
    "    # add a single dimension at the end\n",
    "    # ds = ds.map(lambda x, y: (tf.expand_dims(x, -1), y))\n",
    "\n",
    "    ds = ds.prefetch(buffer_size=batch_size)\n",
    "    def reshape(x,y):\n",
    "        if plane == \"combined\":\n",
    "            for _ in 3:\n",
    "                pass\n",
    "        return (x, y)\n",
    "    if batch_size is not None:\n",
    "        ds = ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        # ds = ds.map(lambda x,y: (tf.reshape(x, ((3, batch_size*n,) if plane == \"combined\" else (batch_size*n,)) + volume_shape[:2] +(1,)), \n",
    "        #                         tf.reshape(y, (batch_size*n,))))\n",
    "\n",
    "    if shuffle_buffer_size:\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat the dataset n_epochs times\n",
    "    ds = ds.repeat(n_epochs)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def structural_slice(x, y, plane, n=4):\n",
    "\n",
    "    \"\"\" Transpose dataset based on the plane\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "    \n",
    "    y:\n",
    "    \n",
    "    plane:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    options = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "    shape = np.array(x.shape)\n",
    "    x = normalize(standardize(x))\n",
    "    \n",
    "    if isinstance(plane, str) and plane in options:\n",
    "        if plane == \"axial\":\n",
    "            idx = np.random.randint(2*shape[0]//5, 3*shape[0]//5)\n",
    "            x = x\n",
    "\n",
    "        if plane == \"coronal\":\n",
    "            idx = np.random.randint(2*shape[1]//4, 3*shape[1]//4)\n",
    "            x = tf.transpose(x, perm=[1, 2, 0])\n",
    "\n",
    "        if plane == \"sagittal\":\n",
    "            idx = np.random.randint(shape[2]//3, 2*shape[2]//3)\n",
    "            x = tf.transpose(x, perm=[2, 0, 1])\n",
    "\n",
    "        if plane == \"combined\":\n",
    "            temp = {}\n",
    "            for op in options[:-1]:\n",
    "                temp[op] = structural_slice(x, y, op, n)[0]\n",
    "            x = temp\n",
    "\n",
    "        if not plane == \"combined\": \n",
    "            # x = tf.squeeze(tf.gather_nd(x, idx.reshape(n, 1, 1)), axis=1)\n",
    "            x = tf.convert_to_tensor(tf.expand_dims(x[idx], axis=-1))\n",
    "        # y = tf.repeat(y, n)\n",
    "        \n",
    "        return x, y\n",
    "    else:\n",
    "        raise ValueError(\"expected plane to be one of ['axial', 'coronal', 'sagittal']\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    n_classes = 2\n",
    "    global_batch_size = 8\n",
    "    volume_shape = (64, 64, 64)\n",
    "    ds = get_dataset(\n",
    "        ROOTDIR + \"tfrecords_no_ds001985/tfrecords_fold_1/data-train_*\",\n",
    "        n_classes=n_classes,\n",
    "        batch_size=global_batch_size,\n",
    "        volume_shape=volume_shape,\n",
    "        plane=\"axial\",\n",
    "        shuffle_buffer_size=3,\n",
    "    )\n",
    "\n",
    "    print(ds)\n",
    "#     for _ in range(100):\n",
    "#         x,y=next(ds.as_numpy_iterator())\n",
    "#         print (np.min(x), np.max(x), np.unique(y))\n",
    "#         if np.max(x) == np.nan:\n",
    "#             print(x)\n",
    "#     print (y)\n",
    "    import matplotlib \n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    for _ in range(100):\n",
    "        x,y=next(ds.as_numpy_iterator())\n",
    "        rows = 5\n",
    "        count = 1\n",
    "        plt.clf()\n",
    "        idx = np.random.randint(0, 8, rows)\n",
    "        for i in range(rows):\n",
    "            for j in range(3):\n",
    "                plt.subplot(rows, 3, count)\n",
    "                plt.imshow(x[idx[i], :, :, 0])\n",
    "                plt.title(str(y[idx[i]]))\n",
    "                count += 1\n",
    "            plt.show()\n",
    "            plt.savefig(\"processed_image_{}_{}.png\".format(i,j))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
