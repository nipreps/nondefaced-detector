{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import nobrainer\n",
    "from nobrainer import dataset, volume\n",
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "dir_path = os.path.abspath(\"csv/faced_defaced/train_test_fold_1/csv/\")\n",
    "csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "# print(dir_path, csv_path)\n",
    "\n",
    "\n",
    "labels = pd.read_csv(csv_path)[\"Y\"].values\n",
    "paths = pd.read_csv(csv_path)[\"X\"].values\n",
    "\n",
    "\n",
    "# print(labels)\n",
    "\n",
    "n_classes=2\n",
    "volume_shape = (256, 256, 256)\n",
    "block_shape = (128, 128, 128)\n",
    "\n",
    "\n",
    "training_paths = zip(paths, labels)\n",
    "\n",
    "print(training_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nobrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reface Script (parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "orig_root_dir = '/work/01329/poldrack/data/mriqc-net/data'\n",
    "orig_face_defaced = os.path.join(orig_root_dir, 'masks')\n",
    "orig_data_deface = os.path.join(orig_root_dir, 'defaced')\n",
    "\n",
    "orig_reface_dir = '/work/06850/sbansal6/maverick2/mriqc-shared/refaced'\n",
    "save_orig_faced_root_dir = os.path.join(orig_reface_dir, 'orig_faced')\n",
    "save_orig_defaced_root_dir = os.path.join(orig_reface_dir, 'orig_defaced')\n",
    "\n",
    "simg_path = os.path.join(orig_reface_dir,'afni-latest.simg')\n",
    "print(simg_path)\n",
    "exclude = ['ds002033_anat', 'ds002149_anat', 'ds001928_anat', \n",
    "           'ds002247_anat', 'ds002001_anat', 'ds002316_anat',\n",
    "           'ds002076_anat', 'ds002156_anat']\n",
    "\n",
    "ds_root_paths = glob.glob(orig_face_defaced + '/*_anat*')\n",
    "\n",
    "processes = set()\n",
    "max_processes = 14\n",
    "    \n",
    "\n",
    "# for data_dir in glob.glob(orig_data_deface + '/*_anat*'):\n",
    "#     print(\"datadir: \", data_dir)\n",
    "#     dd = data_dir.split('/')[-1]\n",
    "#     save_dd_dir = os.path.join(save_orig_faced_root_dir, dd)\n",
    "#     os.makedirs(save_dd_dir, exist_ok=True)\n",
    "#     ds = glob.glob(data_dir)\n",
    "    \n",
    "#     for vol in ds:\n",
    "#         vol_name = vol.split('/')[-1].split('.')[0]\n",
    "#         vol_save_path = os.path.join(save_dd_dir, vol_name)\n",
    "#         os.makedirs(vol_save_path, exist_ok=True)\n",
    "#         prefix_pre = vol_save_path\n",
    "#         prefix = os.path.join(prefix_pre, vol_name + '_refaced.nii.gz')\n",
    "#         print(\"Processing Volume: \", vol)\n",
    "#         print(\"Save Path: \", prefix)        \n",
    "#         processes.add(subprocess.Popen([\"singularity\",\"exec\", simg_path, \"@afni_refacer_run\",\n",
    "#                                         \"-input\", vol,\n",
    "#                                         \"-mode_reface_plus\",\n",
    "#                                         \"-prefix\", prefix\n",
    "#                                        ]))\n",
    "        \n",
    "#         if len(processes) >= max_processes:\n",
    "#             os.wait()\n",
    "#             processes.difference_update([\n",
    "#                 p for p in processes if p.poll() is not None])\n",
    "\n",
    "#     print(ds)\n",
    "# ds_paths = glob.glob(orig_face_defaced + '*_defaced.nii*')\n",
    "\n",
    "# ds_paths = glob.glob(orig_data_deface + '/*ds*anat*')\n",
    "\n",
    "## DEFACE REFACE CODE\n",
    "count = 0\n",
    "for vol_path in glob.glob(orig_data_deface + '/*/*.nii*'):\n",
    "    vol = vol_path.split('/')[-1]\n",
    "    DS = vol_path.split('/')[-2]\n",
    "    \n",
    "    save_vol_name = vol.split('.')[-0]\n",
    "    vol_save_path = os.path.join(save_orig_defaced_root_dir, DS, save_vol_name)\n",
    "    os.makedirs(vol_save_path, exist_ok=True)\n",
    "    prefix = os.path.join(vol_save_path, save_vol_name + '_refaced.nii.gz')\n",
    "    if not os.path.isfile(prefix):\n",
    "        count += 1\n",
    "        print(prefix)\n",
    "    \n",
    "print(count)\n",
    "#         print(\"Processing Volume: \", vol_path)\n",
    "#         print(\"Save Path: \", prefix)\n",
    "        \n",
    "#         processes.add(subprocess.Popen([\n",
    "#             \"singularity\",\"exec\", simg_path, \"@afni_refacer_run\",\n",
    "#             \"-input\", vol_path,\n",
    "#             \"-mode_reface_plus\",\n",
    "#             \"-prefix\", prefix]))\n",
    "        \n",
    "#         if len(processes) >= max_processes:\n",
    "#             os.wait()\n",
    "#             processes.difference_update([p for p in processes if p.poll() is not None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deface script (Parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/ixi_dataset'\n",
    "faced_path = os.path.join(ROOTDIR, 'T1w')\n",
    "defaced_path = os.path.join(ROOTDIR, 'T1w_defaced')\n",
    "\n",
    "processes = set()\n",
    "max_processes = 12\n",
    "\n",
    "for vol_path in glob.glob(faced_path + '/*nii*'):\n",
    "    vol = vol_path.split('/')[-1]\n",
    "    print(vol)\n",
    "    vol_pre = vol.split('.')[0]\n",
    "    outpath = os.path.join(defaced_path, vol_pre + '_defaced.' + vol.split('.', 1)[1])\n",
    "    \n",
    "    processes.add(subprocess.Popen([\"pydeface\", vol_path,\n",
    "                                    \"--outfile\", outpath\n",
    "                                   ]))\n",
    "    if len(processes) >= max_processes:\n",
    "        os.wait()\n",
    "        processes.difference_update([\n",
    "            p for p in processes if p.poll() is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "orig_root_dir = '/work/01329/poldrack/data/mriqc-net/data'\n",
    "orig_face_defaced = os.path.join(orig_root_dir, 'masks')\n",
    "\n",
    "orig_reface_dir = '/work/06850/sbansal6/maverick2/mriqc-shared/refaced'\n",
    "save_orig_faced_root_dir = os.path.join(orig_reface_dir, 'orig_faced')\n",
    "\n",
    "simg_path = os.path.join(orig_reface_dir,'afni-latest.simg')\n",
    "print(simg_path)\n",
    "\n",
    "# list of datasets already refaced from the original defaced datasets\n",
    "deface_exclude = [\n",
    "    'ds002033_anat',\n",
    "    'ds002149_anat',\n",
    "    'ds001928_anat', \n",
    "    'ds002247_anat',\n",
    "    'ds002001_anat',\n",
    "    'ds002316_anat',\n",
    "    'ds002076_anat',\n",
    "    'ds002156_anat'\n",
    "]\n",
    "\n",
    "# list of datasets already refaced from the orignal faced datasets\n",
    "# list of datasets already refaced from the orignal faced datasets\n",
    "face_exclude = [\n",
    "    'ds000140_anat',\n",
    "    'ds000119_anat',\n",
    "    'ds000157_anat',\n",
    "    'ds002509_anat',\n",
    "    'ds001650_anat',\n",
    "    'ds000208_anat',\n",
    "    'ds001569_anat'\n",
    "]\n",
    "\n",
    "face_parallel = [\n",
    "    'ds000157_anat',\n",
    "    'ds000205_anat',\n",
    "    'ds000232_anat',\n",
    "    'ds001019_anat',\n",
    "    'ds001393_anat',\n",
    "    'ds001568_anat',\n",
    "    'ds001650_anat',\n",
    "    'ds002572_anat',\n",
    "    'ds000118_anat',\n",
    "    'ds000140_anat',\n",
    "    'ds000159_anat',\n",
    "    'ds000206_anat',\n",
    "    'ds000245_anat',\n",
    "    'ds001037_anat',\n",
    "    'ds001505_anat',\n",
    "    'ds001569_anat',\n",
    "    'ds001900_anat',\n",
    "    'ds002578_anat'\n",
    "]\n",
    "\n",
    "ds_root_paths = glob.glob(orig_face_defaced + '/*_anat*')\n",
    "\n",
    "processes = set()\n",
    "max_processes = 16\n",
    "\n",
    "for data_dir in ds_root_paths:\n",
    "    print(\"datadir: \", data_dir)\n",
    "    dd = data_dir.split('/')[-1]\n",
    "    if dd not in face_exclude and dd not in face_parallel:\n",
    "        print(dd)\n",
    "        save_dd_dir = os.path.join(save_orig_faced_root_dir, dd)\n",
    "        os.makedirs(save_dd_dir, exist_ok=True)\n",
    "        ds = glob.glob(data_dir + '/*_defaced.nii*')\n",
    "\n",
    "        for vol in ds:\n",
    "            vol_name = vol.split('/')[-1].split('.')[0]\n",
    "            vol_save_path = os.path.join(save_dd_dir, vol_name)\n",
    "            os.makedirs(vol_save_path, exist_ok=True)\n",
    "            prefix_pre = vol_save_path\n",
    "            prefix = os.path.join(prefix_pre, vol_name + '_refaced.nii.gz')\n",
    "            print(\"Processing Volume: \", vol)\n",
    "            print(\"Save Path: \", prefix)\n",
    "            processes.add(subprocess.Popen([\"singularity\",\"exec\", simg_path, \"@afni_refacer_run\",\n",
    "                                            \"-input\", vol,\n",
    "                                            \"-mode_reface_plus\",\n",
    "                                            \"-prefix\", prefix\n",
    "                                           ]))\n",
    "\n",
    "            if len(processes) >= max_processes:\n",
    "                os.wait()\n",
    "                processes.difference_update([\n",
    "                    p for p in processes if p.poll() is not None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "# matplotlib.use('Agg')\n",
    "import os, sys\n",
    "sys.path.append('../defacing')\n",
    "import subprocess\n",
    "\n",
    "from preprocessing.normalization import clip, standardize, normalize\n",
    "from preprocessing.conform import conform_data\n",
    "from helpers.utils import load_vol, save_vol, is_gz_file\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "import multiprocessing as mp\n",
    "    \n",
    "    \n",
    "orig_root_dir = '/work/01329/poldrack/data/mriqc-net/data'\n",
    "orig_data_face = os.path.join(orig_root_dir, 'face/T1w')\n",
    "orig_data_mask = os.path.join(orig_root_dir, 'masks')\n",
    "orig_data_deface = os.path.join(orig_root_dir, 'defaced')\n",
    "\n",
    "save_root_dir = '/work/06850/sbansal6/maverick2/mriqc-shared/'\n",
    "\n",
    "orig_reface_dir = '/work/06850/sbansal6/maverick2/mriqc-shared/refaced'\n",
    "save_orig_faced_root_dir = os.path.join(orig_reface_dir, 'orig_faced')\n",
    "save_orig_defaced_root_dir = os.path.join(orig_reface_dir, 'orig_defaced')\n",
    "\n",
    "orig_data_ixi = '/work/06850/sbansal6/maverick2/mriqc-shared/ixi_dataset/T1w'\n",
    "orig_data_ixi_deface = '/work/06850/sbansal6/maverick2/mriqc-shared/ixi_dataset/T1w_defaced'\n",
    "\n",
    "save_preprocessing_face = os.path.join(save_root_dir, 'preprocessing/face/128')\n",
    "save_conformed_face = os.path.join(save_root_dir, 'conformed/face/128')\n",
    "\n",
    "save_preprocessing_deface = os.path.join(save_root_dir, 'preprocessing/deface/128')\n",
    "save_conformed_deface = os.path.join(save_root_dir, 'conformed/deface/128')\n",
    "\n",
    "save_preprocessing_face_defaced = os.path.join(save_root_dir, 'preprocessing/face_defaced/128')\n",
    "save_conformed_face_defaced = os.path.join(save_root_dir, 'conformed/face_defaced/128')\n",
    "\n",
    "save_preprocessing_deface_refaced = os.path.join(save_root_dir, 'preprocessing/deface_refaced/128')\n",
    "save_conformed_deface_refaced = os.path.join(save_root_dir, 'conformed/deface_refaced/128')\n",
    "\n",
    "save_preprocessing_face_refaced = os.path.join(save_root_dir, 'preprocessing/face_refaced/128')\n",
    "save_conformed_face_refaced = os.path.join(save_root_dir, 'conformed/face_refaced/128')\n",
    "\n",
    "save_preprocessing_ixi = os.path.join(save_root_dir, 'preprocessing/ixi/face/128')\n",
    "save_conformed_ixi = os.path.join(save_root_dir, 'conformed/ixi/face/128')\n",
    "\n",
    "save_preprocessing_ixi_deface = os.path.join(save_root_dir, 'preprocessing/ixi/deface/128')\n",
    "save_conformed_ixi_deface = os.path.join(save_root_dir, 'conformed/ixi/deface/128')\n",
    "\n",
    "os.makedirs(save_preprocessing_face, exist_ok=True)\n",
    "os.makedirs(save_preprocessing_deface, exist_ok=True)\n",
    "os.makedirs(save_preprocessing_face_defaced, exist_ok=True)\n",
    "os.makedirs(save_preprocessing_face_refaced, exist_ok=True)\n",
    "os.makedirs(save_preprocessing_deface_refaced, exist_ok=True)\n",
    "os.makedirs(save_preprocessing_ixi_deface, exist_ok=True)\n",
    "\n",
    "os.makedirs(save_conformed_face, exist_ok=True)\n",
    "os.makedirs(save_conformed_deface, exist_ok=True)\n",
    "os.makedirs(save_conformed_face_defaced, exist_ok=True)\n",
    "os.makedirs(save_conformed_face_refaced, exist_ok=True)\n",
    "os.makedirs(save_conformed_deface_refaced, exist_ok=True)\n",
    "os.makedirs(save_conformed_ixi_deface, exist_ok=True)\n",
    "\n",
    "conform_size = (128, 128, 128)\n",
    "conform_zoom = (2., 2., 2.)\n",
    "\n",
    "\n",
    "def preprocess(orig_vol_pth, conform_pth, preprocess_pth, DS=None, mask_path=None, debug=False):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    filename = orig_vol_pth.split('/')[-1]\n",
    "    volume, affine, _ = load_vol(orig_vol_pth)\n",
    "    \n",
    "    # Preprocessing\n",
    "    volume = clip(volume, q=90)\n",
    "    volume = normalize(volume)\n",
    "    volume = standardize(volume)\n",
    "    \n",
    "    # \n",
    "    save_preprocessing_path = os.path.join(preprocess_pth, filename)\n",
    "    save_conformed_path = os.path.join(conform_pth, filename)\n",
    "    \n",
    "    print(\"save_preprocessing_path: \", save_preprocessing_path)\n",
    "    print(\"save_conformed_path: \", save_conformed_path)\n",
    "    \n",
    "    save_vol(save_preprocessing_path, volume, affine)\n",
    "\n",
    "#     def _plot(data):\n",
    "#         f, axarr = plt.subplots(8, 8, figsize=(12, 12))\n",
    "#         for i in range(8):\n",
    "#             for j in range(8):\n",
    "#                 axarr[i, j].imshow(np.rot90(data[:, :, j + 8*i], 1))\n",
    "\n",
    "#         plt.show()\n",
    "        \n",
    "        \n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.imshow(np.rot90(np.mean(data, axis=0)))\n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(np.rot90(np.mean(data, axis=1)))\n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.imshow(np.rot90(np.mean(data, axis=2)))\n",
    "#         plt.show()\n",
    "        \n",
    "    conform_data(save_preprocessing_path, \n",
    "                 out_file=save_conformed_path, \n",
    "                 out_size=conform_size, \n",
    "                 out_zooms=conform_zoom)\n",
    "\n",
    "    if debug: _plot(load_vol(save_conformed_path)[0])\n",
    "    \n",
    "    if mask_path and DS:\n",
    "        mask = np.array(nib.load(mask_path).dataobj)\n",
    "        masked_volume = volume*mask\n",
    "\n",
    "        save_mpreprocessing_path = os.path.join(save_preprocessing_deface, DS, filename)\n",
    "        save_mconformed_path = os.path.join(save_conformed_deface, DS, filename)\n",
    "        \n",
    "        print(\"save_deface_preprocessing_path: \", save_mpreprocessing_path)\n",
    "        print(\"save_deface_conformed_path: \", save_mconformed_path)\n",
    "        \n",
    "#         os.makedirs(save_mpreprocessing_path, exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(save_mconformed_path), exist_ok=True)\n",
    "    \n",
    "#         save_mpreprocessing_path = os\n",
    "        \n",
    "        save_vol(save_mpreprocessing_path, masked_volume, affine)\n",
    "\n",
    "        conform_data(save_mpreprocessing_path, \n",
    "                 out_file=save_mconformed_path, \n",
    "                 out_size=conform_size, \n",
    "                 out_zooms=conform_zoom)        \n",
    "        \n",
    "        return save_conformed_path, save_mconformed_path\n",
    "\n",
    "    return save_conformed_path\n",
    "\n",
    "\n",
    "print(save_orig_defaced_root_dir)\n",
    "\n",
    "for path in glob(os.path.join(save_orig_defaced_root_dir, '*/*/*_refaced.nii*')):\n",
    "    try:\n",
    "        vol_name = path.split(\"/\")[-1]\n",
    "        DS = path.split(\"/\")[-3]\n",
    "        print(vol_name, DS)\n",
    "        \n",
    "#         directories for saving preprocessed and conformed volumes\n",
    "        ds_save_conform_path = os.path.join(save_conformed_deface_refaced, DS)\n",
    "        ds_save_preprocess_path = os.path.join(save_preprocessing_deface_refaced, DS)\n",
    "        print(ds_save_conform_path, ds_save_preprocess_path)\n",
    "        \n",
    "        mask_path = None\n",
    "        # Get the mask path\n",
    "#         mask_path = glob(os.path.join(orig_data_mask, DS, vol_name.split('.')[0] + \"*_mask*\"))[0]\n",
    "#         print(\"Mask_path\", mask_path)\n",
    "\n",
    "        if not os.path.exists(ds_save_conform_path):\n",
    "            os.makedirs(ds_save_conform_path)\n",
    "\n",
    "        if not os.path.exists(ds_save_preprocess_path):\n",
    "            os.makedirs(ds_save_preprocess_path)\n",
    "\n",
    "        # Check if volume is a proper gunzipped\n",
    "        print(os.path.splitext(path)[1])\n",
    "        print(\"Gzipped: \", is_gz_file(path))\n",
    "        if os.path.splitext(path)[1] == \".gz\" and not is_gz_file(path):\n",
    "            rename_file = os.path.splitext(vol_name)[0]\n",
    "            fixed_gz_tmp = os.path.join(save_conformed_deface_refaced, rename_file)\n",
    "            print(\"Fixed GZIP File: \", fixed_gz_tmp)\n",
    "            subprocess.call([\"cp\", path, fixed_gz_tmp])\n",
    "\n",
    "            print(preprocess(fixed_gz_tmp,\n",
    "                             ds_save_conform_path,\n",
    "                             ds_save_preprocess_path,\n",
    "                             mask_path = mask_path,\n",
    "                             DS=None\n",
    "                            ))\n",
    "            os.remove(fixed_gz_tmp)\n",
    "\n",
    "        else:\n",
    "            print(preprocess(path,\n",
    "                             ds_save_conform_path,\n",
    "                             ds_save_preprocess_path,\n",
    "                             mask_path = mask_path,\n",
    "                             DS=None\n",
    "                            ))\n",
    "    except:\n",
    "        print(\"Preprocessing incomplete. Exception occurred.\")\n",
    "        pass\n",
    "\n",
    "\n",
    "# for ds in include:\n",
    "#     volumes = glob(os.path.join(save_orig_faced_root_dir, ds) + '/*defaced*/*refaced.nii*')\n",
    "    \n",
    "    \n",
    "# for path in glob(orig_data_face + \"/*/*.nii*\"):\n",
    "#     try:\n",
    "\n",
    "#         vol_name = path.split(\"/\")[-1]\n",
    "#         DS = path.split(\"/\")[-2]\n",
    "# #         print(vol_name, DS)\n",
    "#         if DS not in exclude_face_refaced:\n",
    "#             print(vol_name, DS)\n",
    "\n",
    "#             # directories for saving preprocessed and conformed volumes\n",
    "#             ds_save_conform_path = os.path.join(save_conformed_face_refaced, DS)\n",
    "#             ds_save_preprocess_path = os.path.join(save_preprocessing_face_refaced, DS)\n",
    "\n",
    "#             os.makedirs(ds_save_conform_path, exist_ok=True)\n",
    "#             os.makedirs(ds_save_preprocess_path, exist_ok=True)\n",
    "\n",
    "#             # Check if volume is a proper gunzipped\n",
    "#             if not os.path.splitext(path)[1] == \".gz\" and is_gz_file(path):\n",
    "#                 rename_file = os.path.splitext(vol_name)[0]\n",
    "#                 fixed_gz_tmp = os.path.join(save_conformed_face_refaced, rename_file)\n",
    "#                 print(fixed_gz_tmp)\n",
    "#                 subprocess.call([\"cp\", path, fixed_gz_tmp])\n",
    "\n",
    "#                 print(preprocess(fixed_gz_tmp,\n",
    "#                                  ds_save_conform_path,\n",
    "#                                  ds_save_preprocess_path,\n",
    "#                                  DS=DS\n",
    "#                                 ))\n",
    "#                 os.remove(fixed_gz_tmp)\n",
    "\n",
    "#             else:\n",
    "#                 print(preprocess(path,\n",
    "#                                  ds_save_conform_path,\n",
    "#                                  ds_save_preprocess_path,\n",
    "#                                  DS=DS\n",
    "#                                 ))\n",
    "                \n",
    "#     except:\n",
    "#         print(\"Preprocessing incomplete. Exception occurred.\")\n",
    "#         pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "orig_root_dir = '/work/01329/poldrack/data/mriqc-net/data'\n",
    "orig_data_face = os.path.join(orig_root_dir, 'face/T1w')\n",
    "orig_data_mask = os.path.join(orig_root_dir, 'masks')\n",
    "orig_data_deface = os.path.join(orig_root_dir, 'defaced')\n",
    "\n",
    "save_root_dir = '/work/06850/sbansal6/maverick2/mriqc-shared/'\n",
    "\n",
    "orig_reface_dir = '/work/06850/sbansal6/maverick2/mriqc-shared/refaced'\n",
    "save_orig_faced_root_dir = os.path.join(orig_reface_dir, 'orig_faced')\n",
    "save_orig_defaced_root_dir = os.path.join(orig_reface_dir, 'orig_defaced')\n",
    "\n",
    "orig_data_ixi = '/work/06850/sbansal6/maverick2/mriqc-shared/ixi_dataset/T1w'\n",
    "orig_data_ixi_deface = '/work/06850/sbansal6/maverick2/mriqc-shared/ixi_dataset/T1w_defaced'\n",
    "\n",
    "save_preprocessing_face = os.path.join(save_root_dir, 'preprocessing/face/128')\n",
    "save_conformed_face = os.path.join(save_root_dir, 'conformed/face/128')\n",
    "\n",
    "save_preprocessing_deface = os.path.join(save_root_dir, 'preprocessing/deface/128')\n",
    "save_conformed_deface = os.path.join(save_root_dir, 'conformed/deface/128')\n",
    "\n",
    "save_preprocessing_face_defaced = os.path.join(save_root_dir, 'preprocessing/face_defaced/128')\n",
    "save_conformed_face_defaced = os.path.join(save_root_dir, 'conformed/face_defaced/128')\n",
    "\n",
    "save_preprocessing_deface_refaced = os.path.join(save_root_dir, 'preprocessing/deface_refaced/128')\n",
    "save_conformed_deface_refaced = os.path.join(save_root_dir, 'conformed/deface_refaced/128')\n",
    "\n",
    "save_preprocessing_face_refaced = os.path.join(save_root_dir, 'preprocessing/face_refaced/128')\n",
    "save_conformed_face_refaced = os.path.join(save_root_dir, 'conformed/face_refaced/128')\n",
    "\n",
    "save_preprocessing_ixi = os.path.join(save_root_dir, 'preprocessing/ixi/face/128')\n",
    "save_conformed_ixi = os.path.join(save_root_dir, 'conformed/ixi/face/128')\n",
    "\n",
    "save_preprocessing_ixi_deface = os.path.join(save_root_dir, 'preprocessing/ixi/deface/128')\n",
    "save_conformed_ixi_deface = os.path.join(save_root_dir, 'conformed/ixi/deface/128')\n",
    "\n",
    "import glob\n",
    "\n",
    "print(len(glob.glob(save_conformed_deface_refaced + '/*/*.nii*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(glob('/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face_refaced/128/*/*')))\n",
    "print(len(glob('/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face_refaced/64/*/*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "# matplotlib.use('Agg')\n",
    "import os, sys\n",
    "sys.path.append('../defacing')\n",
    "import subprocess\n",
    "\n",
    "from preprocessing.normalization import clip, standardize, normalize\n",
    "from preprocessing.conform import conform_data\n",
    "from helpers.utils import load_vol, save_vol, is_gz_file\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "\n",
    "    \n",
    "data = load_vol('/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face/128/ds000159_anat/sub-44_T1w.nii')\n",
    "\n",
    "def _plot(data):\n",
    "    f, axarr = plt.subplots(8, 8, figsize=(24, 24))\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            axarr[i, j].imshow(np.rot90(data[j + 8*i, :, :], 1))\n",
    "\n",
    "    plt.show()\n",
    "        \n",
    "_plot(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_root_dir = '/work/01329/poldrack/data/mriqc-net/data'\n",
    "orig_data_face = os.path.join(orig_root_dir, 'face/T1w')\n",
    "# orig_data_mask = os.path.join(orig_root_dir, 'masks')\n",
    "\n",
    "orig_data_deface = os.path.join(orig_root_dir, 'defaced')\n",
    "\n",
    "save_root_dir = '/work/06850/sbansal6/maverick2/mriqc-shared/'\n",
    "\n",
    "# save_preprocessing_face = os.path.join(save_root_dir, 'preprocessing/face')\n",
    "save_conformed_face = os.path.join(save_root_dir, 'conformed/face')\n",
    "\n",
    "save_preprocessing_deface = os.path.join(save_root_dir, 'preprocessing/deface')\n",
    "save_conformed_deface = os.path.join(save_root_dir, 'conformed/deface')\n",
    "\n",
    "save_conformed_face_defaced = os.path.join(save_root_dir, 'conformed/face_defaced')\n",
    "# os.makedirs(save_preprocessing_face, exist_ok=True)\n",
    "# os.makedirs(save_preprocessing_deface, exist_ok=True)\n",
    "# os.makedirs(save_conformed_face, exist_ok=True)\n",
    "# os.makedirs(save_conformed_deface, exist_ok=True)\n",
    "\n",
    "\n",
    "def checkNonConformed(orig_path, save_path):\n",
    "\n",
    "    conform = []\n",
    "    orig = []\n",
    "\n",
    "    for path in glob(save_path + \"/*/*.nii*\"):\n",
    "        tempname = path.split(\"/\")[-1]\n",
    "        ds = path.split(\"/\")[-2]\n",
    "        conform.append(ds + \"/\" + tempname)\n",
    "\n",
    "    print(\"Total Conformed: \", len(conform))\n",
    "\n",
    "    for path in glob(orig_path + \"/*/*.nii*\"):\n",
    "        tempname = path.split(\"/\")[-1]\n",
    "        ds = path.split(\"/\")[-2]\n",
    "        orig.append(ds + \"/\" + tempname)\n",
    "\n",
    "    print(\"Total Original: \", len(orig))\n",
    "\n",
    "    print(\"Total not conformed: \", len(orig) - len(conform))\n",
    "\n",
    "    count = 0\n",
    "    for f in orig:\n",
    "        exists = False\n",
    "        for fc in conform:\n",
    "            if fc in f:\n",
    "                exists = True\n",
    "        if not exists:\n",
    "            count += 1\n",
    "            print(\"Not conformed file: \", f)\n",
    "            \n",
    "\n",
    "            \n",
    "checkNonConformed(orig_data_face, save_conformed_face)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_C = []\n",
    "face_O = []\n",
    "\n",
    "for path in glob(save_data_face + \"/*/*.nii*\"):\n",
    "    tempname = path.split(\"/\")[-1]\n",
    "    ds = path.split(\"/\")[-2]\n",
    "    face_C.append(ds + '/' + tempname)\n",
    "\n",
    "print(len(face_C))\n",
    "# print(face_C)\n",
    "\n",
    "\n",
    "for path in glob(orig_data_face + \"/*/*.nii*\"):\n",
    "    tempname = path.split(\"/\")[-1]\n",
    "    ds = path.split(\"/\")[-2]\n",
    "    face_O.append(ds + '/' + tempname)\n",
    "\n",
    "print(len(face_O))\n",
    "# print(face_O)\n",
    "\n",
    "count = 0\n",
    "for f in face_O:\n",
    "    exists = False\n",
    "    for fc in face_C:\n",
    "        if fc in f:\n",
    "            exists = True\n",
    "    if not exists:\n",
    "        count += 1\n",
    "        print(f)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate n-fold CV Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "face_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face\"\n",
    "deface_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/deface\"\n",
    "face_defaced_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face_defaced\"\n",
    "face_refaced_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face_refaced\"\n",
    "\n",
    "paths_d = []\n",
    "paths_f = []\n",
    "labels_d = []\n",
    "labels_f = []\n",
    "\n",
    "for path in glob(face_defaced_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_d.append(path)\n",
    "    labels_d.append(0)\n",
    "    \n",
    "# for path in glob(deface_path + \"/*/*.nii*\"):\n",
    "#     paths.append(path)\n",
    "#     labels.append(0)\n",
    "\n",
    "for path in glob(face_refaced_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_f.append(path)\n",
    "    labels_f.append(1)\n",
    "    \n",
    "# for path in glob(face_path + \"/*/*.nii*\"):\n",
    "#     DS = path.split('/')[-2]\n",
    "#     paths_f.append(path)\n",
    "#     labels_f.append(1)\n",
    "    \n",
    "print(len(paths_f), len(paths_d))\n",
    "print(len(labels_f), len(labels_d))\n",
    "\n",
    "\n",
    "percentage = [25, 50, 75, 100]\n",
    "percentage = [100]\n",
    "\n",
    "for per in percentage:\n",
    "    \n",
    "    SPLITS = 15\n",
    "    \n",
    "#     print(len(paths))\n",
    "    ind = int(len(paths_f)*per/100)\n",
    "    shuffle(paths_d)\n",
    "    shuffle(paths_f)\n",
    "    temp_p = paths_d[:ind] + paths_f[:ind]\n",
    "    temp_l = labels_d[:ind] + labels_f[:ind]\n",
    "    \n",
    "    save_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiment_faced_refaced/exp_face_refaced/exp_add_datasets/csv_F15_P{}\".format(per)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"X\"] = temp_p\n",
    "    df[\"Y\"] = temp_l\n",
    "    df.to_csv(os.path.join(save_path, \"all.csv\"))\n",
    "    \n",
    "    SPLITS = 15\n",
    "    skf = StratifiedKFold(n_splits=SPLITS)\n",
    "    fold_no = 1\n",
    "    \n",
    "    for train_index, test_index in skf.split(temp_p, temp_l):\n",
    "        out_path = os.path.join(save_path, \"train_test_fold_{}/csv/\".format(fold_no))\n",
    "        \n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "            \n",
    "        image_train, image_test = (\n",
    "            itemgetter(*train_index)(temp_p),\n",
    "            itemgetter(*test_index)(temp_p),\n",
    "        )\n",
    "        \n",
    "        label_train, label_test = (\n",
    "            itemgetter(*train_index)(temp_l),\n",
    "            itemgetter(*test_index)(temp_l),\n",
    "        )\n",
    "\n",
    "        train_data = {\"X\": image_train , \"Y\": label_train}\n",
    "        df_train = pd.DataFrame(train_data)\n",
    "        df_train.to_csv(os.path.join(out_path, \"training.csv\"), index=False)\n",
    "        \n",
    "        validation_data = {\"X\": image_test, \"Y\": label_test}\n",
    "        df_validation = pd.DataFrame(validation_data)\n",
    "        df_validation.to_csv(os.path.join(out_path, \"validation.csv\"), index=False)\n",
    "        \n",
    "        fold_no += 1\n",
    "        \n",
    "    \n",
    "    \n",
    "# df = pd.DataFrame()\n",
    "# df[\"X\"] = paths\n",
    "# df[\"Y\"] = labels\n",
    "# df.to_csv(os.path.join(save_path, \"all.csv\"))\n",
    "\n",
    "# SPLITS = 15\n",
    "# skf = StratifiedKFold(n_splits=SPLITS)\n",
    "# fold_no = 1\n",
    "\n",
    "# for train_index, test_index in skf.split(paths, labels):\n",
    "#     out_path = save_path + \"/train_test_fold_{}/csv/\".format(fold_no)\n",
    "\n",
    "#     if not os.path.exists(out_path):\n",
    "#         os.makedirs(out_path)\n",
    "\n",
    "#     image_train, image_test = (\n",
    "#         itemgetter(*train_index)(paths),\n",
    "#         itemgetter(*test_index)(paths),\n",
    "#     )\n",
    "#     label_train, label_test = (\n",
    "#         itemgetter(*train_index)(labels),\n",
    "#         itemgetter(*test_index)(labels),\n",
    "#     )\n",
    "\n",
    "#     # image_train = [os.path.join(data_path, 'sub-' + str(pth) + '_T1w.nii.gz') for pth in image_train]\n",
    "#     train_data = {\"X\": image_train, \"Y\": label_train}\n",
    "#     df_train = pd.DataFrame(train_data)\n",
    "#     df_train.to_csv(os.path.join(out_path, \"training.csv\"), index=False)\n",
    "\n",
    "#     # image_test = [os.path.join(data_path, 'sub-' + str(pth) + '_T1w.nii.gz') for pth in image_test]\n",
    "#     validation_data = {\"X\": image_test, \"Y\": label_test}\n",
    "#     df_validation = pd.DataFrame(validation_data)\n",
    "#     df_validation.to_csv(os.path.join(out_path, \"validation.csv\"), index=False)\n",
    "\n",
    "#     fold_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tfrecords for n-fold CV Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "percentage = [100]\n",
    "\n",
    "SPLITS = 15\n",
    "for per in percentage:\n",
    "    for fold in range(1, SPLITS+1):\n",
    "\n",
    "        dir_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiment_faced_refaced/exp_face_refaced/exp_add_datasets/csv_F15_P{}/train_test_fold_{}/csv/\".format(per, fold)\n",
    "        tf_records_dir = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiment_faced_refaced/exp_face_refaced/exp_add_datasets/tfrecords_F15_P{}/tfrecords_fold_{}/\".format(per, fold)\n",
    "        os.makedirs(tf_records_dir, exist_ok=True)\n",
    "\n",
    "        train_csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "        valid_csv_path = os.path.join(dir_path, \"validation.csv\")\n",
    "\n",
    "        train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "        train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "        train_D = list(zip(train_paths, train_labels))\n",
    "        random.shuffle(train_D)\n",
    "    #     print(train_D[0])\n",
    "\n",
    "        valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "        valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "        valid_D = list(zip(valid_paths, valid_labels))\n",
    "        random.shuffle(valid_D)\n",
    "\n",
    "        train_write_path = os.path.join(tf_records_dir, 'data-train_shard-{shard:03d}.tfrec')\n",
    "        valid_write_path = os.path.join(tf_records_dir, 'data-valid_shard-{shard:03d}.tfrec')\n",
    "\n",
    "        nobrainer.tfrecord.write(\n",
    "            features_labels=train_D,\n",
    "            filename_template=train_write_path,\n",
    "            examples_per_shard=3)\n",
    "\n",
    "        nobrainer.tfrecord.write(\n",
    "            features_labels=valid_D,\n",
    "            filename_template=valid_write_path,\n",
    "            examples_per_shard=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from random import shuffle\n",
    "\n",
    "include = ['ds000140_anat',\n",
    "           'ds000119_anat',\n",
    "           'ds000157_anat',\n",
    "           'ds002509_anat'\n",
    "          ]\n",
    "\n",
    "face_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face\"\n",
    "# deface_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/deface\"\n",
    "face_defaced_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face_defaced\"\n",
    "face_refaced_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/conformed/face_refaced\"\n",
    "\n",
    "paths_d = []\n",
    "paths_f = []\n",
    "labels_d = []\n",
    "labels_f = []\n",
    "\n",
    "for path in glob(face_defaced_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_d.append(path)\n",
    "    labels_d.append(0)\n",
    "    \n",
    "# for path in glob(deface_path + \"/*/*.nii*\"):\n",
    "#     paths.append(path)\n",
    "#     labels.append(0)\n",
    "\n",
    "for path in glob(face_refaced_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_f.append(path)\n",
    "    labels_f.append(1)\n",
    "    \n",
    "# for path in glob(face_path + \"/*/*.nii*\"):\n",
    "#     DS = path.split('/')[-2]\n",
    "#     paths_f.append(path)\n",
    "#     labels_f.append(1)\n",
    "    \n",
    "print(len(paths_f), len(paths_d))\n",
    "print(len(labels_f), len(labels_d))\n",
    "\n",
    "percentage = [25, 50, 75, 100]\n",
    "\n",
    "for per in percentage:\n",
    "#     print(len(paths))\n",
    "    ind = int(len(paths_f)*per/100)\n",
    "    shuffle(paths_d)\n",
    "    shuffle(paths_f)\n",
    "    temp_p = paths_d[:ind] + paths_f[:ind]\n",
    "    temp_l = labels_d[:ind] + labels_f[:ind]\n",
    "    save_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiment_faced_refaced/exp_face_refaced/exp_add_datasets/csv_{}\".format(per)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    train_data = {\"X\": temp_p , \"Y\": temp_l}\n",
    "    df_train = pd.DataFrame(train_data)\n",
    "    df_train.to_csv(os.path.join(save_path, \"training.csv\"), index=False)\n",
    "\n",
    "# validation_data = {\"X\": X_test, \"Y\": y_test}\n",
    "# df_validation = pd.DataFrame(validation_data)\n",
    "# df_validation.to_csv(os.path.join(save_path, \"validation.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "percentage = [25, 50, 75, 100]\n",
    "\n",
    "for per in percentage:\n",
    "    dir_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiment_faced_refaced/exp_face_refaced/exp_add_datasets/csv_{}\".format(per)\n",
    "    tf_records_dir = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiment_faced_refaced/exp_face_refaced/exp_add_datasets/tfrecords_{}\".format(per)\n",
    "    os.makedirs(tf_records_dir, exist_ok=True)\n",
    "    \n",
    "    train_csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "    # valid_csv_path = os.path.join(dir_path, \"validation.csv\")\n",
    "\n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "    train_D = list(zip(train_paths, train_labels))\n",
    "    random.shuffle(train_D)\n",
    "\n",
    "    # valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "    # valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "    # valid_D = list(zip(valid_paths, valid_labels))\n",
    "    # random.shuffle(valid_D)\n",
    "\n",
    "    train_write_path = os.path.join(tf_records_dir, 'data-train_shard-{shard:03d}.tfrec')\n",
    "    # valid_write_path = os.path.join(tf_records_dir, 'data-valid_shard-{shard:03d}.tfrec')\n",
    "\n",
    "    nobrainer.tfrecord.write(\n",
    "        features_labels=train_D,\n",
    "        filename_template=train_write_path,\n",
    "        examples_per_shard=3)\n",
    "\n",
    "# nobrainer.tfrecord.write(\n",
    "#     features_labels=valid_D,\n",
    "#     filename_template=valid_write_path,\n",
    "#     examples_per_shard=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.random.uniform((1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nobrainer\n",
    "from nobrainer.io import _is_gzipped\n",
    "from nobrainer.volume import to_blocks\n",
    "import sys, os\n",
    "sys.path.append('../defacing')\n",
    "from preprocessing.augmentation import VolumeAugmentations, SliceAugmentations\n",
    "from helpers.utils import load_vol\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/experiment_faced_refaced/exp_face_defaced/tfrecords_F15'\n",
    "DISTRIBUTION = load_vol('../defacing/helpers/distribution.nii.gz')[0]\n",
    "DISTRIBUTION /= DISTRIBUTION.sum()\n",
    "COM = np.unravel_index(int(np.sum(DISTRIBUTION.ravel()*np.arange(len(DISTRIBUTION.ravel())))/np.sum(DISTRIBUTION.ravel())), DISTRIBUTION.shape)\n",
    "\n",
    "\n",
    "# sampling from augmented distribution is same as augmenting the sampled points\n",
    "# augmenting distribution at every iteration is expensive, so this way\n",
    "sampler = lambda n, distribution = DISTRIBUTION, threshold = 0.1: np.array([ np.unravel_index(\n",
    "          np.random.choice(np.arange(np.prod(distribution.shape)),\n",
    "                                     p = distribution.ravel()),\n",
    "          distribution.shape) + (+1 if np.random.randn() > 0.5 else -1)*np.random.randint(0, \n",
    "                                        int(distribution.shape[0]*threshold) + 1, 3) for _ in range(n)]) \n",
    "\n",
    "\n",
    "three_d_augmentations = {'rotation': 0.5,\n",
    "                         'translation': 0.5,\n",
    "                         'noop': 0.3\n",
    "                        }\n",
    "\n",
    "augmentvolume = VolumeAugmentations(DISTRIBUTION, three_d_augmentations)\n",
    "\n",
    "two_d_augmentations = {'rotation': 0.5,\n",
    "                       'fliplr': 0.5,\n",
    "                       'flipud': 0.5,\n",
    "                       'zoom': 0.5,\n",
    "                       'noop': 0.3\n",
    "                      }\n",
    "\n",
    "# augmentslice = VolumeAugmentations(DISTRIBUTION, two_d_augmentations)\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    file_pattern,\n",
    "    n_classes,\n",
    "    batch_size,\n",
    "    volume_shape,\n",
    "    plane,\n",
    "    n = 24,\n",
    "    block_shape=None,\n",
    "    n_epochs=None,\n",
    "    mapping=None,\n",
    "    augment=False,\n",
    "    shuffle_buffer_size=None,\n",
    "    num_parallel_calls=AUTOTUNE,\n",
    "):\n",
    "\n",
    "    \"\"\" Returns tf.data.Dataset after preprocessing from\n",
    "    tfrecords for training and validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_pattern:\n",
    "\n",
    "    n_classes:\n",
    "    \"\"\"\n",
    "\n",
    "    files = glob.glob(file_pattern)\n",
    "\n",
    "    if not files:\n",
    "        raise ValueError(\"no files found for pattern '{}'\".format(file_pattern))\n",
    "\n",
    "    compressed = _is_gzipped(files[0])\n",
    "    shuffle = bool(shuffle_buffer_size)\n",
    "\n",
    "    ds = nobrainer.dataset.tfrecord_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        volume_shape=volume_shape,\n",
    "        shuffle=shuffle,\n",
    "        scalar_label=True,\n",
    "        compressed=compressed,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "    )\n",
    "\n",
    "    # if augment:\n",
    "    #     ds = ds.map(\n",
    "    #         lambda x, y: tf.cond(\n",
    "    #             tf.random.uniform((1,)) > 0.5,\n",
    "    #             true_fn=lambda: apply_augmentations(x, y),\n",
    "    #             false_fn=lambda: (x, y),\n",
    "    #         ),\n",
    "    #         num_parallel_calls=num_parallel_calls,\n",
    "    #     )\n",
    "\n",
    "    def _ss(x, y):\n",
    "        if augment:\n",
    "            if three_d_augmentations['noop'] < 1:\n",
    "                x, y = augmentvolume(x,y)\n",
    "        x, y = structural_slice(x, y, \n",
    "                                plane, \n",
    "                                n, \n",
    "                                augment, \n",
    "                                augmentvolume.distribution)\n",
    "        return (x, y)\n",
    "\n",
    "    ds = ds.map(_ss, num_parallel_calls)\n",
    "\n",
    "    #     def _f(x, y):\n",
    "    #         x = to_blocks(x, block_shape)\n",
    "    #         n_blocks = x.shape[0]\n",
    "    #         y = tf.repeat(y, n_blocks)\n",
    "    #         return (x, y)\n",
    "    #     ds = ds.map(_f, num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    # This step is necessary because it reduces the extra dimension.\n",
    "    # ds = ds.unbatch()\n",
    "\n",
    "\n",
    "    ds = ds.prefetch(buffer_size=batch_size)\n",
    "\n",
    "    if batch_size is not None:\n",
    "        ds = ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "\n",
    "    if shuffle_buffer_size:\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat the dataset n_epochs times\n",
    "    ds = ds.repeat(n_epochs)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def structural_slice(x, y, \n",
    "                plane, \n",
    "                n = 4, \n",
    "                augment = False, \n",
    "                distribution = DISTRIBUTION):\n",
    "\n",
    "    \"\"\" Transpose dataset based on the plane\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "\n",
    "    y:\n",
    "\n",
    "    plane:\n",
    "    \n",
    "    n:\n",
    "\n",
    "    augment:\n",
    "    \"\"\"\n",
    "\n",
    "    threshold = 0.1 if augment else 0.0 \n",
    "    options = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "    shape = np.array(x.shape)\n",
    "\n",
    "    if isinstance(plane, str) and plane in options:\n",
    "        idxs = sampler(n, \n",
    "                        distribution, \n",
    "                        threshold)\n",
    "\n",
    "        if plane == \"axial\":\n",
    "            idx = np.random.randint(shape[0]**0.5)\n",
    "            midx = idxs[:, 0]\n",
    "            x = x\n",
    "\n",
    "        if plane == \"coronal\":\n",
    "            idx = np.random.randint(shape[1]**0.5)\n",
    "            midx = idxs[:, 1]\n",
    "            x = tf.transpose(x, perm=[1, 2, 0])\n",
    "\n",
    "\n",
    "        if plane == \"sagittal\":\n",
    "            idx = np.random.randint(shape[2]**0.5)\n",
    "            midx = idxs[:, 2]\n",
    "            x = tf.transpose(x, perm=[2, 0, 1])\n",
    "\n",
    "\n",
    "        if plane == \"combined\":\n",
    "            temp = {}\n",
    "            for op in options[:-1]:\n",
    "                temp[op] = structural_slice(x, y, \n",
    "                                            op, \n",
    "                                            n, \n",
    "                                            augment, \n",
    "                                            distribution)[0]\n",
    "            x = temp\n",
    "\n",
    "        if not plane == \"combined\":\n",
    "            x = tf.squeeze(tf.gather_nd(x, midx.reshape(n, 1, 1)), axis=1)\n",
    "            x = tf.math.reduce_mean(x, axis=0)\n",
    "            x = tf.expand_dims(x, axis=-1)\n",
    "            \n",
    "            if augment:\n",
    "                x = two_d_augmentations(x)\n",
    "                \n",
    "            x = tf.convert_to_tensor(x)\n",
    "        return x, y\n",
    "    else:\n",
    "        raise ValueError(\"expected plane to be one of ['axial', 'coronal', 'sagittal']\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    n_classes = 2\n",
    "    global_batch_size = 8\n",
    "    volume_shape = (64, 64, 64)\n",
    "    ds = get_dataset(\n",
    "        os.path.join(ROOTDIR, \"tfrecords_fold_1/data-train_*\"),\n",
    "        n_classes=n_classes,\n",
    "        batch_size=global_batch_size,\n",
    "        volume_shape=volume_shape,\n",
    "        plane=\"sagittal\",\n",
    "        augment = False,\n",
    "        shuffle_buffer_size=3,\n",
    "    )\n",
    "\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(ds)\n",
    "#     for ii in range(100):\n",
    "#         x,y=next(ds.as_numpy_iterator())\n",
    "#         # print (np.min(x), np.max(x), np.unique(y))\n",
    "#         count = 1\n",
    "#         for i in range(global_batch_size):\n",
    "#             for key in x.keys():\n",
    "#                 plt.subplot(global_batch_size, 3, count)\n",
    "#                 plt.imshow(x[key][i, :, :, 0])\n",
    "#                 plt.title(str(y[i]))\n",
    "#                 plt.xticks([],\" \")\n",
    "#                 plt.yticks([], \" \")\n",
    "#                 count += 1\n",
    "#         plt.savefig(\"processed_image_combined_{}.png\".format(ii))\n",
    "\n",
    "\n",
    "# dataset_train_coronal = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "#                             n_classes=n_classes,\n",
    "#                             batch_size=global_batch_size,\n",
    "#                             volume_shape=volume_shape,\n",
    "#                             block_shape=block_shape,\n",
    "#                             plane='coronal',\n",
    "#                             shuffle_buffer_size=3)\n",
    "\n",
    "# dataset_train_sagittal = get_dataset(\"tfrecords/tfrecords_fold_1/data-train_*\",\n",
    "#                             n_classes=n_classes,\n",
    "#                             batch_size=global_batch_size,\n",
    "#                             volume_shape=volume_shape,\n",
    "#                             block_shape=block_shape,\n",
    "#                             plane='sagittal',\n",
    "#                             shuffle_buffer_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fold = 1\n",
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/experiment_faced_refaced/exp_face_defaced/tfrecords_full'\n",
    "dir_path = '/work/06850/sbansal6/maverick2/mriqc-shared/experiment_faced_refaced/exp_face_defaced/csv_full'\n",
    "\n",
    "train_csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "# valid_csv_path = os.path.join(dir_path, \"validation.csv\")\n",
    "\n",
    "train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "# valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "\n",
    "train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "# valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "\n",
    "print(type(train_labels))\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "weights = class_weight.compute_class_weight('balanced',\n",
    "                                            np.unique(train_labels),\n",
    "                                            train_labels)\n",
    "\n",
    "print(dict(enumerate(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Std packages\n",
    "import sys, os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "sys.path.append(\"../defacing\")\n",
    "\n",
    "# Custom packages\n",
    "from models import modelN\n",
    "# from dataloaders.dataset import get_dataset\n",
    "\n",
    "# Tf packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler,\n",
    "    TensorBoard,\n",
    ")\n",
    "# import nobrainer\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/experiment_faced_refaced/exp_face_refaced/exp_add_datasets'\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 3:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "\n",
    "def train(\n",
    "    volume_shape=(64, 64, 64),\n",
    "    image_size=(64, 64),\n",
    "    dropout=0.2,\n",
    "    batch_size=16,\n",
    "    n_classes=2,\n",
    "    n_epochs=15,\n",
    "    fold=1,\n",
    "    percent=100,\n",
    "):\n",
    "    \n",
    "    \n",
    "    print(\"FOLD {}\".format(fold))\n",
    "    dir_path = os.path.join(ROOTDIR, \"csv_F15_P{}/train_test_fold_{}/csv\".format(percent, fold))\n",
    "#     dir_path = os.path.join(ROOTDIR, \"csv_{}\".format(percent))\n",
    "\n",
    "    train_csv_path = os.path.join(dir_path, \"training.csv\")\n",
    "    valid_csv_path = os.path.join(dir_path, \"validation.csv\")\n",
    "\n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "    \n",
    "    train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "    valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "    \n",
    "    weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(train_labels),\n",
    "                                                train_labels)\n",
    "    weights = dict(enumerate(weights))\n",
    "    \n",
    "    print(weights)\n",
    "    \n",
    "    planes = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "\n",
    "#     strategy = tf.distribute.MirroredStrategy()\n",
    "#     BATCH_SIZE_PER_REPLICA = batch_size\n",
    "#     global_batch_size = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "    global_batch_size = batch_size\n",
    "    \n",
    "    model_save_path = os.path.join(ROOTDIR, \"model_save_dir_F15_P{}/train_test_fold_{}\".format(percent, fold))\n",
    "#     model_save_path = os.path.join(ROOTDIR, \"model_save_dir_{}\".format(percent))\n",
    "\n",
    "    \n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "    cp_save_path = os.path.join(model_save_path, \"weights\")\n",
    "\n",
    "    logdir_path = os.path.join(model_save_path, \"tb_logs\")\n",
    "    if not os.path.exists(logdir_path):\n",
    "        os.makedirs(logdir_path)\n",
    "\n",
    "    for plane in planes:\n",
    "\n",
    "        logdir = os.path.join(logdir_path, plane)\n",
    "        os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "        tbCallback = TensorBoard(\n",
    "            log_dir=logdir, histogram_freq=1, write_graph=True, write_images=True,\n",
    "        )\n",
    "\n",
    "        os.makedirs(os.path.join(cp_save_path, plane), exist_ok=True)\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            os.path.join(cp_save_path, plane, \"best-wts.h5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "\n",
    "#         with strategy.scope():\n",
    "\n",
    "        if not plane == \"combined\": \n",
    "            lr = 1e-3\n",
    "            model = modelN.Submodel(\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                name=plane,\n",
    "                include_top=True,\n",
    "                weights=None,\n",
    "            )\n",
    "        else:\n",
    "            lr = 5e-4\n",
    "            model = modelN.CombinedClassifier(\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                trainable=True,\n",
    "                wts_root=cp_save_path,\n",
    "            )\n",
    "\n",
    "        print(\"Submodel: \", plane)\n",
    "#         print(model.summary())\n",
    "\n",
    "        METRICS = [\n",
    "            metrics.TruePositives(name=\"tp\"),\n",
    "            metrics.FalsePositives(name=\"fp\"),\n",
    "            metrics.TrueNegatives(name=\"tn\"),\n",
    "            metrics.FalseNegatives(name=\"fn\"),\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            metrics.Precision(name=\"precision\"),\n",
    "            metrics.Recall(name=\"recall\"),\n",
    "            metrics.AUC(name=\"auc\"),\n",
    "        ]\n",
    "\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.binary_crossentropy,\n",
    "            optimizer=Adam(learning_rate=lr),\n",
    "            metrics=METRICS,\n",
    "        )\n",
    "\n",
    "        print(\"GLOBAL BATCH SIZE: \", global_batch_size)\n",
    "\n",
    "        dataset_train = get_dataset(\n",
    "            file_pattern=os.path.join(ROOTDIR, \"tfrecords_F15_P{}/tfrecords_fold_{}/data-train_*\".format(percent, fold)),\n",
    "            n_classes=n_classes,\n",
    "            batch_size=global_batch_size,\n",
    "            volume_shape=volume_shape,\n",
    "            plane=plane,\n",
    "            shuffle_buffer_size=global_batch_size,\n",
    "        )\n",
    "\n",
    "        dataset_valid = get_dataset(\n",
    "            file_pattern=os.path.join(ROOTDIR, \"tfrecords_F15_P{}/tfrecords_fold_{}/data-valid_*\".format(percent, fold)),\n",
    "            n_classes=n_classes,\n",
    "            batch_size=global_batch_size,\n",
    "            volume_shape=volume_shape,\n",
    "            plane=plane,\n",
    "            shuffle_buffer_size=global_batch_size,\n",
    "        )\n",
    "        \n",
    "        steps_per_epoch = math.ceil(len(train_paths)/batch_size)\n",
    "        validation_steps = math.ceil(len(valid_paths)/batch_size)\n",
    "\n",
    "        print(steps_per_epoch)\n",
    "    \n",
    "\n",
    "        lrcallback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        \n",
    "        model.fit(\n",
    "            dataset_train,\n",
    "            epochs=n_epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            validation_data=dataset_valid,\n",
    "            validation_steps=validation_steps,\n",
    "            callbacks=[tbCallback, model_checkpoint],\n",
    "            class_weight = weights,\n",
    "        )\n",
    "\n",
    "        del model\n",
    "        K.clear_session()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    percentage = [50, 75]\n",
    "#     for per in percentage:\n",
    "    for fold in range(1, 15):\n",
    "        train(fold=fold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import imgaug\n",
    "import shutil\n",
    "import numpy as np\n",
    "from imgaug import augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../defacing')\n",
    "from helpers.utils import save_vol, load_vol\n",
    "from preprocessing.conform import conform_data\n",
    "from preprocessing.normalization import clip, standardize, normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DataGeneratoronFly(object):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, conform_size=(64, 64, 64),\n",
    "                        conform_zoom=(4., 4., 4.), \n",
    "                        nchannels=1, \n",
    "                        nruns=8,\n",
    "                        nsamples=20,\n",
    "                        save=False, \n",
    "                        transform=None):\n",
    "\n",
    "        self.conform_size=conform_size\n",
    "        self.conform_zoom=conform_zoom\n",
    "        self.nchannels=nchannels\n",
    "        self.transform=transform\n",
    "        self.nsamples=nsamples\n",
    "        self.nruns=nruns\n",
    "        self.save=save\n",
    "        DISTRIBUTION = load_vol('../defacing/helpers/distribution.nii.gz')[0]\n",
    "        assert DISTRIBUTION.shape == conform_size, \"Invalid conform_size needs to regenerate face distribution\"\n",
    "\n",
    "        DISTRIBUTION /= DISTRIBUTION.sum()\n",
    "        self.sampler = lambda n: np.array([ np.unravel_index(\n",
    "                  np.random.choice(np.arange(np.prod(DISTRIBUTION.shape)),\n",
    "                                             p = DISTRIBUTION.ravel()),\n",
    "                  DISTRIBUTION.shape) for _ in range(n)]) \n",
    "\n",
    "\n",
    "\n",
    "    def _augmentation(self, volume):\n",
    "        r\"\"\"\n",
    "                Augmenters that are safe to apply to masks\n",
    "                Some, such as Affine, have settings that make them unsafe, so always\n",
    "                test your augmentation on masks\n",
    "        \"\"\"\n",
    "        volume_shape = volume.shape\n",
    "        det = self.transform.to_deterministic()\n",
    "        volume = det.augment_image(volume)\n",
    "\n",
    "        assert volume.shape == volume_shape, \"Augmentation shouldn't change volume size\"\n",
    "        return volume\n",
    "\n",
    "\n",
    "    def _sample_slices(self, volume, plane=None):\n",
    "\n",
    "        options = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "        assert plane in options, \"expected plane to be one of ['axial', 'coronal', 'sagittal']\"\n",
    "        samples = self.sampler(self.nsamples)\n",
    "\n",
    "        if plane == \"sagittal\":\n",
    "            midx = samples[:, 0]\n",
    "            volume = volume\n",
    "            k = 3\n",
    "\n",
    "        if plane == \"coronal\":\n",
    "            midx = samples[:, 1]\n",
    "            volume = np.transpose(volume, axes=[1, 2, 0])\n",
    "            k = 2\n",
    "\n",
    "        if plane == \"axial\":\n",
    "            midx = samples[:, 2]\n",
    "            volume = np.transpose(volume, axes=[2, 0, 1])\n",
    "            k = 1\n",
    "\n",
    "        if plane == \"combined\":\n",
    "            temp = []\n",
    "            for op in options[:-1]:\n",
    "                temp.append(self._sample_slices(volume, op))\n",
    "            volume = temp\n",
    "#             plt.subplot(1, 3, 1)\n",
    "#             plt.imshow(volume[0][:,:,0])\n",
    "#             plt.subplot(1, 3, 2)\n",
    "#             plt.imshow(volume[1][:,:,0])\n",
    "#             plt.subplot(1, 3, 3)\n",
    "#             plt.imshow(volume[2][:,:,0])\n",
    "#             plt.show()\n",
    "\n",
    "        if not plane == \"combined\":\n",
    "            volume = np.squeeze(volume[midx,:,:])\n",
    "            volume = np.mean(volume, axis=0)\n",
    "            volume = np.rot90(volume, k)\n",
    "            volume = volume[..., None]\n",
    "        return volume\n",
    "\n",
    "\n",
    "    def get_data(self, volume):\n",
    "        # Generate indexes of the batch\n",
    "#         volume = clip(volume, q=90)\n",
    "#         volume = normalize(volume)\n",
    "#         volume = standardize(volume)\n",
    "#         newaffine = np.eye(4)\n",
    "#         newaffine[:3, 3] = -0.5 * (np.array(self.conform_size) - 1)\n",
    "#         os.makedirs('./tmp', exist_ok=True)\n",
    "#         save_vol('./tmp/Pre-processing.nii.gz', volume, newaffine)\n",
    "#         conform_data('./tmp/Pre-processing.nii.gz',\n",
    "#                         './tmp/Conformed.nii.gz',\n",
    "#                         self.conform_size,\n",
    "#                         self.conform_zoom)\n",
    "\n",
    "#         volume = load_vol('./tmp/Conformed.nii.gz')[0]\n",
    "\n",
    "        if self.transform:\n",
    "            volume = self._augmentation(volume)\n",
    "\n",
    "        slices = []\n",
    "        for _ in range(self.nruns):\n",
    "            slices.append(self._sample_slices(volume, \n",
    "                                    plane=\"combined\"))\n",
    "\n",
    "#         if not self.save: \n",
    "#             shutil.rmtree('./tmp')\n",
    "        return slices\n",
    "\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    path = '../sample_vols/defaced/example2.nii.gz'\n",
    "    \n",
    "    vol, _, _ = load_vol(path)\n",
    "    \n",
    "    \n",
    "#     inference_transform_params = {\n",
    "#             \"conform_size\": (64, 64, 64),\n",
    "#             \"conform_zoom\": (4., 4., 4.), \n",
    "#             \"nchannels\": 1, \n",
    "#             \"nruns\": 8,\n",
    "#             \"nsamples\": 24,\n",
    "#             \"save\": False, \n",
    "#             \"transform\": None\n",
    "#         }\n",
    "    \n",
    "#     inference_generator = DataGeneratoronFly(**inference_transform_params)\n",
    "    \n",
    "#     slices = inference_generator.get_data(vol)\n",
    "    \n",
    "#     slices = np.transpose(np.array(slices),axes=[1, 0, 2, 3, 4])\n",
    "#     ds = {}\n",
    "#     ds['axial'] = slices[0]\n",
    "#     ds['coronal'] = slices[1]\n",
    "#     ds['sagittal'] = slices[2]\n",
    "    \n",
    "# #     print(ds)\n",
    "#     print(slices.shape)\n",
    "#         return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import timeit\n",
    "\n",
    "sys.path.append('../defacing')\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from models.modelN import CombinedClassifier\n",
    "# from dataloaders.inference_dataloader import DataGeneratoronFly\n",
    "\n",
    "ROOTDIR = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiment_faced_refaced/exp_face_defaced/exp_add_datasets\"\n",
    "\n",
    "\n",
    "class inferer(object):\n",
    "    \"\"\"\n",
    "       nMontecarlo: for multiple exp for same model\n",
    "       quick: checks for all 3 fold models\n",
    "       mode: method to merge predictions\n",
    "             allowed ['avg', 'max_vote']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, percent=25, nMontecarlo=8, mode=\"avg\"):\n",
    "        r\"\"\"\n",
    "        \"\"\"\n",
    "        inference_transform_params = {\n",
    "            \"conform_size\": (64, 64, 64),\n",
    "            \"conform_zoom\": (4., 4., 4.), \n",
    "            \"nchannels\": 1, \n",
    "            \"nruns\": 8,\n",
    "            \"nsamples\": 32,\n",
    "            \"save\": False, \n",
    "            \"transform\": None\n",
    "        }\n",
    "\n",
    "        self.mode = mode\n",
    "        assert self.mode.lower() in [\n",
    "            \"avg\",\n",
    "            \"max_vote\",\n",
    "        ], \"unknown mode, allowed mode are ['avg', 'max_vote']\"\n",
    "\n",
    "        self.inference_generator = DataGeneratoronFly(**inference_transform_params)\n",
    "        self.model = CombinedClassifier(\n",
    "            input_shape=(64, 64), dropout=0.4, wts_root=None, trainable=True\n",
    "        )\n",
    "        self.model.load_weights(\n",
    "\n",
    "            os.path.abspath(os.path.join(ROOTDIR, \"model_save_dir_{}/weights/combined/best-wts.h5\".format(percent)))\n",
    "        )\n",
    "\n",
    "    def infer(self, vol):\n",
    "        \"\"\"\n",
    "        vol : can be numpy ndarray or path to volume\n",
    "        \"\"\"\n",
    "        slices = self.inference_generator.get_data(vol)\n",
    "        slices = np.transpose(np.array(slices),axes=[1, 0, 2, 3, 4])\n",
    "        \n",
    "        ds = {}\n",
    "        ds['axial'] = slices[0]\n",
    "        ds['coronal'] = slices[1]\n",
    "        ds['sagittal'] = slices[2]\n",
    "    \n",
    "        predictions = self.model.predict(ds)\n",
    "\n",
    "        if self.mode.lower() == \"max_vote\":\n",
    "            predictions = np.round(predictions)\n",
    "            unique_elements = np.unique(predictions)\n",
    "            count_array = np.array(\n",
    "                [\n",
    "                    sum(predictions == unique_element)\n",
    "                    for unique_element in unique_elements\n",
    "                ]\n",
    "            )\n",
    "            pred = (\n",
    "                np.argmax(count_array) if len(count_array) > 1 else unique_elements[0]\n",
    "            )\n",
    "            conf = (\n",
    "                1\n",
    "                if len(count_array) == 1\n",
    "                else count_array[pred] * 1.0 / np.sum(count_array)\n",
    "            )\n",
    "        elif self.mode.lower() == \"avg\":\n",
    "            conf = np.mean(predictions)\n",
    "            pred = np.round(conf)\n",
    "\n",
    "        pred_str = \"faced\" if pred == 1 else \"defaced\"\n",
    "        conf = conf if pred == 1 else 1.0 - conf\n",
    "\n",
    "        print(\"[INFO] Given volume is \" + pred_str + \" with confidence of: {}\".format(conf))\n",
    "        \n",
    "#         del self.model\n",
    "#         K.clear_session()\n",
    "        \n",
    "        return pred, conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "sys.path.append('../defacing')\n",
    "from helpers.utils import load_vol\n",
    "import pandas as pd\n",
    "# from defacing.inference import inferer\n",
    "\n",
    "percentage = [25, 50, 75, 100]\n",
    "for per in percentage:\n",
    "    _inferer = inferer(percent=per)\n",
    "    # path = '../sample_vols/faced/example1.nii.gz'\n",
    "    paths = glob.glob('/work/01329/poldrack/data/mriqc-net/data/test_images/test1_images/*/*')\n",
    "#     paths = glob.glob('/work/01329/poldrack/data/mriqc-net/data/defaced/ds002236_anat/*')\n",
    "\n",
    "    # print(len(paths))\n",
    "    inf = {}\n",
    "\n",
    "    for file in paths:\n",
    "        print(file)\n",
    "        temp = {}\n",
    "        vol, _, _ = load_vol(file)\n",
    "        label, conf = _inferer.infer(vol)\n",
    "        temp['label'] = label\n",
    "        temp['confidence'] = conf\n",
    "        inf[file.split('/')[-2] + '/' + file.split('/')[-1]] = temp\n",
    "    df = pd.DataFrame.from_dict(inf).transpose()\n",
    "    df.to_csv('test_inference_dstest_model_fdefaced_{}.csv'.format(per))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# inference_results_path = 'test_inference_face_refaced.csv'\n",
    "# data = pd.read_csv(inference_results_path)\n",
    "# data.rename(columns={'Unnamed: 0':'volume'}, inplace=True \n",
    "\n",
    "# true_labels = pd.read_csv('/work/06850/sbansal6/maverick2/mriqc-net/2.5D/csv/faced/all.csv')\n",
    "#             volumes, tlabels = true_labels['X'].values, true_labels['Y'].values\n",
    "\n",
    "# tvolumes = {}\n",
    "\n",
    "# for v in range(len(volumes)):\n",
    "    \n",
    "#     tvolumes[volumes[v].split('/')[-2] + '/' + volumes[v].split('/')[-1]] = tlabels[v]\n",
    "    \n",
    "true_labels = pd.read_csv('test1_images_gt.csv')\n",
    "tl_dict = {}\n",
    "for i, row in enumerate(true_labels.values):\n",
    "    \n",
    "    dataset, volume, label = row\n",
    "    l = 0 if 'deface' in label else 1\n",
    "    tl_dict[str(dataset) + '/' + str(volume)] = l\n",
    "    \n",
    "# print(tl_dict)\n",
    "percentage = [25, 50, 75, 100]\n",
    "for per in percentage:\n",
    "    print(\"Percentage Data: \", per)\n",
    "    predicted = pd.read_csv('test_inference_dstest_model_fdefaced_{}.csv'.format(per))\n",
    "    predicted.rename(columns={'Unnamed: 0':'volume'}, inplace=True )\n",
    "\n",
    "    # remove_vols = [\n",
    "    #     'ds001919',\n",
    "    #     'ds000148'\n",
    "    # ]\n",
    "    correct = 0\n",
    "    incorrect = {}\n",
    "    for i, row in enumerate(predicted.values):\n",
    "        volume, confidence, label = row\n",
    "\n",
    "        if volume in tl_dict:\n",
    "            if label == tl_dict[volume]:\n",
    "                correct += 1\n",
    "            else:\n",
    "    #             if 'ds001919' not in volume and 'ds000206' not in volume:\n",
    "                if per == 100:\n",
    "                    print(volume, confidence, label, tl_dict[volume])\n",
    "                incorrect[volume] = {}\n",
    "                incorrect[volume]['label'] = label\n",
    "                incorrect[volume]['confidence'] = confidence\n",
    "\n",
    "    print(len(incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct, len(incorrect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpaths = glob.glob('/work/06850/sbansal6/maverick2/mriqc-shared/test_conformed/*/*')\n",
    "\n",
    "labels = []\n",
    "for c in cpaths:\n",
    "    name = '/'.join(c.rsplit('/', 2)[-2:])\n",
    "    print(name, tl_dict[name], incorrect[name]['label'])\n",
    "    labels.append(tl_dict[name])\n",
    "    \n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('../defacing')\n",
    "\n",
    "import numpy as np\n",
    "# from scipy.misc import toimage\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from helpers.utils import save_vol, load_vol\n",
    "from preprocessing.conform import conform_data\n",
    "from preprocessing.normalization import clip, standardize, normalize\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "conform_size = (128, 128, 128)\n",
    "conform_zoom = (1.5, 1.5, 1.5)\n",
    "\n",
    "testpath = '/work/01329/poldrack/data/mriqc-net/data/test_images/test1_images'\n",
    "\n",
    "# for f in incorrect:\n",
    "    \n",
    "#     dname = f.split('/')[0]\n",
    "#     fname = f.split('/')[1]\n",
    "#     print(dname, fname)\n",
    "    \n",
    "#     file = os.path.join(testpath, f)\n",
    "#     print(file)\n",
    "    \n",
    "#     cpath = '/work/06850/sbansal6/maverick2/mriqc-shared/test_conformed'\n",
    "#     outfile = os.path.join(cpath, dname)\n",
    "#     print(outfile)\n",
    "#     os.makedirs(outfile, exist_ok=True)\n",
    "    \n",
    "#     outfile = os.path.join(outfile, fname)\n",
    "#     print(outfile)\n",
    "#     conform_data(file, \n",
    "#                  out_file=outfile, \n",
    "#                  out_size=conform_size, \n",
    "#                  out_zooms=conform_zoom)\n",
    "    \n",
    "#     vol,_,_ = load_vol(outfile)\n",
    "    \n",
    "#     vol = np.array(vol)\n",
    "#     vol = np.transpose(vol, axes=[1, 2, 0])\n",
    "    \n",
    "#     plt.figure(figsize=(20, 10))\n",
    "#     for i in range(1, 11):\n",
    "#         plt.subplot(1, 10, i)\n",
    "#         plt.imshow(vol[i,:,:])\n",
    "    \n",
    "#     plt.show()\n",
    "    \n",
    "cpaths = glob.glob('/work/06850/sbansal6/maverick2/mriqc-shared/test_conformed/*/*')\n",
    "\n",
    "images = []\n",
    "nslices = 16\n",
    "\n",
    "for v in cpaths:\n",
    "    \n",
    "    \n",
    "    vol, _, _ = load_vol(v)\n",
    "    vol = np.array(vol)\n",
    "    vol = clip(vol, q=90)\n",
    "    vol = normalize(vol)\n",
    "    vol = standardize(vol)\n",
    "    \n",
    "    # get Coronal\n",
    "    vol = np.transpose(vol, (2, 0, 1))\n",
    "#     plt.figure(figsize=(20, 10))\n",
    "#     for i in range(1, 31):\n",
    "#         plt.subplot(1, 31, i)\n",
    "#         plt.imshow(vol[i+24,:,:])\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    for i in range(1, nslices+1):\n",
    "        im = vol[i+64, :, :]\n",
    "        images.append(im)\n",
    "        \n",
    "print(len(images))\n",
    "\n",
    "images = np.array(images)*255\n",
    "\n",
    "total_width = 128*nslices\n",
    "total_height = 128*len(cpaths)\n",
    "    \n",
    "new_im = Image.new('RGB',(total_height, total_width))\n",
    "\n",
    "nrows = len(cpaths)\n",
    "ncols = nslices\n",
    "im_size = 128\n",
    "\n",
    "print(new_im)\n",
    "for im in range(len(images)):\n",
    "    imag = Image.fromarray(images[im])\n",
    "#     print(images[im].shape)\n",
    "    x, y = im_size*int(im/ncols), im_size*int(im%ncols)\n",
    "    new_im.paste(imag, (x,y))\n",
    "    if y % 128 == 0:\n",
    "        draw = ImageDraw.Draw(new_im)\n",
    "        text = str(labels[int(im % len(images)/nslices)])\n",
    "#         print(text)\n",
    "        draw.text((x,y), text)\n",
    "    \n",
    "\n",
    "new_im.save('mosaic/' + 'test_images_misclassified_axial.png')\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(new_im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "f = open(\"discovered.txt\", \"r\")\n",
    "discovered = f.read().splitlines()\n",
    "\n",
    "defaced = os.listdir('/work/01329/poldrack/data/mriqc-net/data/defaced')\n",
    "\n",
    "for ds in defaced:\n",
    "    for dis in discovered:\n",
    "        if dis in ds:\n",
    "            print(dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for vol in data['volume'].values:\n",
    "    conf = data.loc[data['volume'] == vol, 'confidence'].item()\n",
    "    label = data.loc[data['volume'] == vol, 'label'].item()\n",
    "    \n",
    "    if vol in tvolumes:\n",
    "        if label == tvolumes[vol]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "    else:\n",
    "        print(vol)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     || 10.6 MB 12 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.10.0 in /work/06850/sbansal6/anaconda3/lib/python3.7/site-packages (from tensorboard) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from tensorboard) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from tensorboard) (0.33.6)\n",
      "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from tensorboard) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /work/06850/sbansal6/anaconda3/lib/python3.7/site-packages (from tensorboard) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from tensorboard) (0.9.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /work/06850/sbansal6/anaconda3/lib/python3.7/site-packages (from tensorboard) (41.0.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from tensorboard) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from tensorboard) (1.10.1)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from tensorboard) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from tensorboard) (3.11.2)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from tensorboard) (1.26.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /work/06850/sbansal6/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2019.6.16)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.7)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /home1/06850/sbansal6/.local/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /work/06850/sbansal6/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "\u001b[31mERROR: tensorflow-gpu 2.1.0 has requirement tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 2.4.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.1.0 has requirement tensorboard<2.2.0,>=2.1.0, but you'll have tensorboard 2.4.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorboard\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.3.0\n",
      "    Uninstalling tensorboard-2.3.0:\n",
      "      Successfully uninstalled tensorboard-2.3.0\n",
      "Successfully installed tensorboard-2.4.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3 is available.\n",
      "You should consider upgrading via the '/work/06850/sbansal6/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --user --upgrade tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>tag</th>\n",
       "      <th>step</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>0</td>\n",
       "      <td>0.890067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>2</td>\n",
       "      <td>0.967634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>3</td>\n",
       "      <td>0.968192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>4</td>\n",
       "      <td>0.972656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>5</td>\n",
       "      <td>0.975446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>6</td>\n",
       "      <td>0.974888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>7</td>\n",
       "      <td>0.974330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>8</td>\n",
       "      <td>0.967076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>9</td>\n",
       "      <td>0.973214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>10</td>\n",
       "      <td>0.978237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>11</td>\n",
       "      <td>0.974888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>12</td>\n",
       "      <td>0.968192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>13</td>\n",
       "      <td>0.982701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>14</td>\n",
       "      <td>0.967634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>2</td>\n",
       "      <td>0.996739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>3</td>\n",
       "      <td>0.997440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>4</td>\n",
       "      <td>0.998128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>5</td>\n",
       "      <td>0.997259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>6</td>\n",
       "      <td>0.997836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>7</td>\n",
       "      <td>0.997953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>8</td>\n",
       "      <td>0.997287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>9</td>\n",
       "      <td>0.998492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>10</td>\n",
       "      <td>0.998848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>11</td>\n",
       "      <td>0.998746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>12</td>\n",
       "      <td>0.998021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>13</td>\n",
       "      <td>0.999273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_auc</td>\n",
       "      <td>14</td>\n",
       "      <td>0.997965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16170</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>0</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16171</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>1</td>\n",
       "      <td>66.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16172</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>2</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16173</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>3</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16174</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>4</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16175</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>5</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16176</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>6</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16177</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>7</td>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16178</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>8</td>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16179</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>9</td>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16180</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>10</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16181</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>11</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16182</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>12</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16183</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>13</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16184</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tn</td>\n",
       "      <td>14</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16185</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>0</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16186</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>1</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16187</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>2</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16188</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>3</td>\n",
       "      <td>53.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16189</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>4</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16190</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>5</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16191</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>6</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16192</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>7</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16193</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>8</td>\n",
       "      <td>47.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16194</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>9</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16195</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>10</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16196</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>11</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16197</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>12</td>\n",
       "      <td>54.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16198</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>13</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16199</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>14</td>\n",
       "      <td>52.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16200 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 run             tag  step  \\\n",
       "0              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     0   \n",
       "1              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     1   \n",
       "2              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     2   \n",
       "3              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     3   \n",
       "4              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     4   \n",
       "5              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     5   \n",
       "6              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     6   \n",
       "7              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     7   \n",
       "8              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     8   \n",
       "9              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     9   \n",
       "10             train_test_fold_1/tb_logs/axial/train  epoch_accuracy    10   \n",
       "11             train_test_fold_1/tb_logs/axial/train  epoch_accuracy    11   \n",
       "12             train_test_fold_1/tb_logs/axial/train  epoch_accuracy    12   \n",
       "13             train_test_fold_1/tb_logs/axial/train  epoch_accuracy    13   \n",
       "14             train_test_fold_1/tb_logs/axial/train  epoch_accuracy    14   \n",
       "15             train_test_fold_1/tb_logs/axial/train       epoch_auc     0   \n",
       "16             train_test_fold_1/tb_logs/axial/train       epoch_auc     1   \n",
       "17             train_test_fold_1/tb_logs/axial/train       epoch_auc     2   \n",
       "18             train_test_fold_1/tb_logs/axial/train       epoch_auc     3   \n",
       "19             train_test_fold_1/tb_logs/axial/train       epoch_auc     4   \n",
       "20             train_test_fold_1/tb_logs/axial/train       epoch_auc     5   \n",
       "21             train_test_fold_1/tb_logs/axial/train       epoch_auc     6   \n",
       "22             train_test_fold_1/tb_logs/axial/train       epoch_auc     7   \n",
       "23             train_test_fold_1/tb_logs/axial/train       epoch_auc     8   \n",
       "24             train_test_fold_1/tb_logs/axial/train       epoch_auc     9   \n",
       "25             train_test_fold_1/tb_logs/axial/train       epoch_auc    10   \n",
       "26             train_test_fold_1/tb_logs/axial/train       epoch_auc    11   \n",
       "27             train_test_fold_1/tb_logs/axial/train       epoch_auc    12   \n",
       "28             train_test_fold_1/tb_logs/axial/train       epoch_auc    13   \n",
       "29             train_test_fold_1/tb_logs/axial/train       epoch_auc    14   \n",
       "...                                              ...             ...   ...   \n",
       "16170  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn     0   \n",
       "16171  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn     1   \n",
       "16172  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn     2   \n",
       "16173  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn     3   \n",
       "16174  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn     4   \n",
       "16175  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn     5   \n",
       "16176  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn     6   \n",
       "16177  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn     7   \n",
       "16178  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn     8   \n",
       "16179  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn     9   \n",
       "16180  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn    10   \n",
       "16181  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn    11   \n",
       "16182  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn    12   \n",
       "16183  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn    13   \n",
       "16184  train_test_fold_9/tb_logs/sagittal/validation        epoch_tn    14   \n",
       "16185  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp     0   \n",
       "16186  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp     1   \n",
       "16187  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp     2   \n",
       "16188  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp     3   \n",
       "16189  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp     4   \n",
       "16190  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp     5   \n",
       "16191  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp     6   \n",
       "16192  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp     7   \n",
       "16193  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp     8   \n",
       "16194  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp     9   \n",
       "16195  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp    10   \n",
       "16196  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp    11   \n",
       "16197  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp    12   \n",
       "16198  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp    13   \n",
       "16199  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp    14   \n",
       "\n",
       "           value  \n",
       "0       0.890067  \n",
       "1       0.963728  \n",
       "2       0.967634  \n",
       "3       0.968192  \n",
       "4       0.972656  \n",
       "5       0.975446  \n",
       "6       0.974888  \n",
       "7       0.974330  \n",
       "8       0.967076  \n",
       "9       0.973214  \n",
       "10      0.978237  \n",
       "11      0.974888  \n",
       "12      0.968192  \n",
       "13      0.982701  \n",
       "14      0.967634  \n",
       "15      0.943074  \n",
       "16      0.995813  \n",
       "17      0.996739  \n",
       "18      0.997440  \n",
       "19      0.998128  \n",
       "20      0.997259  \n",
       "21      0.997836  \n",
       "22      0.997953  \n",
       "23      0.997287  \n",
       "24      0.998492  \n",
       "25      0.998848  \n",
       "26      0.998746  \n",
       "27      0.998021  \n",
       "28      0.999273  \n",
       "29      0.997965  \n",
       "...          ...  \n",
       "16170  70.000000  \n",
       "16171  66.000000  \n",
       "16172  62.000000  \n",
       "16173  62.000000  \n",
       "16174  64.000000  \n",
       "16175  67.000000  \n",
       "16176  64.000000  \n",
       "16177  60.000000  \n",
       "16178  68.000000  \n",
       "16179  67.000000  \n",
       "16180  45.000000  \n",
       "16181  64.000000  \n",
       "16182  62.000000  \n",
       "16183  65.000000  \n",
       "16184  64.000000  \n",
       "16185  18.000000  \n",
       "16186  16.000000  \n",
       "16187  45.000000  \n",
       "16188  53.000000  \n",
       "16189  52.000000  \n",
       "16190  47.000000  \n",
       "16191  50.000000  \n",
       "16192  54.000000  \n",
       "16193  47.000000  \n",
       "16194  48.000000  \n",
       "16195  65.000000  \n",
       "16196  54.000000  \n",
       "16197  54.000000  \n",
       "16198  49.000000  \n",
       "16199  52.000000  \n",
       "\n",
       "[16200 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from packaging import version\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import tensorboard as tb\n",
    "\n",
    "# tensorboard at https://tensorboard.dev/experiment/wJCGhllKTG2Mjr3w811Xnw/\n",
    "\n",
    "experiment_id = \"V9pzWxW5QlKlzv32lBg3BA\"\n",
    "experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
    "df = experiment.get_scalars()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.466666666666665\n"
     ]
    }
   ],
   "source": [
    "temp = df[df[\"run\"].str.contains('combined/train')]\n",
    "# print(temp[temp[\"tag\"].str.contains('epoch_accuracy')])\n",
    "print(sum(temp[(temp['tag'].str.contains('epoch_fp')) & (temp['step'] == 14)]['value'])/15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9788666135919519\n"
     ]
    }
   ],
   "source": [
    "print(901.6666666666666/(901.6666666666666 + 19.466666666666665))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_paths = glob.glob('/work/06850/sbansal6/maverick2/mriqc-shared/experiment_data_all/F15_csv/train/*')\n",
    "\n",
    "# combined_train_csv = pd.read_csv()\n",
    "average_train_accuracy = 0\n",
    "for csv in csv_paths:\n",
    "    accuracies = pd.read_csv(csv)[\"Value\"].values\n",
    "    average_train_accuracy += accuracies[-1]\n",
    "    \n",
    "average_train_accuracy = average_train_accuracy/15\n",
    "print(\"Average 15 fold training accuracy for combined model: \", average_train_accuracy)\n",
    "\n",
    "csv_paths = glob.glob('/work/06850/sbansal6/maverick2/mriqc-shared/experiment_data_all/F15_csv/validation/*')\n",
    "\n",
    "# combined_train_csv = pd.read_csv()\n",
    "average_valid_accuracy = 0\n",
    "for csv in csv_paths:\n",
    "    accuracies = pd.read_csv(csv)[\"Value\"].values\n",
    "    average_valid_accuracy += accuracies[-1]\n",
    "#     print(accuracies[-1])\n",
    "    \n",
    "average_valid_accuracy = average_valid_accuracy/15\n",
    "print(\"Average 15 fold validation accuracy for combined model: \", average_valid_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
