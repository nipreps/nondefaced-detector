{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run this notebook\n",
    "\n",
    "1. Open 2 terminal tabs\n",
    "2. From Terminal A, ssh into TACC the normal way - `ssh <username>@maverick2.tacc.utexas.edu`\n",
    "3. From terminal B, ssh into TACC using the command - `ssh -L localhost:8888:127.0.0.1:8888 -L localhost:6006:localhost:6006 sbansal6@maverick2.tacc.utexas.edu`\n",
    "4. From terminal A, request some resources on the GPU compute resource - `idev -p gtx -L work -m 180`\n",
    "5. From terminal B, log into the compute resource provisioned using - `ssh -L localhost:8888:127.0.0.1:8888 -L localhost:6006:localhost:6006 <resource-name>`\n",
    "6. From terminal B, run jupyter notebook - `jupyter lab --port 8888`\n",
    "7. You should be able to open the jupyter notebook on the browser of your local computer.\n",
    "\n",
    "# TACC Assumptions\n",
    "\n",
    "Following are the requirements:\n",
    "\n",
    "Packages:\n",
    "\n",
    "```\n",
    "tensorflow                         2.1.0\n",
    "tensorflow-estimator               2.1.0\n",
    "tensorflow-gpu                     2.1.0\n",
    "tensorflow-probability             0.9.0\n",
    "tensorboard                        2.3.0\n",
    "```\n",
    "\n",
    "Modules:\n",
    "\n",
    "`module load intel/17.0.4 python3/3.6.3 cuda/10.0 cudnn/7.6.2 nccl/2.4.7 tacc-singularity/3.4.2`\n",
    "\n",
    "In addition to the above module you might need to add the following to your path:\n",
    "\n",
    "`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/apps/cuda/10.1/targets/x86_64-linux/lib`\n",
    "\n",
    "\n",
    "Because of some missing library issues in TACC and depending on the missing library you might need to find and add the library somewhere accessible and add that path to the `LD_LIBRARY_PATH`:\n",
    "\n",
    "```\n",
    "login1.maverick2(1006)$ find -O3 -L /opt/apps/ -name \"*libcupti.so.10.1*\"\n",
    "/opt/apps/cuda/10.1/extras/CUPTI/lib64/libcupti.so.10.1\n",
    "/opt/apps/cuda/10.1/extras/CUPTI/lib64/libcupti.so.10.1.208\n",
    "\n",
    "login1.maverick2(1006)$ cp /opt/apps/cuda/10.1/extras/CUPTI/lib64/libcupti.so.10.1 $WORK/libcupti.so.10.1\n",
    "login1.maverick2(1006)$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$WORK\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Details\n",
    "\n",
    "There are 2 datasets - \n",
    "\n",
    "**A**: orig_faced + defaced volumes\n",
    "\n",
    "**B**: refaced + defaced volumes\n",
    "\n",
    "\n",
    "1. Randomly select 49 volumes from faced and refaced datasets. \n",
    "2. Generate a new dataset T_A, with these 49 faced + the corresponding 49 defaced, and T_B with the 49 defaced + 49 re-faced.\n",
    "3. Put T_A and T_B aside\n",
    "4. Run your 15-fold CV on the remainder:\n",
    "\n",
    "**A_2 (N = 1000x2)**: 1000 faced + 1000 defaced\n",
    "\n",
    "**B_2 (N = 1000x2)**: 1000 defaced + 1000 re-faced\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 and Step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998 98\n",
      "1983 100\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "# Define paths\n",
    "ROOT_DIR = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/conformed'\n",
    "\n",
    "face_path = os.path.join(ROOT_DIR, 'face/128')\n",
    "defaced_path = os.path.join(ROOT_DIR, 'face_defaced/128')\n",
    "refaced_path = os.path.join(ROOT_DIR, 'face_refaced/128')\n",
    "\n",
    "paths_d = []\n",
    "paths_f = []\n",
    "paths_r = []\n",
    "\n",
    "for path in glob(defaced_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_d.append(path)\n",
    "    \n",
    "for path in glob(refaced_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_r.append(path)\n",
    "    \n",
    "for path in glob(face_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_f.append(path)\n",
    "    \n",
    "\n",
    "def generate_datasets(fpaths, dpaths, size, typ ='faced'):\n",
    "    \n",
    "    if typ not in ['faced', 'refaced']:\n",
    "        print(\"Incorrect value for t. Choose from [faced, refaced]\")\n",
    "        return\n",
    "    \n",
    "    random.shuffle(fpaths)\n",
    "    test_f = fpaths[:size]\n",
    "    main_f = fpaths[size:]\n",
    "\n",
    "    test_d = []\n",
    "    for t in test_f:\n",
    "        if typ == 'faced':\n",
    "            test_d.append(t.replace('face', 'face_defaced'))\n",
    "        \n",
    "        if typ == 'refaced':\n",
    "            DS = t.split('/')[-2]\n",
    "            sub = t.split('/')[-1].replace('_defaced_refaced', '').split('.nii.gz')[0]\n",
    "            search_pattern = os.path.join(DS, sub)\n",
    "            \n",
    "            # match pattern from defaced dataset\n",
    "            for _d in dpaths:\n",
    "                if search_pattern in _d:\n",
    "                    test_d.append(_d)\n",
    "                \n",
    "\n",
    "    test = test_f + test_d\n",
    "    labels_test = [1]*len(test_f) + [0]*len(test_d)\n",
    "    \n",
    "    # remove T_A_D from defaced volume set\n",
    "    main_d = list(set(dpaths) - set(test_d))\n",
    "    \n",
    "    labels_main = [1]*len(main_f) + [0]*len(main_d)\n",
    "    main = main_f + main_d\n",
    "    \n",
    "    return main, labels_main, test, labels_test\n",
    "\n",
    "A_2, L_A_2, T_A, L_T_A = generate_datasets(paths_f, paths_d, 49, typ='faced')\n",
    "B_2, L_B_2, T_B, L_T_B = generate_datasets(paths_r, paths_d, 49, typ='refaced')\n",
    "\n",
    "print(len(A_2), len(T_A))\n",
    "print(len(B_2), len(T_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate n-fold CV Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "\n",
    "def generate_CSV(paths, labels, save_path, test_paths=None, test_labels=None, n=15, mode='CV'):\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"X\"] = paths\n",
    "    df[\"Y\"] = labels\n",
    "    df.to_csv(os.path.join(save_path, \"all.csv\"))\n",
    "    \n",
    "    if mode == 'CV':\n",
    "        SPLITS = n\n",
    "        skf = StratifiedKFold(n_splits=SPLITS)\n",
    "        fold_no = 1\n",
    "\n",
    "        for train_index, test_index in skf.split(paths, labels):\n",
    "            out_path = os.path.join(save_path, \"train_test_fold_{}/csv/\".format(fold_no))\n",
    "\n",
    "            if not os.path.exists(out_path):\n",
    "                os.makedirs(out_path)\n",
    "\n",
    "            image_train, image_test = (\n",
    "                itemgetter(*train_index)(paths),\n",
    "                itemgetter(*test_index)(paths),\n",
    "            )\n",
    "\n",
    "            label_train, label_test = (\n",
    "                itemgetter(*train_index)(labels),\n",
    "                itemgetter(*test_index)(labels),\n",
    "            )\n",
    "\n",
    "            train_data = {\"X\": image_train , \"Y\": label_train}\n",
    "            df_train = pd.DataFrame(train_data)\n",
    "            df_train.to_csv(os.path.join(out_path, \"training.csv\"), index=False)\n",
    "\n",
    "            validation_data = {\"X\": image_test, \"Y\": label_test}\n",
    "            df_validation = pd.DataFrame(validation_data)\n",
    "            df_validation.to_csv(os.path.join(out_path, \"validation.csv\"), index=False)\n",
    "\n",
    "            fold_no += 1\n",
    "    else:\n",
    "        train_data = {\"X\": paths , \"Y\": labels}\n",
    "        df_train = pd.DataFrame(train_data)\n",
    "        df_train.to_csv(os.path.join(save_path, \"training.csv\"), index=False)\n",
    "        \n",
    "        test_data = {\"X\": test_paths , \"Y\": test_labels}\n",
    "        df_test = pd.DataFrame(test_data)\n",
    "        df_test.to_csv(os.path.join(save_path, \"testing.csv\"), index=False)\n",
    "        \n",
    "ROOTDIR = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments'\n",
    "\n",
    "## CROSS VALIDATION\n",
    "# generate_CSV(A_2, L_A_2, \"experiments/experiment_A/csv_F15\")\n",
    "generate_CSV(B_2, L_B_2, os.path.join(ROOTDIR, \"experiment_B/128/csv_F15\"), mode='CV')\n",
    "\n",
    "\n",
    "## DEFINE A ROOT DIR where all the data will be stored <<<<<\n",
    "# ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/experiments' \n",
    "\n",
    "## FULL DATASET\n",
    "# generate_CSV(A_2,\n",
    "#              L_A_2,\n",
    "#              os.path.join(ROOTDIR, 'experiment_A/128/csv_full'),\n",
    "#              test_paths=T_A,\n",
    "#              test_labels=L_T_A,\n",
    "#              mode='full')\n",
    "\n",
    "# generate_CSV(B_2,\n",
    "#              L_B_2,\n",
    "#              os.path.join(ROOTDIR, 'experiment_B/128/csv_full'),\n",
    "#              test_paths=T_B,\n",
    "#              test_labels=L_T_B,\n",
    "#              mode='full')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tfrecords for n-fold CV datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD:  1\n",
      "617/617 [==============================] - 173s 277ms/step\n",
      "133/133 [==============================] - 12s 79ms/step\n",
      "FOLD:  2\n",
      "617/617 [==============================] - 124s 199ms/step\n",
      "133/133 [==============================] - 12s 80ms/step\n",
      "FOLD:  3\n",
      "617/617 [==============================] - 48s 75ms/step\n",
      "133/133 [==============================] - 8s 56ms/step\n",
      "FOLD:  4\n",
      "617/617 [==============================] - 53s 84ms/step\n",
      "132/132 [==============================] - 7s 44ms/step\n",
      "FOLD:  5\n",
      "617/617 [==============================] - 40s 62ms/step\n",
      "132/132 [==============================] - 4s 20ms/step\n",
      "FOLD:  6\n",
      "617/617 [==============================] - 40s 62ms/step\n",
      "132/132 [==============================] - 4s 21ms/step\n",
      "FOLD:  7\n",
      "617/617 [==============================] - 43s 67ms/step\n",
      "132/132 [==============================] - 4s 20ms/step\n",
      "FOLD:  8\n",
      "617/617 [==============================] - 40s 62ms/step\n",
      "132/132 [==============================] - 4s 22ms/step\n",
      "FOLD:  9\n",
      "617/617 [==============================] - 43s 68ms/step\n",
      "132/132 [==============================] - 4s 20ms/step\n",
      "FOLD:  10\n",
      "617/617 [==============================] - 41s 64ms/step\n",
      "132/132 [==============================] - 4s 20ms/step\n",
      "FOLD:  11\n",
      "617/617 [==============================] - 39s 61ms/step\n",
      "132/132 [==============================] - 4s 21ms/step\n",
      "FOLD:  12\n",
      "617/617 [==============================] - 42s 66ms/step\n",
      "132/132 [==============================] - 4s 21ms/step\n",
      "FOLD:  13\n",
      "617/617 [==============================] - 40s 62ms/step\n",
      "132/132 [==============================] - 4s 24ms/step\n",
      "FOLD:  14\n",
      "617/617 [==============================] - 43s 68ms/step\n",
      "132/132 [==============================] - 4s 20ms/step\n",
      "FOLD:  15\n",
      "617/617 [==============================] - 42s 65ms/step\n",
      "132/132 [==============================] - 4s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_tfrecords(csv_path, records_save_path, mode='CV'):\n",
    "    \n",
    "    os.makedirs(records_save_path, exist_ok=True)\n",
    "\n",
    "    train_csv_path = os.path.join(csv_path, \"training.csv\")\n",
    "    \n",
    "\n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "    train_D = list(zip(train_paths, train_labels))\n",
    "    \n",
    "    random.shuffle(train_D)\n",
    "\n",
    "    train_write_path = os.path.join(records_save_path, 'data-train_shard-{shard:03d}.tfrec')\n",
    "    \n",
    "    nobrainer.tfrecord.write(\n",
    "        features_labels=train_D,\n",
    "        filename_template=train_write_path,\n",
    "        examples_per_shard=3)\n",
    "    \n",
    "    if mode =='CV':\n",
    "        valid_csv_path = os.path.join(csv_path, \"validation.csv\")\n",
    "        valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "        valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "        valid_D = list(zip(valid_paths, valid_labels))\n",
    "        random.shuffle(valid_D)\n",
    "        valid_write_path = os.path.join(records_save_path, 'data-valid_shard-{shard:03d}.tfrec')\n",
    "\n",
    "        nobrainer.tfrecord.write(\n",
    "            features_labels=valid_D,\n",
    "            filename_template=valid_write_path,\n",
    "            examples_per_shard=1)\n",
    "    \n",
    "    if mode == 'test':\n",
    "        test_csv_path = os.path.join(csv_path, \"testing.csv\")\n",
    "        test_paths = pd.read_csv(test_csv_path)[\"X\"].values\n",
    "        test_labels = pd.read_csv(test_csv_path)[\"Y\"].values\n",
    "        test_D = list(zip(test_paths, test_labels))\n",
    "        random.shuffle(test_D)\n",
    "        test_write_path = os.path.join(records_save_path, 'data-test_shard-{shard:03d}.tfrec')\n",
    "\n",
    "        nobrainer.tfrecord.write(\n",
    "            features_labels=test_D,\n",
    "            filename_template=test_write_path,\n",
    "            examples_per_shard=1)\n",
    "\n",
    "ROOTDIR = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments'\n",
    "\n",
    "# Cross-Validation \n",
    "SPLITS = 15\n",
    "for fold in range(1, SPLITS+1):\n",
    "    print(\"FOLD: \", fold)\n",
    "    csv_path = os.path.join(\n",
    "        ROOTDIR, \"experiment_B/128/csv_F15/train_test_fold_{}/csv/\".format(fold)\n",
    "    )\n",
    "    \n",
    "    tf_records_dir = os.path.join(\n",
    "        ROOTDIR, \"experiment_B/128/tfrecords_F15/tfrecords_fold_{}/\".format(fold)\n",
    "    )\n",
    "    generate_tfrecords(csv_path, tf_records_dir)\n",
    "\n",
    "\n",
    "# Test (full dataset)\n",
    "# experiment_A\n",
    "# csv_path = os.path.join(ROOT_DIR, \"experiment_A/128/csv_full\")\n",
    "# tf_records_dir = os.path.join(ROOT_DIR, \"experiment_A/128/tfrecords_full\")\n",
    "# generate_tfrecords(csv_path, tf_records_dir, mode='test')\n",
    "\n",
    "# experiment_B\n",
    "# csv_path = os.path.join(ROOT_DIR, \"experiment_B/128/csv_full\")\n",
    "# tf_records_dir = os.path.join(ROOT_DIR, \"experiment_B/128/tfrecords_full\")\n",
    "# generate_tfrecords(csv_path, tf_records_dir, mode='test')\n",
    "\n",
    "## Main held-out Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nobrainer\n",
      "  Downloading nobrainer-0.0.3-py3-none-any.whl (66 kB)\n",
      "\u001b[K     |████████████████████████████████| 66 kB 2.7 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.5 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-image\n",
      "  Downloading scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nibabel\n",
      "  Downloading nibabel-3.2.1-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-probability>=0.7.0\n",
      "  Downloading tensorflow_probability-0.12.1-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8 MB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from nobrainer) (1.19.5)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-0.24.1-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 22.2 MB 380 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2\n",
      "  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "\u001b[K     |████████████████████████████████| 510 kB 12.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->nobrainer) (3.3.3)\n",
      "Collecting scipy>=1.0.1\n",
      "  Downloading scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 25.9 MB 8.4 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting imageio>=2.3.0\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting networkx>=2.0\n",
      "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 17.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->nobrainer) (8.1.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.6/dist-packages (from nibabel->nobrainer) (20.8)\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.7.0->nobrainer) (0.4.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.7.0->nobrainer) (1.15.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability>=0.7.0->nobrainer) (4.4.2)\n",
      "Collecting cloudpickle>=1.3\n",
      "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.5-cp36-cp36m-manylinux1_x86_64.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Collecting joblib>=0.11\n",
      "  Downloading joblib-1.0.0-py3-none-any.whl (302 kB)\n",
      "\u001b[K     |████████████████████████████████| 302 kB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->nobrainer) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->nobrainer) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->nobrainer) (0.10.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=72e5a0446614f46f714c758e41fb3051635cc11abd8277d057beb1293c1b82ab\n",
      "  Stored in directory: /root/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: click, PyWavelets, scipy, imageio, tifffile, networkx, scikit-image, nibabel, cloudpickle, dm-tree, tensorflow-probability, nobrainer, threadpoolctl, joblib, scikit-learn, sklearn, pytz, pandas\n",
      "Successfully installed PyWavelets-1.1.1 click-7.1.2 cloudpickle-1.6.0 dm-tree-0.1.5 imageio-2.9.0 joblib-1.0.0 networkx-2.5 nibabel-3.2.1 nobrainer-0.0.3 pandas-1.1.5 pytz-2021.1 scikit-image-0.17.2 scikit-learn-0.24.1 scipy-1.5.4 sklearn-0.0 tensorflow-probability-0.12.1 threadpoolctl-2.1.0 tifffile-2020.9.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nobrainer sklearn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize dataset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RepeatDataset shapes: ((8, 64, 64, 1), (8, 1)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "import nobrainer\n",
    "from nobrainer.io import _is_gzipped\n",
    "from nobrainer.volume import to_blocks\n",
    "import sys, os\n",
    "sys.path.append('../defacing')\n",
    "from preprocessing.augmentation import VolumeAugmentations, SliceAugmentations\n",
    "from helpers.utils import load_vol\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "DISTRIBUTION = load_vol('../defacing/helpers/distribution.nii.gz')[0]\n",
    "DISTRIBUTION /= DISTRIBUTION.sum()\n",
    "COM = np.unravel_index(int(np.sum(DISTRIBUTION.ravel()*np.arange(len(DISTRIBUTION.ravel())))/np.sum(DISTRIBUTION.ravel())), DISTRIBUTION.shape)\n",
    "\n",
    "\n",
    "# sampling from augmented distribution is same as augmenting the sampled points\n",
    "# augmenting distribution at every iteration is expensive, so this way\n",
    "sampler = lambda n_slices, distribution = DISTRIBUTION, threshold = 0.1: np.array([ np.unravel_index(\n",
    "          np.random.choice(np.arange(np.prod(distribution.shape)),\n",
    "                                     p = distribution.ravel()),\n",
    "          distribution.shape) + (+1 if np.random.randn() > 0.5 else -1)*np.random.randint(0, \n",
    "                                        int(distribution.shape[0]*threshold) + 1, 3) for _ in range(n_slices)]) \n",
    "\n",
    "\n",
    "three_d_augmentations = {'rotation': 0.5,\n",
    "                         'translation': 0.5,\n",
    "                         'noop': 0.3\n",
    "                        }\n",
    "\n",
    "augmentvolume = VolumeAugmentations(DISTRIBUTION, three_d_augmentations)\n",
    "\n",
    "two_d_augmentations = {'rotation': 0.5,\n",
    "                       'fliplr': 0.5,\n",
    "                       'flipud': 0.5,\n",
    "                       'zoom': 0.5,\n",
    "                       'noop': 0.3\n",
    "                      }\n",
    "\n",
    "# augmentslice = VolumeAugmentations(DISTRIBUTION, two_d_augmentations)\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    file_pattern,\n",
    "    n_classes,\n",
    "    batch_size,\n",
    "    volume_shape,\n",
    "    plane,\n",
    "    n_slices = 24,\n",
    "    block_shape=None,\n",
    "    n_epochs=None,\n",
    "    mapping=None,\n",
    "    augment=False,\n",
    "    shuffle_buffer_size=None,\n",
    "    num_parallel_calls=AUTOTUNE,\n",
    "    mode='train',\n",
    "):\n",
    "\n",
    "    \"\"\" Returns tf.data.Dataset after preprocessing from\n",
    "    tfrecords for training and validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_pattern:\n",
    "\n",
    "    n_classes:\n",
    "    \"\"\"\n",
    "\n",
    "    files = glob.glob(file_pattern)\n",
    "\n",
    "    if not files:\n",
    "        raise ValueError(\"no files found for pattern '{}'\".format(file_pattern))\n",
    "\n",
    "    compressed = _is_gzipped(files[0])\n",
    "    shuffle = bool(shuffle_buffer_size)\n",
    "\n",
    "    ds = nobrainer.dataset.tfrecord_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        volume_shape=volume_shape,\n",
    "        shuffle=shuffle,\n",
    "        scalar_label=True,\n",
    "        compressed=compressed,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "    )\n",
    "\n",
    "    # if augment:\n",
    "    #     ds = ds.map(\n",
    "    #         lambda x, y: tf.cond(\n",
    "    #             tf.random.uniform((1,)) > 0.5,\n",
    "    #             true_fn=lambda: apply_augmentations(x, y),\n",
    "    #             false_fn=lambda: (x, y),\n",
    "    #         ),\n",
    "    #         num_parallel_calls=num_parallel_calls,\n",
    "    #     )\n",
    "\n",
    "    def _ss(x, y):\n",
    "        if augment:\n",
    "            if three_d_augmentations['noop'] < 1:\n",
    "                x, y = augmentvolume(x,y)\n",
    "        x, y = structural_slice(x, y, \n",
    "                                plane, \n",
    "                                n_slices, \n",
    "                                augment, \n",
    "                                augmentvolume.distribution)\n",
    "        return (x, y)\n",
    "    \n",
    "    \n",
    "    ds = ds.map(_ss, num_parallel_calls)\n",
    "    \n",
    "    ds = ds.prefetch(buffer_size=batch_size)\n",
    "\n",
    "    if batch_size is not None:\n",
    "        ds = ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        \n",
    "    if mode == 'train':\n",
    "        if shuffle_buffer_size:\n",
    "            ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "        # Repeat the dataset n_epochs times\n",
    "        ds = ds.repeat(n_epochs)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def structural_slice(x, y, \n",
    "                plane, \n",
    "                n_slices = 4, \n",
    "                augment = False, \n",
    "                distribution = DISTRIBUTION):\n",
    "\n",
    "    \"\"\" Transpose dataset based on the plane\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "\n",
    "    y:\n",
    "\n",
    "    plane:\n",
    "    \n",
    "    n:\n",
    "\n",
    "    augment:\n",
    "    \"\"\"\n",
    "\n",
    "    threshold = 0.1 if augment else 0.0 \n",
    "    options = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "    shape = np.array(x.shape)\n",
    "\n",
    "    if isinstance(plane, str) and plane in options:\n",
    "        idxs = sampler(n_slices, \n",
    "                        distribution, \n",
    "                        threshold)\n",
    "\n",
    "        if plane == \"axial\":\n",
    "            idx = np.random.randint(shape[0]**0.5)\n",
    "            midx = idxs[:, 0]\n",
    "            x = x\n",
    "\n",
    "        if plane == \"coronal\":\n",
    "            idx = np.random.randint(shape[1]**0.5)\n",
    "            midx = idxs[:, 1]\n",
    "            x = tf.transpose(x, perm=[1, 2, 0])\n",
    "\n",
    "\n",
    "        if plane == \"sagittal\":\n",
    "            idx = np.random.randint(shape[2]**0.5)\n",
    "            midx = idxs[:, 2]\n",
    "            x = tf.transpose(x, perm=[2, 0, 1])\n",
    "\n",
    "\n",
    "        if plane == \"combined\":\n",
    "            temp = {}\n",
    "            for op in options[:-1]:\n",
    "                temp[op] = structural_slice(x, y, \n",
    "                                            op, \n",
    "                                            n_slices, \n",
    "                                            augment, \n",
    "                                            distribution)[0]\n",
    "            x = temp\n",
    "\n",
    "        if not plane == \"combined\":\n",
    "            x = tf.squeeze(tf.gather_nd(x, midx.reshape(n_slices, 1, 1)), axis=1)\n",
    "            x = tf.math.reduce_mean(x, axis=0)\n",
    "            x = tf.expand_dims(x, axis=-1)\n",
    "            \n",
    "            if augment:\n",
    "                x = two_d_augmentations(x)\n",
    "                \n",
    "            x = tf.convert_to_tensor(x)\n",
    "        return x, y\n",
    "    else:\n",
    "        raise ValueError(\"expected plane to be one of ['axial', 'coronal', 'sagittal']\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ROOTDIR = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_A/128/tfrecords_full'\n",
    "    n_classes = 2\n",
    "    global_batch_size = 8\n",
    "    volume_shape = (64, 64, 64)\n",
    "    ds = get_dataset(\n",
    "        os.path.join(ROOTDIR, \"data-train_*\"),\n",
    "        n_classes=n_classes,\n",
    "        batch_size=global_batch_size,\n",
    "        volume_shape=volume_shape,\n",
    "        plane=\"sagittal\",\n",
    "        augment = False,\n",
    "        shuffle_buffer_size=3,\n",
    "    )\n",
    "\n",
    "#     import matplotlib\n",
    "#     matplotlib.use('Agg')\n",
    "#     import matplotlib.pyplot as plt\n",
    "\n",
    "    print(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std packages\n",
    "import sys, os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "sys.path.append(\"../defacing\")\n",
    "\n",
    "# Custom packages\n",
    "from models import modelN\n",
    "# from dataloaders.dataset import get_dataset\n",
    "\n",
    "# Tf packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 3:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "\n",
    "def train(\n",
    "    csv_path,\n",
    "    model_save_path,\n",
    "    tfrecords_path,\n",
    "    volume_shape=(64, 64, 64),\n",
    "    image_size=(64, 64),\n",
    "    dropout=0.2,\n",
    "    batch_size=16,\n",
    "    n_classes=2,\n",
    "    n_epochs=15,\n",
    "    percent=100,\n",
    "    mode='CV',\n",
    "):\n",
    "    \n",
    "    \n",
    "    train_csv_path = os.path.join(csv_path, \"training.csv\")\n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "    \n",
    "    if mode == 'CV':\n",
    "        valid_csv_path = os.path.join(csv_path, \"validation.csv\")\n",
    "        valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "        valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "    \n",
    "    weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(train_labels),\n",
    "                                                train_labels)\n",
    "    weights = dict(enumerate(weights))\n",
    "    \n",
    "    print(weights)\n",
    "    \n",
    "    planes = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "    \n",
    "\n",
    "    global_batch_size = batch_size\n",
    "    \n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    cp_save_path = os.path.join(model_save_path, \"weights\")\n",
    "    logdir_path = os.path.join(model_save_path, \"tb_logs\")\n",
    "    metrics_path = os.path.join(model_save_path, \"metrics\")\n",
    "    \n",
    "    os.makedirs(metrics_path, exist_ok=True)\n",
    "#     os.makedirs(logdir_path, exist_ok=True)\n",
    "        \n",
    "    for plane in planes:\n",
    "\n",
    "        logdir = os.path.join(logdir_path, plane)\n",
    "        os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "        tbCallback = TensorBoard(\n",
    "            log_dir=logdir, histogram_freq=1, write_graph=True, write_images=True,\n",
    "        )\n",
    "\n",
    "        os.makedirs(os.path.join(cp_save_path, plane), exist_ok=True)\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            os.path.join(cp_save_path, plane, \"best-wts.h5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "\n",
    "#         with strategy.scope():\n",
    "\n",
    "        if not plane == \"combined\": \n",
    "            lr = 1e-3\n",
    "            model = modelN.Submodel(\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                name=plane,\n",
    "                include_top=True,\n",
    "                weights=None,\n",
    "            )\n",
    "        else:\n",
    "            lr = 5e-4\n",
    "            model = modelN.CombinedClassifier(\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                trainable=True,\n",
    "                wts_root=cp_save_path,\n",
    "            )\n",
    "\n",
    "        print(\"Submodel: \", plane)\n",
    "#         print(model.summary())\n",
    "\n",
    "        METRICS = [\n",
    "            metrics.TruePositives(name=\"tp\"),\n",
    "            metrics.FalsePositives(name=\"fp\"),\n",
    "            metrics.TrueNegatives(name=\"tn\"),\n",
    "            metrics.FalseNegatives(name=\"fn\"),\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            metrics.Precision(name=\"precision\"),\n",
    "            metrics.Recall(name=\"recall\"),\n",
    "            metrics.AUC(name=\"auc\"),\n",
    "        ]\n",
    "\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.binary_crossentropy,\n",
    "            optimizer=Adam(learning_rate=lr),\n",
    "            metrics=METRICS,\n",
    "        )\n",
    "\n",
    "        print(\"GLOBAL BATCH SIZE: \", global_batch_size)\n",
    "\n",
    "        dataset_train = get_dataset(\n",
    "            file_pattern=os.path.join(tfrecords_path, 'data-train_*'),\n",
    "            n_classes=n_classes,\n",
    "            batch_size=global_batch_size,\n",
    "            volume_shape=volume_shape,\n",
    "            plane=plane,\n",
    "            shuffle_buffer_size=global_batch_size,\n",
    "        )\n",
    "        \n",
    "        steps_per_epoch = math.ceil(len(train_paths)/batch_size)\n",
    "        print(steps_per_epoch)\n",
    "        lrcallback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        \n",
    "        if mode == 'CV':\n",
    "            dataset_valid = get_dataset(\n",
    "                file_pattern=os.path.join(tfrecords_path, \"data-valid_*\"),\n",
    "                n_classes=n_classes,\n",
    "                batch_size=global_batch_size,\n",
    "                volume_shape=volume_shape,\n",
    "                plane=plane,\n",
    "                shuffle_buffer_size=global_batch_size,\n",
    "            )\n",
    "            \n",
    "            validation_steps = math.ceil(len(valid_paths)/batch_size)\n",
    "            \n",
    "            history = model.fit(\n",
    "                dataset_train,\n",
    "                epochs=n_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                validation_data=dataset_valid,\n",
    "                validation_steps=validation_steps,\n",
    "                callbacks=[tbCallback, model_checkpoint],\n",
    "                class_weight = weights,\n",
    "            )\n",
    "            hist_df =  pd.DataFrame(history.history)\n",
    "        else:\n",
    "            print(model.summary())\n",
    "            print(\"Steps/Epoch: \", steps_per_epoch)\n",
    "            history = model.fit(\n",
    "                dataset_train,\n",
    "                epochs=n_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                callbacks=[tbCallback, model_checkpoint],\n",
    "                class_weight = weights,\n",
    "            )\n",
    "            \n",
    "        hist_df =  pd.DataFrame(history.history)\n",
    "        jsonfile = os.path.join(metrics_path, plane + '.json')\n",
    "        \n",
    "        with open(jsonfile, mode='w') as f:\n",
    "            hist_df.to_json(f)\n",
    "        \n",
    "        del model\n",
    "        K.clear_session()\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "{0: 0.9946236559139785, 1: 1.0054347826086956}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 70s 523ms/step - loss: 0.8238 - tp: 436.1880 - fp: 29.7179 - tn: 443.5128 - fn: 34.4444 - accuracy: 0.8832 - precision: 0.8876 - recall: 0.8703 - auc: 0.9133 - val_loss: 4.4781 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0240 - tp: 462.3162 - fp: 2.2308 - tn: 475.7265 - fn: 3.5897 - accuracy: 0.9914 - precision: 0.9915 - recall: 0.9901 - auc: 0.9989 - val_loss: 7.2161 - val_tp: 68.0000 - val_fp: 76.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4722 - val_precision: 0.4722 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0049 - tp: 475.0769 - fp: 0.8718 - tn: 467.1453 - fn: 0.7692 - accuracy: 0.9981 - precision: 0.9979 - recall: 0.9985 - auc: 1.0000 - val_loss: 10.9469 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0176 - tp: 468.3419 - fp: 2.5812 - tn: 469.4872 - fn: 3.4530 - accuracy: 0.9954 - precision: 0.9962 - recall: 0.9947 - auc: 0.9983 - val_loss: 2.5701 - val_tp: 70.0000 - val_fp: 44.0000 - val_tn: 30.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6944 - val_precision: 0.6140 - val_recall: 1.0000 - val_auc: 0.8243\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0119 - tp: 478.8034 - fp: 1.6752 - tn: 461.2222 - fn: 2.1624 - accuracy: 0.9970 - precision: 0.9979 - recall: 0.9963 - auc: 0.9993 - val_loss: 0.4127 - val_tp: 69.0000 - val_fp: 10.0000 - val_tn: 65.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9306 - val_precision: 0.8734 - val_recall: 1.0000 - val_auc: 0.9667\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 2.2776e-04 - tp: 465.6410 - fp: 0.0000e+00 - tn: 478.1368 - fn: 0.0855 - accuracy: 1.0000 - precision: 1.0000 - recall: 0.9999 - auc: 1.0000 - val_loss: 0.3857 - val_tp: 70.0000 - val_fp: 5.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9653 - val_precision: 0.9333 - val_recall: 1.0000 - val_auc: 0.9865\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0036 - tp: 468.9744 - fp: 1.0598 - tn: 473.7350 - fn: 0.0940 - accuracy: 0.9988 - precision: 0.9976 - recall: 0.9999 - auc: 0.9999 - val_loss: 0.3677 - val_tp: 74.0000 - val_fp: 6.0000 - val_tn: 64.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9583 - val_precision: 0.9250 - val_recall: 1.0000 - val_auc: 0.9786\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0065 - tp: 467.1538 - fp: 1.8632 - tn: 473.9829 - fn: 0.8632 - accuracy: 0.9972 - precision: 0.9963 - recall: 0.9981 - auc: 1.0000 - val_loss: 0.1369 - val_tp: 68.0000 - val_fp: 3.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9577 - val_recall: 1.0000 - val_auc: 0.9934\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0071 - tp: 475.9145 - fp: 0.9744 - tn: 466.6838 - fn: 0.2906 - accuracy: 0.9989 - precision: 0.9982 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.7649 - val_tp: 58.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 16.0000 - val_accuracy: 0.8889 - val_precision: 1.0000 - val_recall: 0.7838 - val_auc: 0.9392\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0469 - tp: 469.1282 - fp: 1.7521 - tn: 470.8376 - fn: 2.1453 - accuracy: 0.9954 - precision: 0.9963 - recall: 0.9946 - auc: 0.9974 - val_loss: 0.4544 - val_tp: 56.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 17.0000 - val_accuracy: 0.8750 - val_precision: 0.9825 - val_recall: 0.7671 - val_auc: 0.9765\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0067 - tp: 462.7179 - fp: 0.7949 - tn: 479.0598 - fn: 1.2906 - accuracy: 0.9967 - precision: 0.9983 - recall: 0.9948 - auc: 0.9998 - val_loss: 0.1212 - val_tp: 74.0000 - val_fp: 1.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9867 - val_recall: 1.0000 - val_auc: 0.9929\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0012 - tp: 475.4274 - fp: 0.2479 - tn: 468.0940 - fn: 0.0940 - accuracy: 0.9998 - precision: 0.9997 - recall: 0.9999 - auc: 1.0000 - val_loss: 0.2543 - val_tp: 63.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 7.0000 - val_accuracy: 0.9444 - val_precision: 0.9844 - val_recall: 0.9000 - val_auc: 0.9710\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 8s 66ms/step - loss: 0.0192 - tp: 462.3761 - fp: 2.2991 - tn: 477.0427 - fn: 2.1453 - accuracy: 0.9952 - precision: 0.9929 - recall: 0.9967 - auc: 0.9992 - val_loss: 0.1281 - val_tp: 65.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 5.0000 - val_accuracy: 0.9583 - val_precision: 0.9848 - val_recall: 0.9286 - val_auc: 0.9904\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0027 - tp: 473.0598 - fp: 0.8632 - tn: 469.5299 - fn: 0.4103 - accuracy: 0.9990 - precision: 0.9986 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0457 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 0.9923\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0040 - tp: 464.9829 - fp: 0.5726 - tn: 477.6154 - fn: 0.6923 - accuracy: 0.9989 - precision: 0.9991 - recall: 0.9987 - auc: 0.9995 - val_loss: 4.5681e-04 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 86ms/step - loss: 1.6886 - tp: 347.8718 - fp: 112.2650 - tn: 364.8632 - fn: 118.8632 - accuracy: 0.6945 - precision: 0.6913 - recall: 0.6752 - auc: 0.7208 - val_loss: 1.5866 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.5037\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 82ms/step - loss: 0.1166 - tp: 455.1197 - fp: 19.5128 - tn: 456.6239 - fn: 12.6068 - accuracy: 0.9636 - precision: 0.9545 - recall: 0.9730 - auc: 0.9905 - val_loss: 1.4167 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.5530\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 76ms/step - loss: 0.0690 - tp: 462.3419 - fp: 10.5470 - tn: 463.0256 - fn: 7.9487 - accuracy: 0.9793 - precision: 0.9774 - recall: 0.9822 - auc: 0.9971 - val_loss: 1.0848 - val_tp: 69.0000 - val_fp: 67.0000 - val_tn: 8.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5347 - val_precision: 0.5074 - val_recall: 1.0000 - val_auc: 0.9647\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0328 - tp: 471.2393 - fp: 6.3761 - tn: 460.4444 - fn: 5.8034 - accuracy: 0.9854 - precision: 0.9854 - recall: 0.9860 - auc: 0.9991 - val_loss: 0.4177 - val_tp: 70.0000 - val_fp: 28.0000 - val_tn: 46.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8056 - val_precision: 0.7143 - val_recall: 1.0000 - val_auc: 0.9966\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 76ms/step - loss: 0.0303 - tp: 462.8803 - fp: 6.2051 - tn: 467.7436 - fn: 7.0342 - accuracy: 0.9877 - precision: 0.9891 - recall: 0.9862 - auc: 0.9993 - val_loss: 0.0801 - val_tp: 72.0000 - val_fp: 6.0000 - val_tn: 66.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9583 - val_precision: 0.9231 - val_recall: 1.0000 - val_auc: 0.9996\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 76ms/step - loss: 0.0207 - tp: 464.8547 - fp: 1.2222 - tn: 475.7863 - fn: 2.0000 - accuracy: 0.9956 - precision: 0.9983 - recall: 0.9933 - auc: 0.9996 - val_loss: 0.0303 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 0.9998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 76ms/step - loss: 0.0716 - tp: 454.8205 - fp: 9.8889 - tn: 472.6838 - fn: 6.4701 - accuracy: 0.9780 - precision: 0.9752 - recall: 0.9798 - auc: 0.9953 - val_loss: 0.0334 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9859 - val_recall: 0.9859 - val_auc: 0.9992\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 76ms/step - loss: 0.0048 - tp: 468.7009 - fp: 0.5897 - tn: 473.2735 - fn: 1.2991 - accuracy: 0.9985 - precision: 0.9992 - recall: 0.9977 - auc: 1.0000 - val_loss: 0.0577 - val_tp: 72.0000 - val_fp: 2.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9730 - val_recall: 1.0000 - val_auc: 0.9985\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0271 - tp: 464.4530 - fp: 5.8974 - tn: 468.1795 - fn: 5.3333 - accuracy: 0.9893 - precision: 0.9874 - recall: 0.9912 - auc: 0.9992 - val_loss: 0.0103 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0181 - tp: 468.7350 - fp: 4.9829 - tn: 468.0000 - fn: 2.1453 - accuracy: 0.9925 - precision: 0.9890 - recall: 0.9959 - auc: 0.9998 - val_loss: 0.1728 - val_tp: 65.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 8.0000 - val_accuracy: 0.9444 - val_precision: 1.0000 - val_recall: 0.8904 - val_auc: 0.9992\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0011 - tp: 470.0940 - fp: 0.0000e+00 - tn: 473.7692 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0145 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9857 - val_recall: 1.0000 - val_auc: 0.9996\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 76ms/step - loss: 3.6723e-04 - tp: 468.7009 - fp: 0.0000e+00 - tn: 475.1624 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0283 - val_tp: 69.0000 - val_fp: 2.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9718 - val_recall: 1.0000 - val_auc: 0.9996\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 1.3651e-04 - tp: 467.9145 - fp: 0.0000e+00 - tn: 475.9487 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0145 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 0.9998\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 8.8698e-05 - tp: 463.3333 - fp: 0.0000e+00 - tn: 480.5299 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0158 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 0.9998\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 2.8919e-04 - tp: 475.5897 - fp: 0.0000e+00 - tn: 468.2735 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0125 - val_tp: 75.0000 - val_fp: 1.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9868 - val_recall: 1.0000 - val_auc: 0.9998\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 13s 89ms/step - loss: 0.8768 - tp: 423.8889 - fp: 32.9744 - tn: 448.0171 - fn: 38.9829 - accuracy: 0.8887 - precision: 0.8897 - recall: 0.8745 - auc: 0.9199 - val_loss: 0.6709 - val_tp: 72.0000 - val_fp: 71.0000 - val_tn: 1.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5035 - val_recall: 1.0000 - val_auc: 0.8828\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0262 - tp: 449.6752 - fp: 8.7265 - tn: 472.4444 - fn: 13.0171 - accuracy: 0.9817 - precision: 0.9871 - recall: 0.9754 - auc: 0.9992 - val_loss: 2.7755 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.7214\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0499 - tp: 459.9744 - fp: 16.5726 - tn: 456.6496 - fn: 10.6667 - accuracy: 0.9717 - precision: 0.9616 - recall: 0.9826 - auc: 0.9980 - val_loss: 0.1468 - val_tp: 72.0000 - val_fp: 8.0000 - val_tn: 64.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9444 - val_precision: 0.9000 - val_recall: 1.0000 - val_auc: 0.9985\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0385 - tp: 451.0940 - fp: 11.7607 - tn: 465.0855 - fn: 15.9231 - accuracy: 0.9756 - precision: 0.9823 - recall: 0.9680 - auc: 0.9982 - val_loss: 0.3844 - val_tp: 75.0000 - val_fp: 10.0000 - val_tn: 59.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9306 - val_precision: 0.8824 - val_recall: 1.0000 - val_auc: 0.9752\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0432 - tp: 453.6667 - fp: 10.4103 - tn: 464.4872 - fn: 15.2991 - accuracy: 0.9734 - precision: 0.9838 - recall: 0.9623 - auc: 0.9983 - val_loss: 0.0218 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9859 - val_recall: 0.9859 - val_auc: 0.9998\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0368 - tp: 458.9145 - fp: 15.8547 - tn: 454.9658 - fn: 14.1282 - accuracy: 0.9692 - precision: 0.9655 - recall: 0.9743 - auc: 0.9983 - val_loss: 0.0051 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0328 - tp: 451.7350 - fp: 0.0000e+00 - tn: 471.3162 - fn: 20.8120 - accuracy: 0.9787 - precision: 1.0000 - recall: 0.9575 - auc: 0.9987 - val_loss: 6.4173e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0383 - tp: 441.3846 - fp: 2.1966 - tn: 474.5983 - fn: 25.6838 - accuracy: 0.9703 - precision: 0.9969 - recall: 0.9418 - auc: 0.9985 - val_loss: 3.6307e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0309 - tp: 447.2564 - fp: 0.5470 - tn: 474.2735 - fn: 21.7863 - accuracy: 0.9783 - precision: 0.9991 - recall: 0.9572 - auc: 0.9988 - val_loss: 0.0746 - val_tp: 68.0000 - val_fp: 3.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9577 - val_recall: 1.0000 - val_auc: 0.9929\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0781 - tp: 455.0000 - fp: 9.7521 - tn: 461.6239 - fn: 17.4872 - accuracy: 0.9696 - precision: 0.9781 - recall: 0.9621 - auc: 0.9937 - val_loss: 3.8643 - val_tp: 40.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 30.0000 - val_accuracy: 0.7917 - val_precision: 1.0000 - val_recall: 0.5714 - val_auc: 0.7316\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0289 - tp: 448.6923 - fp: 1.7949 - tn: 473.4017 - fn: 19.9744 - accuracy: 0.9781 - precision: 0.9949 - recall: 0.9606 - auc: 0.9987 - val_loss: 0.2284 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 5.0000 - val_accuracy: 0.9653 - val_precision: 1.0000 - val_recall: 0.9333 - val_auc: 0.9926\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0339 - tp: 437.8205 - fp: 0.6325 - tn: 483.9658 - fn: 21.4444 - accuracy: 0.9760 - precision: 0.9989 - recall: 0.9511 - auc: 0.9987 - val_loss: 0.1087 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9710 - val_auc: 0.9926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0303 - tp: 445.5128 - fp: 4.4701 - tn: 473.7265 - fn: 20.1538 - accuracy: 0.9729 - precision: 0.9881 - recall: 0.9572 - auc: 0.9990 - val_loss: 0.0974 - val_tp: 69.0000 - val_fp: 3.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9583 - val_recall: 1.0000 - val_auc: 0.9997\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0323 - tp: 456.5983 - fp: 6.1197 - tn: 463.1966 - fn: 17.9487 - accuracy: 0.9739 - precision: 0.9871 - recall: 0.9609 - auc: 0.9988 - val_loss: 0.0012 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0379 - tp: 450.8889 - fp: 6.6154 - tn: 463.5556 - fn: 22.8034 - accuracy: 0.9675 - precision: 0.9898 - recall: 0.9446 - auc: 0.9986 - val_loss: 4.7350e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 109ms/step - loss: 2.9985 - tp: 343.9744 - fp: 112.9231 - tn: 366.4872 - fn: 120.4786 - accuracy: 0.6793 - precision: 0.6746 - recall: 0.6670 - auc: 0.7163 - val_loss: 1.1369 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.8188\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.0232 - tp: 466.3846 - fp: 2.8718 - tn: 471.4017 - fn: 3.2051 - accuracy: 0.9940 - precision: 0.9948 - recall: 0.9931 - auc: 0.9996 - val_loss: 1.7321 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.9522\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0112 - tp: 461.0085 - fp: 2.4701 - tn: 478.5556 - fn: 1.8291 - accuracy: 0.9938 - precision: 0.9943 - recall: 0.9931 - auc: 1.0000 - val_loss: 1.7319 - val_tp: 69.0000 - val_fp: 74.0000 - val_tn: 1.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4825 - val_recall: 1.0000 - val_auc: 0.9945\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0013 - tp: 461.9060 - fp: 0.0684 - tn: 481.7949 - fn: 0.0940 - accuracy: 0.9999 - precision: 0.9999 - recall: 0.9999 - auc: 1.0000 - val_loss: 0.3140 - val_tp: 75.0000 - val_fp: 19.0000 - val_tn: 50.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8681 - val_precision: 0.7979 - val_recall: 1.0000 - val_auc: 0.9975\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0095 - tp: 459.2393 - fp: 1.1026 - tn: 482.4017 - fn: 1.1197 - accuracy: 0.9980 - precision: 0.9979 - recall: 0.9980 - auc: 0.9991 - val_loss: 0.0663 - val_tp: 75.0000 - val_fp: 2.0000 - val_tn: 67.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9740 - val_recall: 1.0000 - val_auc: 0.9994\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0047 - tp: 465.0256 - fp: 1.0342 - tn: 476.5897 - fn: 1.2137 - accuracy: 0.9975 - precision: 0.9980 - recall: 0.9968 - auc: 1.0000 - val_loss: 0.0127 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0013 - tp: 460.4103 - fp: 0.7949 - tn: 482.6581 - fn: 0.0000e+00 - accuracy: 0.9992 - precision: 0.9983 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0284 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9571 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 8.0730e-05 - tp: 475.3077 - fp: 0.0000e+00 - tn: 468.5556 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0272 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9577 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 1.5590e-04 - tp: 476.2308 - fp: 0.0000e+00 - tn: 467.6325 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0406 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9589 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 3.3367e-05 - tp: 466.1966 - fp: 0.0000e+00 - tn: 477.6667 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0220 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9583 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 5.1929e-05 - tp: 462.5812 - fp: 0.0000e+00 - tn: 481.2821 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0220 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9577 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 2.0882e-05 - tp: 473.3675 - fp: 0.0000e+00 - tn: 470.4957 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0251 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 67.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9610 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 4.5945e-05 - tp: 476.3162 - fp: 0.0000e+00 - tn: 467.5470 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0155 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9865 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 2.0287e-05 - tp: 460.3333 - fp: 0.0000e+00 - tn: 483.5299 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0112 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 4.8073e-05 - tp: 458.4359 - fp: 0.0000e+00 - tn: 485.4274 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0148 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9859 - val_auc: 1.0000\n",
      "FOLD 2\n",
      "{0: 0.9946236559139785, 1: 1.0054347826086956}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 80s 595ms/step - loss: 1.5058 - tp: 403.7436 - fp: 51.9402 - tn: 424.8718 - fn: 63.3077 - accuracy: 0.8114 - precision: 0.8168 - recall: 0.7893 - auc: 0.8426 - val_loss: 5.7747 - val_tp: 67.0000 - val_fp: 77.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4653 - val_precision: 0.4653 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0270 - tp: 474.0855 - fp: 5.1795 - tn: 462.0769 - fn: 2.5214 - accuracy: 0.9903 - precision: 0.9860 - recall: 0.9950 - auc: 0.9994 - val_loss: 8.6000 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0015 - tp: 478.6239 - fp: 0.1624 - tn: 464.8462 - fn: 0.2308 - accuracy: 0.9998 - precision: 0.9998 - recall: 0.9997 - auc: 1.0000 - val_loss: 12.1144 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 8s 66ms/step - loss: 0.0138 - tp: 463.1197 - fp: 2.0769 - tn: 475.9231 - fn: 2.7436 - accuracy: 0.9966 - precision: 0.9971 - recall: 0.9959 - auc: 0.9995 - val_loss: 0.1993 - val_tp: 69.0000 - val_fp: 3.0000 - val_tn: 70.0000 - val_fn: 2.0000 - val_accuracy: 0.9653 - val_precision: 0.9583 - val_recall: 0.9718 - val_auc: 0.9835\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0086 - tp: 453.8376 - fp: 1.0684 - tn: 487.3846 - fn: 1.5726 - accuracy: 0.9975 - precision: 0.9981 - recall: 0.9967 - auc: 0.9996 - val_loss: 0.0373 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0111 - tp: 465.3248 - fp: 2.3846 - tn: 474.9145 - fn: 1.2393 - accuracy: 0.9969 - precision: 0.9956 - recall: 0.9982 - auc: 0.9997 - val_loss: 0.4215 - val_tp: 73.0000 - val_fp: 9.0000 - val_tn: 62.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9375 - val_precision: 0.8902 - val_recall: 1.0000 - val_auc: 0.9718\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0120 - tp: 470.6239 - fp: 1.4957 - tn: 471.3162 - fn: 0.4274 - accuracy: 0.9983 - precision: 0.9972 - recall: 0.9994 - auc: 0.9988 - val_loss: 1.2512 - val_tp: 25.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 44.0000 - val_accuracy: 0.6944 - val_precision: 1.0000 - val_recall: 0.3623 - val_auc: 0.9203\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0031 - tp: 464.8291 - fp: 0.5214 - tn: 477.7692 - fn: 0.7436 - accuracy: 0.9989 - precision: 0.9992 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0294 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9718 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 6.7108e-05 - tp: 469.5470 - fp: 0.0000e+00 - tn: 474.3162 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0011 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 1.6318e-05 - tp: 466.4872 - fp: 0.0000e+00 - tn: 477.3761 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.3508e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 9.2914e-05 - tp: 466.9145 - fp: 0.0000e+00 - tn: 476.9487 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 9.6155e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 9.4708e-06 - tp: 469.8974 - fp: 0.0000e+00 - tn: 473.9658 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.5744e-04 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 7.1244e-04 - tp: 480.1111 - fp: 0.5385 - tn: 463.2137 - fn: 0.0000e+00 - accuracy: 0.9996 - precision: 0.9992 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0020 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0145 - tp: 461.7265 - fp: 0.6581 - tn: 478.6667 - fn: 2.8120 - accuracy: 0.9966 - precision: 0.9988 - recall: 0.9943 - auc: 0.9982 - val_loss: 0.0298 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0379 - tp: 464.2991 - fp: 1.6838 - tn: 475.0598 - fn: 2.8205 - accuracy: 0.9932 - precision: 0.9922 - recall: 0.9931 - auc: 0.9987 - val_loss: 0.8546 - val_tp: 52.0000 - val_fp: 2.0000 - val_tn: 69.0000 - val_fn: 21.0000 - val_accuracy: 0.8403 - val_precision: 0.9630 - val_recall: 0.7123 - val_auc: 0.9046\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 87ms/step - loss: 2.3367 - tp: 344.6667 - fp: 123.3162 - tn: 352.9829 - fn: 122.8974 - accuracy: 0.6739 - precision: 0.6788 - recall: 0.6843 - auc: 0.6982 - val_loss: 1.4838 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.5137\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0930 - tp: 462.1111 - fp: 18.1624 - tn: 447.8718 - fn: 15.7179 - accuracy: 0.9642 - precision: 0.9611 - recall: 0.9686 - auc: 0.9946 - val_loss: 1.2497 - val_tp: 69.0000 - val_fp: 75.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4792 - val_recall: 1.0000 - val_auc: 0.7099\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0400 - tp: 461.7094 - fp: 5.8462 - tn: 469.1538 - fn: 7.1538 - accuracy: 0.9846 - precision: 0.9891 - recall: 0.9801 - auc: 0.9993 - val_loss: 2.7934 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.6712\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0512 - tp: 471.9231 - fp: 6.0684 - tn: 458.4872 - fn: 7.3846 - accuracy: 0.9848 - precision: 0.9865 - recall: 0.9841 - auc: 0.9970 - val_loss: 0.5800 - val_tp: 72.0000 - val_fp: 35.0000 - val_tn: 37.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7569 - val_precision: 0.6729 - val_recall: 1.0000 - val_auc: 0.9771\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0171 - tp: 469.0855 - fp: 3.5043 - tn: 467.6068 - fn: 3.6667 - accuracy: 0.9944 - precision: 0.9941 - recall: 0.9947 - auc: 0.9998 - val_loss: 0.2714 - val_tp: 72.0000 - val_fp: 16.0000 - val_tn: 56.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8889 - val_precision: 0.8182 - val_recall: 1.0000 - val_auc: 0.9980\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0056 - tp: 458.3932 - fp: 0.8034 - tn: 484.3675 - fn: 0.2991 - accuracy: 0.9993 - precision: 0.9989 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0239 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0102 - tp: 458.3333 - fp: 1.8547 - tn: 482.1709 - fn: 1.5043 - accuracy: 0.9977 - precision: 0.9974 - recall: 0.9978 - auc: 0.9996 - val_loss: 0.0390 - val_tp: 74.0000 - val_fp: 1.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9867 - val_recall: 1.0000 - val_auc: 0.9998\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0134 - tp: 471.7179 - fp: 1.5470 - tn: 468.2735 - fn: 2.3248 - accuracy: 0.9958 - precision: 0.9967 - recall: 0.9949 - auc: 0.9999 - val_loss: 0.4438 - val_tp: 72.0000 - val_fp: 21.0000 - val_tn: 51.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8542 - val_precision: 0.7742 - val_recall: 1.0000 - val_auc: 0.9792\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 76ms/step - loss: 0.0130 - tp: 461.2479 - fp: 3.4274 - tn: 476.4188 - fn: 2.7692 - accuracy: 0.9938 - precision: 0.9928 - recall: 0.9946 - auc: 0.9999 - val_loss: 0.8387 - val_tp: 71.0000 - val_fp: 33.0000 - val_tn: 40.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7708 - val_precision: 0.6827 - val_recall: 1.0000 - val_auc: 0.9589\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0120 - tp: 463.2564 - fp: 2.1624 - tn: 475.1538 - fn: 3.2906 - accuracy: 0.9948 - precision: 0.9968 - recall: 0.9927 - auc: 0.9999 - val_loss: 0.0790 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 4.0000 - val_accuracy: 0.9722 - val_precision: 1.0000 - val_recall: 0.9459 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0046 - tp: 472.7350 - fp: 0.7436 - tn: 469.5556 - fn: 0.8291 - accuracy: 0.9983 - precision: 0.9986 - recall: 0.9981 - auc: 1.0000 - val_loss: 0.0855 - val_tp: 71.0000 - val_fp: 5.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9653 - val_precision: 0.9342 - val_recall: 1.0000 - val_auc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 3.0188e-04 - tp: 472.8376 - fp: 0.0000e+00 - tn: 471.0256 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0269 - val_tp: 74.0000 - val_fp: 3.0000 - val_tn: 67.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9610 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 2.7002e-04 - tp: 463.0598 - fp: 0.0000e+00 - tn: 480.8034 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0046 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 9.0769e-05 - tp: 468.8974 - fp: 0.0000e+00 - tn: 474.9658 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0068 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 2.3469e-04 - tp: 469.3333 - fp: 0.0000e+00 - tn: 474.5299 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0034 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 87ms/step - loss: 0.4936 - tp: 440.6154 - fp: 17.6752 - tn: 462.3248 - fn: 23.2479 - accuracy: 0.9244 - precision: 0.9347 - recall: 0.9097 - auc: 0.9464 - val_loss: 0.4226 - val_tp: 33.0000 - val_fp: 0.0000e+00 - val_tn: 68.0000 - val_fn: 43.0000 - val_accuracy: 0.7014 - val_precision: 1.0000 - val_recall: 0.4342 - val_auc: 0.9971\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0383 - tp: 458.2650 - fp: 5.3846 - tn: 474.2650 - fn: 5.9487 - accuracy: 0.9875 - precision: 0.9880 - recall: 0.9861 - auc: 0.9986 - val_loss: 1.8938 - val_tp: 8.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 64.0000 - val_accuracy: 0.5556 - val_precision: 1.0000 - val_recall: 0.1111 - val_auc: 0.8611\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0477 - tp: 458.3675 - fp: 2.9145 - tn: 480.7350 - fn: 1.8462 - accuracy: 0.9953 - precision: 0.9943 - recall: 0.9962 - auc: 0.9971 - val_loss: 1.9763 - val_tp: 14.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 57.0000 - val_accuracy: 0.6042 - val_precision: 1.0000 - val_recall: 0.1972 - val_auc: 0.8521\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0195 - tp: 466.8120 - fp: 1.4701 - tn: 473.5983 - fn: 1.9829 - accuracy: 0.9969 - precision: 0.9977 - recall: 0.9961 - auc: 0.9991 - val_loss: 0.0903 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9722 - val_auc: 0.9931\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0600 - tp: 468.5983 - fp: 2.0513 - tn: 470.2650 - fn: 2.9487 - accuracy: 0.9936 - precision: 0.9954 - recall: 0.9916 - auc: 0.9976 - val_loss: 2.0815e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0271 - tp: 469.6923 - fp: 1.6752 - tn: 470.9573 - fn: 1.5385 - accuracy: 0.9955 - precision: 0.9945 - recall: 0.9963 - auc: 0.9980 - val_loss: 0.0019 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0051 - tp: 466.5812 - fp: 0.8547 - tn: 476.4274 - fn: 0.0000e+00 - accuracy: 0.9992 - precision: 0.9983 - recall: 1.0000 - auc: 0.9992 - val_loss: 1.3845e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0049 - tp: 475.5043 - fp: 0.3590 - tn: 466.7521 - fn: 1.2479 - accuracy: 0.9977 - precision: 0.9995 - recall: 0.9960 - auc: 0.9998 - val_loss: 0.0143 - val_tp: 71.0000 - val_fp: 2.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9726 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 8.9645e-04 - tp: 464.6239 - fp: 0.0000e+00 - tn: 479.2393 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0011 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0029 - tp: 478.4274 - fp: 0.6496 - tn: 464.2137 - fn: 0.5726 - accuracy: 0.9990 - precision: 0.9989 - recall: 0.9991 - auc: 1.0000 - val_loss: 5.0983e-05 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0208 - tp: 467.7692 - fp: 1.6667 - tn: 472.1111 - fn: 2.3162 - accuracy: 0.9956 - precision: 0.9962 - recall: 0.9950 - auc: 0.9990 - val_loss: 5.2151e-04 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0111 - tp: 464.5726 - fp: 1.2906 - tn: 476.3675 - fn: 1.6325 - accuracy: 0.9971 - precision: 0.9974 - recall: 0.9966 - auc: 0.9991 - val_loss: 2.1767e-04 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0064 - tp: 458.2051 - fp: 0.8291 - tn: 484.0513 - fn: 0.7778 - accuracy: 0.9983 - precision: 0.9981 - recall: 0.9983 - auc: 0.9991 - val_loss: 1.9110e-04 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 5.0155e-04 - tp: 467.1282 - fp: 0.0000e+00 - tn: 476.7350 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.1265e-05 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 2.5404e-04 - tp: 472.0085 - fp: 0.0000e+00 - tn: 471.8547 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.6981e-05 - val_tp: 66.0000 - val_fp: 0.0000e+00 - val_tn: 78.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 112ms/step - loss: 2.8571 - tp: 386.3333 - fp: 71.7179 - tn: 403.5470 - fn: 82.2650 - accuracy: 0.7533 - precision: 0.7549 - recall: 0.7349 - auc: 0.7820 - val_loss: 1.7554 - val_tp: 69.0000 - val_fp: 75.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4792 - val_recall: 1.0000 - val_auc: 0.7206\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0186 - tp: 472.9060 - fp: 3.9573 - tn: 465.8205 - fn: 1.1795 - accuracy: 0.9950 - precision: 0.9921 - recall: 0.9981 - auc: 0.9996 - val_loss: 1.7104 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.9726\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0042 - tp: 459.4444 - fp: 1.3162 - tn: 482.8547 - fn: 0.2479 - accuracy: 0.9986 - precision: 0.9975 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.9233 - val_tp: 70.0000 - val_fp: 60.0000 - val_tn: 14.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5833 - val_precision: 0.5385 - val_recall: 1.0000 - val_auc: 0.9968\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.0118 - tp: 473.9744 - fp: 2.2735 - tn: 465.6496 - fn: 1.9658 - accuracy: 0.9964 - precision: 0.9958 - recall: 0.9970 - auc: 0.9999 - val_loss: 1.6184 - val_tp: 76.0000 - val_fp: 53.0000 - val_tn: 15.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6319 - val_precision: 0.5891 - val_recall: 1.0000 - val_auc: 0.8897\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0055 - tp: 474.8718 - fp: 1.9744 - tn: 466.1453 - fn: 0.8718 - accuracy: 0.9967 - precision: 0.9957 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0548 - val_tp: 70.0000 - val_fp: 4.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9722 - val_precision: 0.9459 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 5.0800e-04 - tp: 464.1282 - fp: 0.0000e+00 - tn: 479.7350 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0039 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 77.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 1.3146e-04 - tp: 460.9573 - fp: 0.0000e+00 - tn: 482.9060 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0014 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 2.7271e-04 - tp: 478.2479 - fp: 0.0000e+00 - tn: 465.6154 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.2175e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 5.4240e-04 - tp: 464.2906 - fp: 0.5556 - tn: 479.0171 - fn: 0.0000e+00 - accuracy: 0.9996 - precision: 0.9991 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.8186e-04 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 8.9315e-05 - tp: 460.1709 - fp: 0.0000e+00 - tn: 483.6923 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0012 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 6.6592e-04 - tp: 467.1538 - fp: 0.3248 - tn: 476.1368 - fn: 0.2479 - accuracy: 0.9996 - precision: 0.9996 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0022 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 4.1243e-04 - tp: 473.7607 - fp: 0.1453 - tn: 469.9573 - fn: 0.0000e+00 - accuracy: 0.9999 - precision: 0.9998 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0012 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 4.0965e-04 - tp: 466.6239 - fp: 0.0000e+00 - tn: 477.2393 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0131 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 3.8590e-04 - tp: 470.2821 - fp: 0.0000e+00 - tn: 473.5812 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.5394e-04 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 2.5904e-04 - tp: 475.5043 - fp: 0.0000e+00 - tn: 468.3590 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0026 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "FOLD 3\n",
      "{0: 0.9946236559139785, 1: 1.0054347826086956}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 82s 617ms/step - loss: 1.1908 - tp: 403.5726 - fp: 66.2821 - tn: 405.9915 - fn: 68.0171 - accuracy: 0.7860 - precision: 0.7964 - recall: 0.7894 - auc: 0.8248 - val_loss: 1.5231 - val_tp: 69.0000 - val_fp: 75.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4792 - val_recall: 1.0000 - val_auc: 0.6824\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 14s 118ms/step - loss: 0.0122 - tp: 468.7863 - fp: 1.9915 - tn: 470.2308 - fn: 2.8547 - accuracy: 0.9950 - precision: 0.9963 - recall: 0.9939 - auc: 0.9999 - val_loss: 2.3186 - val_tp: 68.0000 - val_fp: 76.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4722 - val_precision: 0.4722 - val_recall: 1.0000 - val_auc: 0.8041\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0019 - tp: 460.8889 - fp: 0.0000e+00 - tn: 482.9744 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.2553 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.5278\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0050 - tp: 470.8120 - fp: 1.2991 - tn: 469.9573 - fn: 1.7949 - accuracy: 0.9978 - precision: 0.9982 - recall: 0.9973 - auc: 1.0000 - val_loss: 0.5170 - val_tp: 70.0000 - val_fp: 27.0000 - val_tn: 46.0000 - val_fn: 1.0000 - val_accuracy: 0.8056 - val_precision: 0.7216 - val_recall: 0.9859 - val_auc: 0.9631\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0029 - tp: 463.2051 - fp: 0.0000e+00 - tn: 480.0171 - fn: 0.6410 - accuracy: 0.9995 - precision: 1.0000 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.2138 - val_tp: 72.0000 - val_fp: 12.0000 - val_tn: 60.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9167 - val_precision: 0.8571 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 3.4634e-04 - tp: 462.6239 - fp: 0.0000e+00 - tn: 481.2393 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0666 - val_tp: 70.0000 - val_fp: 2.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9722 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 7.7841e-05 - tp: 475.1709 - fp: 0.0000e+00 - tn: 468.6923 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0356 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 5.3901e-05 - tp: 463.7436 - fp: 0.0000e+00 - tn: 480.1197 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0373 - val_tp: 72.0000 - val_fp: 2.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9730 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 2.8627e-05 - tp: 478.1966 - fp: 0.0000e+00 - tn: 465.6667 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0122 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 6.6061e-05 - tp: 469.6581 - fp: 0.0000e+00 - tn: 474.2051 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0126 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0045 - tp: 472.5043 - fp: 0.8718 - tn: 470.0427 - fn: 0.4444 - accuracy: 0.9992 - precision: 0.9989 - recall: 0.9995 - auc: 0.9998 - val_loss: 4.2393 - val_tp: 8.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 65.0000 - val_accuracy: 0.5486 - val_precision: 1.0000 - val_recall: 0.1096 - val_auc: 0.6233\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0436 - tp: 468.3162 - fp: 6.7521 - tn: 461.4188 - fn: 7.3761 - accuracy: 0.9851 - precision: 0.9868 - recall: 0.9838 - auc: 0.9976 - val_loss: 2.6735 - val_tp: 12.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 60.0000 - val_accuracy: 0.5833 - val_precision: 1.0000 - val_recall: 0.1667 - val_auc: 0.7500\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0144 - tp: 468.7009 - fp: 2.5214 - tn: 471.8718 - fn: 0.7692 - accuracy: 0.9957 - precision: 0.9925 - recall: 0.9988 - auc: 0.9999 - val_loss: 6.7019e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0137 - tp: 470.1966 - fp: 1.2735 - tn: 471.4103 - fn: 0.9829 - accuracy: 0.9979 - precision: 0.9975 - recall: 0.9982 - auc: 0.9997 - val_loss: 0.0436 - val_tp: 67.0000 - val_fp: 1.0000 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9853 - val_recall: 1.0000 - val_auc: 0.9935\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0032 - tp: 467.4017 - fp: 0.5641 - tn: 475.5299 - fn: 0.3675 - accuracy: 0.9994 - precision: 0.9993 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0062 - val_tp: 74.0000 - val_fp: 1.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9867 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 86ms/step - loss: 2.1964 - tp: 339.2991 - fp: 124.2393 - tn: 361.1368 - fn: 119.1880 - accuracy: 0.6819 - precision: 0.6742 - recall: 0.6785 - auc: 0.7217 - val_loss: 1.5390 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.4919\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0820 - tp: 449.2650 - fp: 13.3162 - tn: 466.3590 - fn: 14.9231 - accuracy: 0.9720 - precision: 0.9738 - recall: 0.9693 - auc: 0.9960 - val_loss: 1.6850 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.6794\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0281 - tp: 456.2991 - fp: 2.0513 - tn: 481.2991 - fn: 4.2137 - accuracy: 0.9939 - precision: 0.9963 - recall: 0.9910 - auc: 0.9994 - val_loss: 1.3360 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.9249\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0362 - tp: 459.4872 - fp: 5.0171 - tn: 472.6154 - fn: 6.7436 - accuracy: 0.9893 - precision: 0.9913 - recall: 0.9870 - auc: 0.9991 - val_loss: 0.2488 - val_tp: 70.0000 - val_fp: 18.0000 - val_tn: 55.0000 - val_fn: 1.0000 - val_accuracy: 0.8681 - val_precision: 0.7955 - val_recall: 0.9859 - val_auc: 0.9912\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0098 - tp: 476.7692 - fp: 1.6667 - tn: 464.7949 - fn: 0.6325 - accuracy: 0.9970 - precision: 0.9950 - recall: 0.9990 - auc: 0.9999 - val_loss: 0.1301 - val_tp: 73.0000 - val_fp: 9.0000 - val_tn: 62.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9375 - val_precision: 0.8902 - val_recall: 1.0000 - val_auc: 0.9999\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0088 - tp: 465.4103 - fp: 1.2051 - tn: 475.5812 - fn: 1.6667 - accuracy: 0.9975 - precision: 0.9979 - recall: 0.9970 - auc: 0.9999 - val_loss: 0.0037 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 3.2165e-04 - tp: 468.3419 - fp: 0.0000e+00 - tn: 475.5214 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.8864e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 2.2501e-04 - tp: 472.6239 - fp: 0.0000e+00 - tn: 471.2393 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0020 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 4.0784e-04 - tp: 467.1453 - fp: 0.0000e+00 - tn: 476.7179 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0112 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9855 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 1.1883e-04 - tp: 476.2906 - fp: 0.0000e+00 - tn: 467.5726 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0192 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9857 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0014 - tp: 462.8034 - fp: 0.0000e+00 - tn: 481.0598 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0532 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9722 - val_auc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0415 - tp: 463.2137 - fp: 11.2735 - tn: 460.4017 - fn: 8.9744 - accuracy: 0.9846 - precision: 0.9825 - recall: 0.9868 - auc: 0.9981 - val_loss: 0.1036 - val_tp: 65.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 4.0000 - val_accuracy: 0.9722 - val_precision: 1.0000 - val_recall: 0.9420 - val_auc: 0.9990\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0406 - tp: 460.4017 - fp: 6.6581 - tn: 468.5983 - fn: 8.2051 - accuracy: 0.9826 - precision: 0.9828 - recall: 0.9821 - auc: 0.9992 - val_loss: 2.0448 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 73.0000 - val_accuracy: 0.4931 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.8973\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0225 - tp: 465.1026 - fp: 3.2051 - tn: 473.6325 - fn: 1.9231 - accuracy: 0.9935 - precision: 0.9907 - recall: 0.9961 - auc: 0.9997 - val_loss: 0.2050 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 67.0000 - val_fn: 7.0000 - val_accuracy: 0.9514 - val_precision: 1.0000 - val_recall: 0.9091 - val_auc: 0.9870\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0024 - tp: 479.8547 - fp: 0.2906 - tn: 463.7179 - fn: 0.0000e+00 - accuracy: 0.9998 - precision: 0.9996 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2336 - val_tp: 66.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 6.0000 - val_accuracy: 0.9583 - val_precision: 1.0000 - val_recall: 0.9167 - val_auc: 0.9792\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 88ms/step - loss: 1.0672 - tp: 426.0171 - fp: 44.1111 - tn: 429.7863 - fn: 43.9487 - accuracy: 0.8555 - precision: 0.8508 - recall: 0.8525 - auc: 0.9082 - val_loss: 0.7139 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0087 - tp: 465.4786 - fp: 1.4444 - tn: 475.4103 - fn: 1.5299 - accuracy: 0.9976 - precision: 0.9981 - recall: 0.9971 - auc: 0.9998 - val_loss: 2.7492 - val_tp: 75.0000 - val_fp: 69.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5208 - val_precision: 0.5208 - val_recall: 1.0000 - val_auc: 0.5652\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0025 - tp: 467.6923 - fp: 0.2821 - tn: 475.7436 - fn: 0.1453 - accuracy: 0.9997 - precision: 0.9996 - recall: 0.9998 - auc: 1.0000 - val_loss: 7.7076 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0046 - tp: 464.0940 - fp: 0.6752 - tn: 477.7009 - fn: 1.3932 - accuracy: 0.9979 - precision: 0.9988 - recall: 0.9969 - auc: 1.0000 - val_loss: 2.9501 - val_tp: 68.0000 - val_fp: 74.0000 - val_tn: 2.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4789 - val_recall: 1.0000 - val_auc: 0.7105\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 5.8633e-04 - tp: 477.2137 - fp: 0.0000e+00 - tn: 466.6496 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0093 - val_tp: 70.0000 - val_fp: 42.0000 - val_tn: 32.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7083 - val_precision: 0.6250 - val_recall: 1.0000 - val_auc: 0.9392\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 1.0378e-04 - tp: 469.8034 - fp: 0.0000e+00 - tn: 474.0598 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0083 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 2.0943e-04 - tp: 477.9402 - fp: 0.0000e+00 - tn: 465.9231 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.4200e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 82ms/step - loss: 8.7500e-05 - tp: 468.6068 - fp: 0.0000e+00 - tn: 475.2564 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0014 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 4.6166e-05 - tp: 470.7692 - fp: 0.0000e+00 - tn: 473.0940 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.4052e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 2.2674e-05 - tp: 465.8803 - fp: 0.0000e+00 - tn: 477.9829 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.9536e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 7.3321e-05 - tp: 474.0342 - fp: 0.0000e+00 - tn: 469.8291 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.5491e-05 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 6.4249e-05 - tp: 469.9231 - fp: 0.0000e+00 - tn: 473.9402 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.5792e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 2.4145e-05 - tp: 476.1538 - fp: 0.0000e+00 - tn: 467.7094 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.6239e-05 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 2.8106e-05 - tp: 480.8632 - fp: 0.0000e+00 - tn: 463.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.9989e-05 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 2.1716e-05 - tp: 462.9915 - fp: 0.0000e+00 - tn: 480.8718 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.3811e-07 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 111ms/step - loss: 2.6751 - tp: 398.8803 - fp: 71.3504 - tn: 400.0513 - fn: 73.5812 - accuracy: 0.7817 - precision: 0.7733 - recall: 0.7730 - auc: 0.8241 - val_loss: 1.8150 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.6698\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0300 - tp: 459.7863 - fp: 3.2051 - tn: 478.7350 - fn: 2.1368 - accuracy: 0.9930 - precision: 0.9932 - recall: 0.9920 - auc: 0.9995 - val_loss: 2.2008 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.8418\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0166 - tp: 472.6410 - fp: 3.2991 - tn: 465.2906 - fn: 2.6325 - accuracy: 0.9927 - precision: 0.9920 - recall: 0.9938 - auc: 0.9998 - val_loss: 3.1162 - val_tp: 76.0000 - val_fp: 68.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5278 - val_precision: 0.5278 - val_recall: 1.0000 - val_auc: 0.5074\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0118 - tp: 463.3846 - fp: 2.9915 - tn: 475.8205 - fn: 1.6667 - accuracy: 0.9956 - precision: 0.9938 - recall: 0.9974 - auc: 0.9999 - val_loss: 1.2118 - val_tp: 71.0000 - val_fp: 49.0000 - val_tn: 24.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6597 - val_precision: 0.5917 - val_recall: 1.0000 - val_auc: 0.9752\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0025 - tp: 478.9658 - fp: 0.7265 - tn: 464.1709 - fn: 0.0000e+00 - accuracy: 0.9993 - precision: 0.9987 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1249 - val_tp: 72.0000 - val_fp: 5.0000 - val_tn: 67.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9653 - val_precision: 0.9351 - val_recall: 1.0000 - val_auc: 0.9924\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 8.7245e-04 - tp: 465.0085 - fp: 0.0000e+00 - tn: 478.8547 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0858 - val_tp: 74.0000 - val_fp: 2.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9737 - val_recall: 1.0000 - val_auc: 0.9926\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.0018 - tp: 471.5470 - fp: 0.7607 - tn: 470.8547 - fn: 0.7009 - accuracy: 0.9986 - precision: 0.9985 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0596 - val_tp: 76.0000 - val_fp: 1.0000 - val_tn: 67.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9870 - val_recall: 1.0000 - val_auc: 0.9923\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 9.4218e-04 - tp: 466.0513 - fp: 0.0000e+00 - tn: 477.8120 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0651 - val_tp: 67.0000 - val_fp: 1.0000 - val_tn: 75.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9853 - val_recall: 0.9853 - val_auc: 0.9931\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 7.5293e-04 - tp: 478.1282 - fp: 0.0000e+00 - tn: 465.7350 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0688 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9861 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 12s 99ms/step - loss: 1.1397e-04 - tp: 463.1880 - fp: 0.0000e+00 - tn: 480.6752 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0546 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9861 - val_recall: 1.0000 - val_auc: 0.9929\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 7.2775e-04 - tp: 475.5128 - fp: 0.0000e+00 - tn: 468.3504 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0742 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 3.3489e-05 - tp: 458.0684 - fp: 0.0000e+00 - tn: 485.7949 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0778 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9861 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 6.1303e-05 - tp: 462.4786 - fp: 0.0000e+00 - tn: 481.3846 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1531 - val_tp: 72.0000 - val_fp: 2.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9730 - val_recall: 1.0000 - val_auc: 0.9859\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 2.9507e-05 - tp: 472.9829 - fp: 0.0000e+00 - tn: 470.8803 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0783 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 0.9929\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 2.0705e-05 - tp: 471.8718 - fp: 0.0000e+00 - tn: 471.9915 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0768 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9861 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "FOLD 4\n",
      "{0: 0.994092373791622, 1: 1.0059782608695653}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 89s 667ms/step - loss: 1.7703 - tp: 402.9060 - fp: 81.0085 - tn: 388.7179 - fn: 71.2308 - accuracy: 0.7658 - precision: 0.7628 - recall: 0.7826 - auc: 0.8080 - val_loss: 0.9725 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.7488\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 10s 85ms/step - loss: 0.0072 - tp: 470.1966 - fp: 0.9744 - tn: 471.2991 - fn: 1.3932 - accuracy: 0.9984 - precision: 0.9988 - recall: 0.9979 - auc: 0.9999 - val_loss: 9.5255 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0119 - tp: 464.6239 - fp: 2.2222 - tn: 475.4188 - fn: 1.5983 - accuracy: 0.9963 - precision: 0.9950 - recall: 0.9974 - auc: 0.9999 - val_loss: 11.9530 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0045 - tp: 471.2393 - fp: 1.4103 - tn: 469.8889 - fn: 1.3248 - accuracy: 0.9981 - precision: 0.9980 - recall: 0.9982 - auc: 0.9998 - val_loss: 12.4898 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0011 - tp: 463.8547 - fp: 0.0855 - tn: 479.9231 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 0.9999 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.7311 - val_tp: 73.0000 - val_fp: 26.0000 - val_tn: 45.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8194 - val_precision: 0.7374 - val_recall: 1.0000 - val_auc: 0.9507\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0023 - tp: 467.4786 - fp: 0.0000e+00 - tn: 476.3846 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0054 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 5.7995e-05 - tp: 464.8889 - fp: 0.0000e+00 - tn: 478.9744 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.7856e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 7.3568e-05 - tp: 470.9487 - fp: 0.0000e+00 - tn: 472.9145 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.1832e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 6.1872e-05 - tp: 469.5214 - fp: 0.0000e+00 - tn: 474.3419 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.4361e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 3.1185e-05 - tp: 468.9573 - fp: 0.0000e+00 - tn: 474.9060 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.1944e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 6.0701e-06 - tp: 463.1624 - fp: 0.0000e+00 - tn: 480.7009 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.4993e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.0550e-05 - tp: 472.7094 - fp: 0.0000e+00 - tn: 471.1538 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.2995e-05 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 1.6614e-05 - tp: 469.7949 - fp: 0.0000e+00 - tn: 474.0684 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.7915e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 1.4199e-05 - tp: 471.2906 - fp: 0.0000e+00 - tn: 472.5726 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.1751e-06 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 8.5768e-06 - tp: 466.0598 - fp: 0.0000e+00 - tn: 477.8034 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.1526e-06 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 86ms/step - loss: 1.3904 - tp: 383.5556 - fp: 94.0769 - tn: 378.3761 - fn: 87.8547 - accuracy: 0.7505 - precision: 0.7561 - recall: 0.7664 - auc: 0.7988 - val_loss: 1.5011 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.7208\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0915 - tp: 449.6068 - fp: 18.6068 - tn: 461.2051 - fn: 14.4444 - accuracy: 0.9599 - precision: 0.9512 - recall: 0.9681 - auc: 0.9946 - val_loss: 1.1702 - val_tp: 73.0000 - val_fp: 66.0000 - val_tn: 5.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5417 - val_precision: 0.5252 - val_recall: 1.0000 - val_auc: 0.9623\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0657 - tp: 460.4017 - fp: 12.3675 - tn: 461.4017 - fn: 9.6923 - accuracy: 0.9819 - precision: 0.9793 - recall: 0.9845 - auc: 0.9962 - val_loss: 0.0704 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 3.0000 - val_accuracy: 0.9722 - val_precision: 0.9859 - val_recall: 0.9589 - val_auc: 0.9969\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0430 - tp: 473.3248 - fp: 3.7778 - tn: 461.3333 - fn: 5.4274 - accuracy: 0.9906 - precision: 0.9937 - recall: 0.9878 - auc: 0.9983 - val_loss: 0.0309 - val_tp: 69.0000 - val_fp: 2.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9718 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0234 - tp: 459.9744 - fp: 4.7094 - tn: 474.6667 - fn: 4.5128 - accuracy: 0.9908 - precision: 0.9894 - recall: 0.9919 - auc: 0.9989 - val_loss: 0.0378 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9859 - val_recall: 0.9859 - val_auc: 0.9994\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0018 - tp: 455.6752 - fp: 0.0000e+00 - tn: 488.1880 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0295 - val_tp: 75.0000 - val_fp: 1.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9868 - val_recall: 1.0000 - val_auc: 0.9999\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0018 - tp: 478.4786 - fp: 0.0000e+00 - tn: 465.3846 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1604 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 67.0000 - val_fn: 7.0000 - val_accuracy: 0.9444 - val_precision: 0.9857 - val_recall: 0.9079 - val_auc: 0.9851\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0060 - tp: 477.9316 - fp: 1.2564 - tn: 463.7436 - fn: 0.9316 - accuracy: 0.9986 - precision: 0.9984 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0897 - val_tp: 65.0000 - val_fp: 2.0000 - val_tn: 73.0000 - val_fn: 4.0000 - val_accuracy: 0.9583 - val_precision: 0.9701 - val_recall: 0.9420 - val_auc: 0.9973\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 76ms/step - loss: 0.1234 - tp: 445.0000 - fp: 20.9573 - tn: 455.9573 - fn: 21.9487 - accuracy: 0.9609 - precision: 0.9607 - recall: 0.9598 - auc: 0.9911 - val_loss: 0.1329 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 5.0000 - val_accuracy: 0.9653 - val_precision: 1.0000 - val_recall: 0.9306 - val_auc: 0.9914\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0321 - tp: 459.5470 - fp: 3.9060 - tn: 475.1282 - fn: 5.2821 - accuracy: 0.9908 - precision: 0.9927 - recall: 0.9885 - auc: 0.9987 - val_loss: 0.0331 - val_tp: 71.0000 - val_fp: 2.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9726 - val_recall: 1.0000 - val_auc: 0.9996\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0082 - tp: 467.7009 - fp: 0.9487 - tn: 474.2308 - fn: 0.9829 - accuracy: 0.9964 - precision: 0.9969 - recall: 0.9960 - auc: 1.0000 - val_loss: 0.0180 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9857 - val_recall: 1.0000 - val_auc: 0.9998\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0010 - tp: 467.0427 - fp: 0.0000e+00 - tn: 476.8205 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0379 - val_tp: 71.0000 - val_fp: 2.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9726 - val_recall: 1.0000 - val_auc: 0.9996\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 8.6863e-04 - tp: 464.7350 - fp: 0.0000e+00 - tn: 479.1282 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0421 - val_tp: 73.0000 - val_fp: 2.0000 - val_tn: 68.0000 - val_fn: 1.0000 - val_accuracy: 0.9792 - val_precision: 0.9733 - val_recall: 0.9865 - val_auc: 0.9996\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 1.7862e-04 - tp: 473.4615 - fp: 0.0000e+00 - tn: 470.4017 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0181 - val_tp: 74.0000 - val_fp: 1.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9867 - val_recall: 1.0000 - val_auc: 0.9998\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 2.7082e-04 - tp: 455.1880 - fp: 0.0000e+00 - tn: 488.6752 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0236 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9857 - val_recall: 0.9857 - val_auc: 0.9998\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 13s 93ms/step - loss: 1.2853 - tp: 413.7949 - fp: 41.4444 - tn: 447.6581 - fn: 40.9658 - accuracy: 0.8584 - precision: 0.8428 - recall: 0.8462 - auc: 0.8919 - val_loss: 1.3548 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 70.0000 - val_accuracy: 0.5139 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.9571\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0135 - tp: 470.9316 - fp: 2.4872 - tn: 469.2137 - fn: 1.2308 - accuracy: 0.9960 - precision: 0.9942 - recall: 0.9980 - auc: 0.9995 - val_loss: 0.5523 - val_tp: 23.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 46.0000 - val_accuracy: 0.6806 - val_precision: 1.0000 - val_recall: 0.3333 - val_auc: 1.0000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0155 - tp: 467.1026 - fp: 2.1282 - tn: 473.3932 - fn: 1.2393 - accuracy: 0.9951 - precision: 0.9932 - recall: 0.9967 - auc: 0.9997 - val_loss: 0.0420 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9857 - val_auc: 1.0000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0166 - tp: 470.0171 - fp: 0.9573 - tn: 470.1538 - fn: 2.7350 - accuracy: 0.9936 - precision: 0.9979 - recall: 0.9894 - auc: 0.9999 - val_loss: 0.0016 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0533 - tp: 457.9744 - fp: 6.0256 - tn: 471.7863 - fn: 8.0769 - accuracy: 0.9855 - precision: 0.9873 - recall: 0.9833 - auc: 0.9961 - val_loss: 5.3667e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0040 - tp: 464.8291 - fp: 0.7350 - tn: 478.2991 - fn: 0.0000e+00 - accuracy: 0.9993 - precision: 0.9986 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.5363e-06 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0050 - tp: 482.2222 - fp: 0.9402 - tn: 459.7265 - fn: 0.9744 - accuracy: 0.9967 - precision: 0.9973 - recall: 0.9966 - auc: 1.0000 - val_loss: 3.1683e-05 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 4.2754e-04 - tp: 470.9402 - fp: 0.1111 - tn: 472.4615 - fn: 0.3504 - accuracy: 0.9997 - precision: 0.9999 - recall: 0.9995 - auc: 1.0000 - val_loss: 2.1318e-06 - val_tp: 78.0000 - val_fp: 0.0000e+00 - val_tn: 66.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0122 - tp: 468.9829 - fp: 1.2735 - tn: 472.4274 - fn: 1.1795 - accuracy: 0.9976 - precision: 0.9972 - recall: 0.9980 - auc: 0.9994 - val_loss: 1.0811e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 2.4007e-04 - tp: 463.0000 - fp: 0.0000e+00 - tn: 480.7607 - fn: 0.1026 - accuracy: 0.9999 - precision: 1.0000 - recall: 0.9999 - auc: 1.0000 - val_loss: 1.8086e-04 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 3.4341e-04 - tp: 462.8547 - fp: 0.0000e+00 - tn: 481.0085 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.1834e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 5.5631e-05 - tp: 461.7094 - fp: 0.0000e+00 - tn: 482.1538 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.6530e-05 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0030 - tp: 469.1880 - fp: 0.7692 - tn: 472.6581 - fn: 1.2479 - accuracy: 0.9982 - precision: 0.9984 - recall: 0.9979 - auc: 1.0000 - val_loss: 2.8300e-04 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 4.9056e-04 - tp: 465.5385 - fp: 0.0000e+00 - tn: 478.3248 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0561e-04 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0015 - tp: 470.6325 - fp: 0.5812 - tn: 472.0769 - fn: 0.5726 - accuracy: 0.9992 - precision: 0.9992 - recall: 0.9993 - auc: 1.0000 - val_loss: 2.2974e-04 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 15s 108ms/step - loss: 2.7519 - tp: 404.0598 - fp: 71.8889 - tn: 392.6752 - fn: 75.2393 - accuracy: 0.7802 - precision: 0.7980 - recall: 0.7902 - auc: 0.7981 - val_loss: 1.2938 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.8839\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0377 - tp: 469.4188 - fp: 8.2564 - tn: 462.5385 - fn: 3.6496 - accuracy: 0.9876 - precision: 0.9824 - recall: 0.9932 - auc: 0.9965 - val_loss: 2.2530 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.9027\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0031 - tp: 468.0000 - fp: 0.0000e+00 - tn: 475.7265 - fn: 0.1368 - accuracy: 0.9999 - precision: 1.0000 - recall: 0.9998 - auc: 1.0000 - val_loss: 1.8073 - val_tp: 72.0000 - val_fp: 70.0000 - val_tn: 2.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5070 - val_recall: 1.0000 - val_auc: 0.9306\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.0027 - tp: 468.1026 - fp: 0.2991 - tn: 475.3932 - fn: 0.0684 - accuracy: 0.9998 - precision: 0.9996 - recall: 0.9999 - auc: 1.0000 - val_loss: 0.4258 - val_tp: 72.0000 - val_fp: 24.0000 - val_tn: 48.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8333 - val_precision: 0.7500 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0057 - tp: 474.1111 - fp: 0.9231 - tn: 468.0000 - fn: 0.8291 - accuracy: 0.9977 - precision: 0.9974 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0298 - val_tp: 71.0000 - val_fp: 2.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9726 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0035 - tp: 457.1282 - fp: 0.2051 - tn: 485.7607 - fn: 0.7692 - accuracy: 0.9994 - precision: 0.9998 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0125 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 11s 97ms/step - loss: 0.0031 - tp: 466.7350 - fp: 1.1368 - tn: 475.4017 - fn: 0.5897 - accuracy: 0.9986 - precision: 0.9981 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0233 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0024 - tp: 465.9316 - fp: 1.0171 - tn: 476.9145 - fn: 0.0000e+00 - accuracy: 0.9988 - precision: 0.9976 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0080 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9861 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0011 - tp: 457.7692 - fp: 0.0000e+00 - tn: 486.0940 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0206 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 3.0006e-04 - tp: 462.8547 - fp: 0.0000e+00 - tn: 481.0085 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0460 - val_tp: 72.0000 - val_fp: 2.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9730 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 2.1527e-04 - tp: 470.1282 - fp: 0.0000e+00 - tn: 473.7350 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0197 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 7.8156e-05 - tp: 473.1026 - fp: 0.0000e+00 - tn: 470.7607 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0200 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 1.1900e-04 - tp: 472.1966 - fp: 0.0000e+00 - tn: 471.6667 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0199 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9861 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 1.1700e-04 - tp: 470.0855 - fp: 0.0000e+00 - tn: 473.7778 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0219 - val_tp: 74.0000 - val_fp: 1.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9867 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 5.5919e-05 - tp: 492.7863 - fp: 0.0000e+00 - tn: 451.0769 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0197 - val_tp: 75.0000 - val_fp: 1.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9868 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "FOLD 5\n",
      "{0: 0.994092373791622, 1: 1.0059782608695653}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 80s 594ms/step - loss: 2.6374 - tp: 346.5470 - fp: 137.9573 - tn: 338.0684 - fn: 121.2906 - accuracy: 0.6530 - precision: 0.6423 - recall: 0.6744 - auc: 0.6661 - val_loss: 2.2215 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.7372\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 10s 89ms/step - loss: 0.0172 - tp: 477.8205 - fp: 2.4872 - tn: 462.4103 - fn: 1.1453 - accuracy: 0.9961 - precision: 0.9940 - recall: 0.9985 - auc: 0.9998 - val_loss: 4.5388 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0127 - tp: 472.7350 - fp: 1.9744 - tn: 468.4188 - fn: 0.7350 - accuracy: 0.9972 - precision: 0.9959 - recall: 0.9986 - auc: 0.9999 - val_loss: 4.8522 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0083 - tp: 477.1282 - fp: 2.1026 - tn: 464.1538 - fn: 0.4786 - accuracy: 0.9969 - precision: 0.9945 - recall: 0.9993 - auc: 1.0000 - val_loss: 1.3878 - val_tp: 73.0000 - val_fp: 42.0000 - val_tn: 29.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7083 - val_precision: 0.6348 - val_recall: 1.0000 - val_auc: 0.8592\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0017 - tp: 456.2991 - fp: 0.0000e+00 - tn: 487.5641 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1628 - val_tp: 70.0000 - val_fp: 12.0000 - val_tn: 62.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9167 - val_precision: 0.8537 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0141 - tp: 469.4957 - fp: 2.3590 - tn: 470.5812 - fn: 1.4274 - accuracy: 0.9957 - precision: 0.9946 - recall: 0.9969 - auc: 0.9998 - val_loss: 1.0854 - val_tp: 73.0000 - val_fp: 35.0000 - val_tn: 36.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7569 - val_precision: 0.6759 - val_recall: 1.0000 - val_auc: 0.9296\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 62ms/step - loss: 0.0030 - tp: 479.4530 - fp: 0.0000e+00 - tn: 464.4103 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0270 - val_tp: 72.0000 - val_fp: 2.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9730 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 8.3411e-04 - tp: 472.6410 - fp: 0.0000e+00 - tn: 471.2222 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0324 - val_tp: 70.0000 - val_fp: 2.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9722 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 7.6408e-04 - tp: 466.3504 - fp: 0.0000e+00 - tn: 477.5128 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0439 - val_tp: 70.0000 - val_fp: 3.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9589 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0011 - tp: 454.8120 - fp: 0.0000e+00 - tn: 489.0513 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0069 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 1.2316e-04 - tp: 477.1795 - fp: 0.0000e+00 - tn: 466.6838 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0036 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.6689e-04 - tp: 445.8120 - fp: 0.0000e+00 - tn: 498.0513 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0048 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 9.4301e-04 - tp: 471.2564 - fp: 0.0000e+00 - tn: 472.6068 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0020 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 3.0904e-04 - tp: 475.5128 - fp: 0.0000e+00 - tn: 468.3504 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.7421e-04 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.4669e-04 - tp: 458.9573 - fp: 0.0000e+00 - tn: 484.9060 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0012 - val_tp: 76.0000 - val_fp: 0.0000e+00 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 85ms/step - loss: 1.6735 - tp: 348.7350 - fp: 106.5128 - tn: 371.4188 - fn: 117.1966 - accuracy: 0.7025 - precision: 0.6935 - recall: 0.6745 - auc: 0.7461 - val_loss: 2.3744 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.5191\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0761 - tp: 464.3248 - fp: 14.5470 - tn: 453.2650 - fn: 11.7265 - accuracy: 0.9723 - precision: 0.9698 - recall: 0.9759 - auc: 0.9957 - val_loss: 2.8328 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.5214\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0517 - tp: 450.1880 - fp: 5.8974 - tn: 477.8034 - fn: 9.9744 - accuracy: 0.9844 - precision: 0.9886 - recall: 0.9791 - auc: 0.9980 - val_loss: 3.3596 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.5500\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0307 - tp: 460.6410 - fp: 5.0684 - tn: 470.6581 - fn: 7.4957 - accuracy: 0.9896 - precision: 0.9925 - recall: 0.9867 - auc: 0.9993 - val_loss: 2.3305 - val_tp: 72.0000 - val_fp: 69.0000 - val_tn: 3.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5208 - val_precision: 0.5106 - val_recall: 1.0000 - val_auc: 0.7708\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0189 - tp: 473.8376 - fp: 2.9658 - tn: 463.2906 - fn: 3.7692 - accuracy: 0.9907 - precision: 0.9905 - recall: 0.9914 - auc: 0.9998 - val_loss: 1.7112 - val_tp: 72.0000 - val_fp: 51.0000 - val_tn: 21.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6458 - val_precision: 0.5854 - val_recall: 1.0000 - val_auc: 0.8611\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 76ms/step - loss: 0.0073 - tp: 472.2821 - fp: 1.4957 - tn: 469.0085 - fn: 1.0769 - accuracy: 0.9977 - precision: 0.9975 - recall: 0.9980 - auc: 1.0000 - val_loss: 0.1514 - val_tp: 73.0000 - val_fp: 8.0000 - val_tn: 63.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9444 - val_precision: 0.9012 - val_recall: 1.0000 - val_auc: 0.9994\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0364 - tp: 465.0085 - fp: 4.5556 - tn: 467.7009 - fn: 6.5983 - accuracy: 0.9876 - precision: 0.9897 - recall: 0.9855 - auc: 0.9985 - val_loss: 0.0438 - val_tp: 70.0000 - val_fp: 2.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9722 - val_recall: 1.0000 - val_auc: 0.9998\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0309 - tp: 466.8034 - fp: 4.4957 - tn: 467.2650 - fn: 5.2991 - accuracy: 0.9869 - precision: 0.9879 - recall: 0.9859 - auc: 0.9994 - val_loss: 0.0115 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0050 - tp: 473.9402 - fp: 0.5043 - tn: 468.5897 - fn: 0.8291 - accuracy: 0.9992 - precision: 0.9994 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.0122 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0161 - tp: 461.0085 - fp: 3.3248 - tn: 476.9744 - fn: 2.5556 - accuracy: 0.9941 - precision: 0.9911 - recall: 0.9968 - auc: 0.9993 - val_loss: 0.1843 - val_tp: 71.0000 - val_fp: 11.0000 - val_tn: 62.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9236 - val_precision: 0.8659 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0081 - tp: 466.3077 - fp: 1.9744 - tn: 474.3248 - fn: 1.2564 - accuracy: 0.9970 - precision: 0.9959 - recall: 0.9981 - auc: 1.0000 - val_loss: 0.0304 - val_tp: 73.0000 - val_fp: 2.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9733 - val_recall: 1.0000 - val_auc: 0.9996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0041 - tp: 466.4188 - fp: 0.9231 - tn: 476.5214 - fn: 0.0000e+00 - accuracy: 0.9987 - precision: 0.9972 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0026 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 4.0265e-04 - tp: 469.9744 - fp: 0.0000e+00 - tn: 473.8889 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0205 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 1.3997e-04 - tp: 476.1368 - fp: 0.0000e+00 - tn: 467.7265 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0128 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9861 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 5.1443e-05 - tp: 466.6496 - fp: 0.0000e+00 - tn: 477.2137 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0144 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9726 - val_auc: 1.0000\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 87ms/step - loss: 1.2739 - tp: 431.0427 - fp: 37.2051 - tn: 438.9231 - fn: 36.6923 - accuracy: 0.8662 - precision: 0.8613 - recall: 0.8601 - auc: 0.8956 - val_loss: 0.8073 - val_tp: 13.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 59.0000 - val_accuracy: 0.5903 - val_precision: 1.0000 - val_recall: 0.1806 - val_auc: 0.9407\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0240 - tp: 465.8205 - fp: 2.1282 - tn: 472.1282 - fn: 3.7863 - accuracy: 0.9945 - precision: 0.9967 - recall: 0.9923 - auc: 0.9984 - val_loss: 0.1469 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 64.0000 - val_fn: 8.0000 - val_accuracy: 0.9375 - val_precision: 0.9861 - val_recall: 0.8987 - val_auc: 0.9950\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0024 - tp: 476.3248 - fp: 0.4017 - tn: 466.7265 - fn: 0.4103 - accuracy: 0.9994 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.1328 - val_tp: 70.0000 - val_fp: 4.0000 - val_tn: 68.0000 - val_fn: 2.0000 - val_accuracy: 0.9583 - val_precision: 0.9459 - val_recall: 0.9722 - val_auc: 0.9896\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 2.5278e-04 - tp: 471.1538 - fp: 0.0000e+00 - tn: 472.7094 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0699 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9722 - val_auc: 0.9946\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 1.3773e-04 - tp: 469.2735 - fp: 0.0000e+00 - tn: 474.5897 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0718 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9722 - val_auc: 0.9969\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 1.9884e-04 - tp: 474.9060 - fp: 0.0000e+00 - tn: 468.9573 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0800 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9730 - val_auc: 0.9865\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0012 - tp: 474.8120 - fp: 0.5641 - tn: 468.0256 - fn: 0.4615 - accuracy: 0.9993 - precision: 0.9992 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.1391 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 4.0000 - val_accuracy: 0.9722 - val_precision: 1.0000 - val_recall: 0.9437 - val_auc: 0.9855\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0287 - tp: 459.2393 - fp: 3.6667 - tn: 476.9573 - fn: 4.0000 - accuracy: 0.9929 - precision: 0.9935 - recall: 0.9918 - auc: 0.9988 - val_loss: 0.1214 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9589 - val_auc: 0.9789\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0246 - tp: 454.5556 - fp: 3.2906 - tn: 482.6325 - fn: 3.3846 - accuracy: 0.9929 - precision: 0.9926 - recall: 0.9927 - auc: 0.9992 - val_loss: 0.0545 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9726 - val_auc: 0.9990\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0206 - tp: 468.7692 - fp: 1.2479 - tn: 472.4444 - fn: 1.4017 - accuracy: 0.9975 - precision: 0.9975 - recall: 0.9975 - auc: 0.9990 - val_loss: 0.0653 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9726 - val_auc: 0.9981\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0051 - tp: 467.7607 - fp: 1.4274 - tn: 474.0940 - fn: 0.5812 - accuracy: 0.9978 - precision: 0.9967 - recall: 0.9991 - auc: 0.9996 - val_loss: 0.0797 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9733 - val_auc: 0.9865\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 3.4330e-04 - tp: 476.5299 - fp: 0.0000e+00 - tn: 467.3333 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0802 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9718 - val_auc: 0.9857\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 1.3196e-04 - tp: 471.8718 - fp: 0.0000e+00 - tn: 471.9915 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0448 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9859 - val_auc: 0.9930\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 4.4606e-05 - tp: 467.0513 - fp: 0.0000e+00 - tn: 476.8120 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0936 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9718 - val_auc: 0.9859\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 7.1616e-05 - tp: 460.1880 - fp: 0.0000e+00 - tn: 483.6752 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0481 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 0.9932\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 109ms/step - loss: 3.2531 - tp: 359.8547 - fp: 125.7692 - tn: 339.7436 - fn: 118.4957 - accuracy: 0.6677 - precision: 0.6683 - recall: 0.6827 - auc: 0.7018 - val_loss: 0.5892 - val_tp: 73.0000 - val_fp: 62.0000 - val_tn: 9.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5694 - val_precision: 0.5407 - val_recall: 1.0000 - val_auc: 0.9309\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.0480 - tp: 467.3932 - fp: 4.5897 - tn: 467.4188 - fn: 4.4615 - accuracy: 0.9903 - precision: 0.9905 - recall: 0.9902 - auc: 0.9967 - val_loss: 0.4094 - val_tp: 71.0000 - val_fp: 19.0000 - val_tn: 54.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8681 - val_precision: 0.7889 - val_recall: 1.0000 - val_auc: 0.9928\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0159 - tp: 471.8120 - fp: 2.4274 - tn: 468.9231 - fn: 0.7009 - accuracy: 0.9970 - precision: 0.9949 - recall: 0.9991 - auc: 0.9998 - val_loss: 0.3289 - val_tp: 74.0000 - val_fp: 20.0000 - val_tn: 50.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8611 - val_precision: 0.7872 - val_recall: 1.0000 - val_auc: 0.9994\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0086 - tp: 468.0085 - fp: 2.2821 - tn: 473.1026 - fn: 0.4701 - accuracy: 0.9978 - precision: 0.9962 - recall: 0.9993 - auc: 0.9999 - val_loss: 0.0408 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.0124 - tp: 466.5470 - fp: 0.0000e+00 - tn: 475.4786 - fn: 1.8376 - accuracy: 0.9962 - precision: 1.0000 - recall: 0.9927 - auc: 1.0000 - val_loss: 0.0037 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0026 - tp: 452.8291 - fp: 0.0000e+00 - tn: 490.5641 - fn: 0.4701 - accuracy: 0.9997 - precision: 1.0000 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0036 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0067 - tp: 479.4957 - fp: 0.8376 - tn: 463.5299 - fn: 0.0000e+00 - accuracy: 0.9990 - precision: 0.9981 - recall: 1.0000 - auc: 0.9999 - val_loss: 0.0012 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 5.1397e-04 - tp: 471.1282 - fp: 0.0000e+00 - tn: 472.7350 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.0338e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 102ms/step - loss: 8.5694e-04 - tp: 470.3248 - fp: 0.0000e+00 - tn: 473.5385 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.6279e-04 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 4.5045e-04 - tp: 467.2991 - fp: 0.0000e+00 - tn: 476.5641 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.4921e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 2.7753e-04 - tp: 471.6838 - fp: 0.0000e+00 - tn: 472.1795 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.3235e-04 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 3.5873e-04 - tp: 471.0427 - fp: 0.0000e+00 - tn: 472.8205 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.1981e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 5.0182e-04 - tp: 470.7607 - fp: 0.0000e+00 - tn: 473.1026 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.7426e-04 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0040 - tp: 464.2479 - fp: 0.9829 - tn: 478.6325 - fn: 0.0000e+00 - accuracy: 0.9979 - precision: 0.9958 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.7708e-04 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 5.2447e-04 - tp: 469.8205 - fp: 0.0000e+00 - tn: 474.0427 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.6437e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "FOLD 6\n",
      "{0: 0.994092373791622, 1: 1.0059782608695653}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 82s 608ms/step - loss: 1.5238 - tp: 421.2051 - fp: 55.3932 - tn: 412.0256 - fn: 55.2393 - accuracy: 0.8173 - precision: 0.8193 - recall: 0.8226 - auc: 0.8485 - val_loss: 1.6537 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 10s 87ms/step - loss: 0.0166 - tp: 479.3846 - fp: 2.4444 - tn: 462.0342 - fn: 0.0000e+00 - accuracy: 0.9972 - precision: 0.9946 - recall: 1.0000 - auc: 0.9980 - val_loss: 2.9762 - val_tp: 68.0000 - val_fp: 76.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4722 - val_precision: 0.4722 - val_recall: 1.0000 - val_auc: 0.5066\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0121 - tp: 471.5983 - fp: 2.4701 - tn: 467.8718 - fn: 1.9231 - accuracy: 0.9967 - precision: 0.9960 - recall: 0.9975 - auc: 0.9991 - val_loss: 1.4238 - val_tp: 72.0000 - val_fp: 39.0000 - val_tn: 33.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7292 - val_precision: 0.6486 - val_recall: 1.0000 - val_auc: 0.8472\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0093 - tp: 471.0513 - fp: 0.0000e+00 - tn: 470.8547 - fn: 1.9573 - accuracy: 0.9959 - precision: 1.0000 - recall: 0.9919 - auc: 1.0000 - val_loss: 2.9572 - val_tp: 75.0000 - val_fp: 50.0000 - val_tn: 19.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6528 - val_precision: 0.6000 - val_recall: 1.0000 - val_auc: 0.7174\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0010 - tp: 474.5385 - fp: 0.0000e+00 - tn: 469.1368 - fn: 0.1880 - accuracy: 0.9999 - precision: 1.0000 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.8433 - val_tp: 73.0000 - val_fp: 21.0000 - val_tn: 50.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8542 - val_precision: 0.7766 - val_recall: 1.0000 - val_auc: 0.9296\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0016 - tp: 478.0855 - fp: 0.1880 - tn: 465.2991 - fn: 0.2906 - accuracy: 0.9997 - precision: 0.9998 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.2850 - val_tp: 48.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 22.0000 - val_accuracy: 0.8472 - val_precision: 1.0000 - val_recall: 0.6857 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0026 - tp: 460.5128 - fp: 0.6838 - tn: 482.6667 - fn: 0.0000e+00 - accuracy: 0.9994 - precision: 0.9988 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0157 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9859 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 2.1132e-04 - tp: 469.9487 - fp: 0.0000e+00 - tn: 473.9145 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0793e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 8.8503e-05 - tp: 473.4701 - fp: 0.0000e+00 - tn: 470.3932 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.5637e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 6.5399e-05 - tp: 465.8718 - fp: 0.0000e+00 - tn: 477.9915 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.3663e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 4.3677e-05 - tp: 464.3248 - fp: 0.0000e+00 - tn: 479.5385 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.3800e-05 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 4.3564e-05 - tp: 468.8291 - fp: 0.0000e+00 - tn: 475.0342 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.9640e-05 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 6.6391e-04 - tp: 457.4615 - fp: 0.0000e+00 - tn: 486.4017 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.5144e-05 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 5.0397e-05 - tp: 470.8462 - fp: 0.0000e+00 - tn: 473.0171 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.1736e-05 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 3.8254e-05 - tp: 478.1966 - fp: 0.0000e+00 - tn: 465.6667 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.1247e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 86ms/step - loss: 2.1638 - tp: 334.6410 - fp: 132.8547 - tn: 343.4103 - fn: 132.9573 - accuracy: 0.6687 - precision: 0.6556 - recall: 0.6516 - auc: 0.7039 - val_loss: 0.8164 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.6007\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.1341 - tp: 443.6923 - fp: 22.7436 - tn: 454.1197 - fn: 23.3077 - accuracy: 0.9541 - precision: 0.9547 - recall: 0.9519 - auc: 0.9879 - val_loss: 0.6921 - val_tp: 75.0000 - val_fp: 69.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5208 - val_precision: 0.5208 - val_recall: 1.0000 - val_auc: 0.9864\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0359 - tp: 461.4872 - fp: 6.2137 - tn: 470.7265 - fn: 5.4359 - accuracy: 0.9881 - precision: 0.9858 - recall: 0.9904 - auc: 0.9991 - val_loss: 0.1193 - val_tp: 73.0000 - val_fp: 3.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9605 - val_recall: 1.0000 - val_auc: 0.9990\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0218 - tp: 455.9658 - fp: 2.4103 - tn: 479.1197 - fn: 6.3675 - accuracy: 0.9905 - precision: 0.9959 - recall: 0.9849 - auc: 0.9998 - val_loss: 0.0226 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0210 - tp: 455.1966 - fp: 3.5043 - tn: 481.2137 - fn: 3.9487 - accuracy: 0.9930 - precision: 0.9929 - recall: 0.9924 - auc: 0.9997 - val_loss: 0.0177 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9857 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0110 - tp: 466.8547 - fp: 1.0342 - tn: 474.2564 - fn: 1.7179 - accuracy: 0.9972 - precision: 0.9985 - recall: 0.9959 - auc: 0.9999 - val_loss: 0.0131 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0063 - tp: 463.8974 - fp: 2.5470 - tn: 477.0427 - fn: 0.3761 - accuracy: 0.9967 - precision: 0.9937 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0025 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0050 - tp: 473.4274 - fp: 1.1624 - tn: 469.0940 - fn: 0.1795 - accuracy: 0.9991 - precision: 0.9984 - recall: 0.9998 - auc: 1.0000 - val_loss: 0.0079 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0233 - tp: 455.1880 - fp: 3.5983 - tn: 482.1709 - fn: 2.9060 - accuracy: 0.9909 - precision: 0.9890 - recall: 0.9929 - auc: 0.9996 - val_loss: 0.3282 - val_tp: 59.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 12.0000 - val_accuracy: 0.9167 - val_precision: 1.0000 - val_recall: 0.8310 - val_auc: 0.9789\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0158 - tp: 455.4701 - fp: 2.4786 - tn: 482.4872 - fn: 3.4274 - accuracy: 0.9921 - precision: 0.9938 - recall: 0.9896 - auc: 0.9999 - val_loss: 0.1712 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 5.0000 - val_accuracy: 0.9653 - val_precision: 1.0000 - val_recall: 0.9306 - val_auc: 0.9931\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0300 - tp: 454.5214 - fp: 3.1880 - tn: 479.8718 - fn: 6.2821 - accuracy: 0.9888 - precision: 0.9944 - recall: 0.9825 - auc: 0.9996 - val_loss: 0.2736 - val_tp: 71.0000 - val_fp: 17.0000 - val_tn: 56.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8819 - val_precision: 0.8068 - val_recall: 1.0000 - val_auc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0101 - tp: 457.2308 - fp: 1.5897 - tn: 483.3932 - fn: 1.6496 - accuracy: 0.9964 - precision: 0.9965 - recall: 0.9959 - auc: 0.9999 - val_loss: 3.1395e-04 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 9.2745e-04 - tp: 473.3248 - fp: 0.0000e+00 - tn: 470.5385 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.9823e-04 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 9.6502e-05 - tp: 472.1538 - fp: 0.0000e+00 - tn: 471.7094 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.2466e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 3.0136e-04 - tp: 472.3761 - fp: 0.0000e+00 - tn: 471.4872 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.4486e-05 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 89ms/step - loss: 1.3131 - tp: 385.7692 - fp: 65.7949 - tn: 422.6325 - fn: 69.6667 - accuracy: 0.7929 - precision: 0.7828 - recall: 0.7888 - auc: 0.8385 - val_loss: 0.4641 - val_tp: 36.0000 - val_fp: 0.0000e+00 - val_tn: 68.0000 - val_fn: 40.0000 - val_accuracy: 0.7222 - val_precision: 1.0000 - val_recall: 0.4737 - val_auc: 0.9999\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0144 - tp: 475.5897 - fp: 1.9231 - tn: 464.6410 - fn: 1.7094 - accuracy: 0.9970 - precision: 0.9969 - recall: 0.9972 - auc: 0.9990 - val_loss: 1.0186 - val_tp: 28.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 45.0000 - val_accuracy: 0.6875 - val_precision: 1.0000 - val_recall: 0.3836 - val_auc: 0.9963\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0178 - tp: 455.4872 - fp: 1.4786 - tn: 486.3761 - fn: 0.5214 - accuracy: 0.9979 - precision: 0.9964 - recall: 0.9992 - auc: 0.9982 - val_loss: 0.9276 - val_tp: 33.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 35.0000 - val_accuracy: 0.7569 - val_precision: 1.0000 - val_recall: 0.4853 - val_auc: 0.9918\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0015 - tp: 465.5556 - fp: 0.1966 - tn: 478.1111 - fn: 0.0000e+00 - accuracy: 0.9999 - precision: 0.9998 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0136 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0013 - tp: 455.1453 - fp: 0.0000e+00 - tn: 488.7179 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0014 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 1.2464e-04 - tp: 463.0085 - fp: 0.0000e+00 - tn: 480.8547 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0098e-04 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 8.3365e-05 - tp: 470.9145 - fp: 0.0000e+00 - tn: 472.9487 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.6640e-04 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 7.4540e-05 - tp: 475.5214 - fp: 0.0000e+00 - tn: 468.3419 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.4576e-05 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 6.5770e-05 - tp: 467.8205 - fp: 0.0000e+00 - tn: 476.0427 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.7332e-05 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 4.0997e-05 - tp: 461.8462 - fp: 0.0000e+00 - tn: 482.0171 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.8445e-05 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 7.5161e-05 - tp: 466.9573 - fp: 0.0000e+00 - tn: 476.9060 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.3966e-06 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 2.9577e-05 - tp: 474.0855 - fp: 0.0000e+00 - tn: 469.7778 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0285e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 1.1492e-04 - tp: 471.1282 - fp: 0.0000e+00 - tn: 472.7350 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.1438e-05 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 3.7104e-05 - tp: 456.0940 - fp: 0.0000e+00 - tn: 487.7692 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.2871e-06 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0216 - tp: 468.2991 - fp: 3.8889 - tn: 468.5043 - fn: 3.1709 - accuracy: 0.9951 - precision: 0.9945 - recall: 0.9956 - auc: 0.9987 - val_loss: 6.6709 - val_tp: 3.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 70.0000 - val_accuracy: 0.5139 - val_precision: 1.0000 - val_recall: 0.0411 - val_auc: 0.5616\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 15s 109ms/step - loss: 2.4033 - tp: 395.0085 - fp: 69.7607 - tn: 406.0171 - fn: 73.0769 - accuracy: 0.7805 - precision: 0.7778 - recall: 0.7731 - auc: 0.8098 - val_loss: 0.6303 - val_tp: 39.0000 - val_fp: 21.0000 - val_tn: 55.0000 - val_fn: 29.0000 - val_accuracy: 0.6528 - val_precision: 0.6500 - val_recall: 0.5735 - val_auc: 0.7871\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.0367 - tp: 464.1368 - fp: 4.9316 - tn: 470.0769 - fn: 4.7179 - accuracy: 0.9897 - precision: 0.9897 - recall: 0.9895 - auc: 0.9973 - val_loss: 0.4201 - val_tp: 67.0000 - val_fp: 24.0000 - val_tn: 50.0000 - val_fn: 3.0000 - val_accuracy: 0.8125 - val_precision: 0.7363 - val_recall: 0.9571 - val_auc: 0.9623\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0061 - tp: 471.4444 - fp: 0.5641 - tn: 471.1880 - fn: 0.6667 - accuracy: 0.9991 - precision: 0.9991 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.3229 - val_tp: 72.0000 - val_fp: 19.0000 - val_tn: 53.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8681 - val_precision: 0.7912 - val_recall: 1.0000 - val_auc: 0.9994\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 7.0203e-04 - tp: 465.2650 - fp: 0.0000e+00 - tn: 478.5983 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0396 - val_tp: 70.0000 - val_fp: 2.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9722 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 3.7552e-04 - tp: 482.6496 - fp: 0.0000e+00 - tn: 461.2137 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0028 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 7.0097e-04 - tp: 479.5897 - fp: 0.0769 - tn: 464.1966 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 0.9999 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0689 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 68.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9737 - val_auc: 0.9934\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.0026 - tp: 477.6410 - fp: 0.1880 - tn: 465.6667 - fn: 0.3675 - accuracy: 0.9997 - precision: 0.9998 - recall: 0.9996 - auc: 0.9999 - val_loss: 1.5070 - val_tp: 32.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 40.0000 - val_accuracy: 0.7222 - val_precision: 1.0000 - val_recall: 0.4444 - val_auc: 0.8819\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0796 - tp: 459.6239 - fp: 10.7009 - tn: 464.6752 - fn: 8.8632 - accuracy: 0.9763 - precision: 0.9727 - recall: 0.9801 - auc: 0.9956 - val_loss: 0.6514 - val_tp: 58.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 12.0000 - val_accuracy: 0.9167 - val_precision: 1.0000 - val_recall: 0.8286 - val_auc: 0.9500\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0110 - tp: 468.5299 - fp: 2.7521 - tn: 471.1453 - fn: 1.4359 - accuracy: 0.9959 - precision: 0.9945 - recall: 0.9973 - auc: 0.9999 - val_loss: 0.0969 - val_tp: 65.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 5.0000 - val_accuracy: 0.9653 - val_precision: 1.0000 - val_recall: 0.9286 - val_auc: 0.9929\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.0096 - tp: 471.0342 - fp: 1.8547 - tn: 470.2650 - fn: 0.7094 - accuracy: 0.9977 - precision: 0.9967 - recall: 0.9987 - auc: 0.9991 - val_loss: 2.4472e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 6.4619e-05 - tp: 468.3419 - fp: 0.0000e+00 - tn: 475.5214 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.4266e-04 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 3.5444e-05 - tp: 461.8376 - fp: 0.0000e+00 - tn: 482.0256 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.2205e-04 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 1.5341e-04 - tp: 467.6752 - fp: 0.0000e+00 - tn: 476.1880 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.1481e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 1.9553e-05 - tp: 465.9231 - fp: 0.0000e+00 - tn: 477.9402 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.6045e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 1.2242e-05 - tp: 479.7863 - fp: 0.0000e+00 - tn: 464.0769 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.2296e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "FOLD 7\n",
      "{0: 0.994092373791622, 1: 1.0059782608695653}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 78s 582ms/step - loss: 1.4633 - tp: 402.0256 - fp: 59.8889 - tn: 417.6581 - fn: 64.2906 - accuracy: 0.7937 - precision: 0.7875 - recall: 0.7762 - auc: 0.8472 - val_loss: 1.3427 - val_tp: 69.0000 - val_fp: 75.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4792 - val_recall: 1.0000 - val_auc: 0.9615\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 12s 102ms/step - loss: 0.0074 - tp: 464.5214 - fp: 1.4274 - tn: 477.5812 - fn: 0.3333 - accuracy: 0.9986 - precision: 0.9975 - recall: 0.9996 - auc: 1.0000 - val_loss: 1.1865 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.9967\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0035 - tp: 485.0085 - fp: 0.0000e+00 - tn: 458.8547 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3640 - val_tp: 72.0000 - val_fp: 28.0000 - val_tn: 44.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8056 - val_precision: 0.7200 - val_recall: 1.0000 - val_auc: 0.9999\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0014 - tp: 469.1197 - fp: 0.1880 - tn: 474.4530 - fn: 0.1026 - accuracy: 0.9998 - precision: 0.9998 - recall: 0.9999 - auc: 0.9999 - val_loss: 0.7444 - val_tp: 71.0000 - val_fp: 37.0000 - val_tn: 36.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7431 - val_precision: 0.6574 - val_recall: 1.0000 - val_auc: 0.9795\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0047 - tp: 460.5385 - fp: 0.1197 - tn: 483.2051 - fn: 0.0000e+00 - accuracy: 0.9999 - precision: 0.9999 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0063 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 9.9590e-04 - tp: 464.0598 - fp: 0.0000e+00 - tn: 479.8034 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.4678e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 4.4413e-04 - tp: 465.1453 - fp: 0.0000e+00 - tn: 478.7179 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.1408e-05 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 7.2018e-05 - tp: 470.2650 - fp: 0.0000e+00 - tn: 473.5983 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.6488e-05 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 3.7222e-05 - tp: 482.1966 - fp: 0.0000e+00 - tn: 461.6667 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.8916e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 2.9201e-05 - tp: 473.2479 - fp: 0.0000e+00 - tn: 470.6154 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.9518e-05 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 9.3195e-05 - tp: 468.1624 - fp: 0.0000e+00 - tn: 475.7009 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.1508e-05 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.8099e-05 - tp: 476.4274 - fp: 0.0000e+00 - tn: 467.4359 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0626e-04 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.9530e-05 - tp: 465.2906 - fp: 0.0000e+00 - tn: 478.5726 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.8819e-05 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 1.8176e-05 - tp: 468.4701 - fp: 0.0000e+00 - tn: 475.3932 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.6084e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 4.0937e-05 - tp: 465.1197 - fp: 0.0000e+00 - tn: 478.7436 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 9.4474e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 86ms/step - loss: 1.8803 - tp: 339.0855 - fp: 130.3675 - tn: 346.3248 - fn: 128.0855 - accuracy: 0.6731 - precision: 0.6703 - recall: 0.6732 - auc: 0.6937 - val_loss: 1.1839 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.4610\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.1302 - tp: 447.8547 - fp: 17.6154 - tn: 458.7009 - fn: 19.6923 - accuracy: 0.9566 - precision: 0.9611 - recall: 0.9508 - auc: 0.9887 - val_loss: 1.9708 - val_tp: 78.0000 - val_fp: 66.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5417 - val_precision: 0.5417 - val_recall: 1.0000 - val_auc: 0.5554\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 76ms/step - loss: 0.0282 - tp: 458.8291 - fp: 2.3248 - tn: 477.8291 - fn: 4.8803 - accuracy: 0.9933 - precision: 0.9960 - recall: 0.9903 - auc: 0.9996 - val_loss: 2.0432 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.8961\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0211 - tp: 469.9316 - fp: 3.3846 - tn: 468.1197 - fn: 2.4274 - accuracy: 0.9934 - precision: 0.9915 - recall: 0.9952 - auc: 0.9998 - val_loss: 0.3007 - val_tp: 72.0000 - val_fp: 22.0000 - val_tn: 50.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8472 - val_precision: 0.7660 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0149 - tp: 470.4274 - fp: 1.7350 - tn: 469.7436 - fn: 1.9573 - accuracy: 0.9966 - precision: 0.9969 - recall: 0.9963 - auc: 0.9999 - val_loss: 0.0183 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0158 - tp: 462.7863 - fp: 3.9145 - tn: 473.8632 - fn: 3.2991 - accuracy: 0.9943 - precision: 0.9931 - recall: 0.9953 - auc: 0.9998 - val_loss: 0.0366 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9571 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0153 - tp: 461.7607 - fp: 0.7949 - tn: 477.9487 - fn: 3.3590 - accuracy: 0.9946 - precision: 0.9983 - recall: 0.9906 - auc: 0.9998 - val_loss: 0.0854 - val_tp: 74.0000 - val_fp: 2.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9737 - val_recall: 1.0000 - val_auc: 0.9929\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0011 - tp: 457.9744 - fp: 0.0000e+00 - tn: 485.8889 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0776 - val_tp: 73.0000 - val_fp: 4.0000 - val_tn: 67.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9722 - val_precision: 0.9481 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 76ms/step - loss: 0.0032 - tp: 470.0769 - fp: 1.0000 - tn: 472.3504 - fn: 0.4359 - accuracy: 0.9991 - precision: 0.9986 - recall: 0.9995 - auc: 1.0000 - val_loss: 2.1079 - val_tp: 26.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 48.0000 - val_accuracy: 0.6667 - val_precision: 1.0000 - val_recall: 0.3514 - val_auc: 0.8311\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0433 - tp: 464.6581 - fp: 5.3077 - tn: 467.2137 - fn: 6.6838 - accuracy: 0.9842 - precision: 0.9869 - recall: 0.9822 - auc: 0.9977 - val_loss: 4.5681 - val_tp: 6.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 68.0000 - val_accuracy: 0.5278 - val_precision: 1.0000 - val_recall: 0.0811 - val_auc: 0.6351\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0095 - tp: 455.9316 - fp: 1.1966 - tn: 485.3504 - fn: 1.3846 - accuracy: 0.9976 - precision: 0.9978 - recall: 0.9972 - auc: 1.0000 - val_loss: 0.0482 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 2.0000 - val_accuracy: 0.9792 - val_precision: 0.9857 - val_recall: 0.9718 - val_auc: 0.9996\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0096 - tp: 475.4444 - fp: 0.6923 - tn: 466.0000 - fn: 1.7265 - accuracy: 0.9978 - precision: 0.9991 - recall: 0.9965 - auc: 0.9999 - val_loss: 0.0858 - val_tp: 69.0000 - val_fp: 2.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9718 - val_recall: 1.0000 - val_auc: 0.9863\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0153 - tp: 462.5043 - fp: 3.3504 - tn: 475.4872 - fn: 2.5214 - accuracy: 0.9959 - precision: 0.9951 - recall: 0.9965 - auc: 0.9996 - val_loss: 0.0912 - val_tp: 71.0000 - val_fp: 7.0000 - val_tn: 66.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9514 - val_precision: 0.9103 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0164 - tp: 456.0855 - fp: 2.8376 - tn: 483.3077 - fn: 1.6325 - accuracy: 0.9960 - precision: 0.9944 - recall: 0.9972 - auc: 0.9994 - val_loss: 0.0102 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9865 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0041 - tp: 464.0513 - fp: 0.7094 - tn: 478.2308 - fn: 0.8718 - accuracy: 0.9984 - precision: 0.9987 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0451 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 13s 89ms/step - loss: 1.3950 - tp: 424.9573 - fp: 41.0256 - tn: 430.7607 - fn: 47.1197 - accuracy: 0.8480 - precision: 0.8599 - recall: 0.8449 - auc: 0.8753 - val_loss: 0.4173 - val_tp: 54.0000 - val_fp: 12.0000 - val_tn: 61.0000 - val_fn: 17.0000 - val_accuracy: 0.7986 - val_precision: 0.8182 - val_recall: 0.7606 - val_auc: 0.8499\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0129 - tp: 471.3333 - fp: 1.9402 - tn: 468.8291 - fn: 1.7607 - accuracy: 0.9973 - precision: 0.9971 - recall: 0.9976 - auc: 0.9995 - val_loss: 1.5244 - val_tp: 73.0000 - val_fp: 52.0000 - val_tn: 19.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6389 - val_precision: 0.5840 - val_recall: 1.0000 - val_auc: 0.8262\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0238 - tp: 474.7778 - fp: 1.8462 - tn: 465.8547 - fn: 1.3846 - accuracy: 0.9958 - precision: 0.9945 - recall: 0.9970 - auc: 0.9993 - val_loss: 0.3953 - val_tp: 71.0000 - val_fp: 19.0000 - val_tn: 54.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8681 - val_precision: 0.7889 - val_recall: 1.0000 - val_auc: 0.9751\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0056 - tp: 471.8205 - fp: 1.4103 - tn: 469.8462 - fn: 0.7863 - accuracy: 0.9981 - precision: 0.9975 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0023 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0125 - tp: 464.2564 - fp: 1.2051 - tn: 476.7265 - fn: 1.6752 - accuracy: 0.9968 - precision: 0.9977 - recall: 0.9957 - auc: 0.9996 - val_loss: 2.1277 - val_tp: 26.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 45.0000 - val_accuracy: 0.6875 - val_precision: 1.0000 - val_recall: 0.3662 - val_auc: 0.8431\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0049 - tp: 459.4444 - fp: 0.3932 - tn: 483.2137 - fn: 0.8120 - accuracy: 0.9990 - precision: 0.9995 - recall: 0.9985 - auc: 1.0000 - val_loss: 3.6891e-04 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 6.0986e-05 - tp: 470.0000 - fp: 0.0000e+00 - tn: 473.8632 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.7020e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 6.5300e-05 - tp: 479.6752 - fp: 0.0000e+00 - tn: 464.1880 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.1391e-05 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 7.2010e-05 - tp: 477.7778 - fp: 0.0000e+00 - tn: 466.0855 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.8096e-05 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 3.1390e-05 - tp: 473.7521 - fp: 0.0000e+00 - tn: 470.1111 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.8548e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0028 - tp: 469.3675 - fp: 0.8291 - tn: 472.7436 - fn: 0.9231 - accuracy: 0.9977 - precision: 0.9982 - recall: 0.9974 - auc: 1.0000 - val_loss: 1.8109e-05 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 1.5204e-05 - tp: 448.6496 - fp: 0.0000e+00 - tn: 495.2137 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0185e-05 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 2.8576e-05 - tp: 467.7436 - fp: 0.0000e+00 - tn: 476.1197 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.9770e-06 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 9.2535e-06 - tp: 476.7692 - fp: 0.0000e+00 - tn: 467.0940 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.0121e-06 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 3.2553e-05 - tp: 472.0427 - fp: 0.0000e+00 - tn: 471.8205 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.9627e-06 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 15s 109ms/step - loss: 2.0105 - tp: 414.2906 - fp: 54.4872 - tn: 420.0427 - fn: 55.0427 - accuracy: 0.8252 - precision: 0.8303 - recall: 0.8315 - auc: 0.8561 - val_loss: 1.1505 - val_tp: 69.0000 - val_fp: 75.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4792 - val_recall: 1.0000 - val_auc: 0.7193\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0197 - tp: 468.0769 - fp: 4.3333 - tn: 468.2051 - fn: 3.2479 - accuracy: 0.9909 - precision: 0.9887 - recall: 0.9928 - auc: 0.9997 - val_loss: 1.0828 - val_tp: 75.0000 - val_fp: 69.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5208 - val_precision: 0.5208 - val_recall: 1.0000 - val_auc: 0.8973\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0048 - tp: 467.0513 - fp: 1.6496 - tn: 475.1624 - fn: 0.0000e+00 - accuracy: 0.9981 - precision: 0.9963 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.5615 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 11s 98ms/step - loss: 0.0241 - tp: 462.0684 - fp: 5.6752 - tn: 469.9915 - fn: 6.1282 - accuracy: 0.9904 - precision: 0.9907 - recall: 0.9900 - auc: 0.9996 - val_loss: 0.1521 - val_tp: 73.0000 - val_fp: 9.0000 - val_tn: 62.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9375 - val_precision: 0.8902 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0024 - tp: 469.7778 - fp: 0.9658 - tn: 472.8462 - fn: 0.2735 - accuracy: 0.9991 - precision: 0.9984 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.1085 - val_tp: 74.0000 - val_fp: 6.0000 - val_tn: 64.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9583 - val_precision: 0.9250 - val_recall: 1.0000 - val_auc: 0.9928\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0038 - tp: 457.8632 - fp: 0.6496 - tn: 484.1282 - fn: 1.2222 - accuracy: 0.9984 - precision: 0.9990 - recall: 0.9977 - auc: 1.0000 - val_loss: 8.3210e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0019 - tp: 470.6154 - fp: 0.2479 - tn: 472.0256 - fn: 0.9744 - accuracy: 0.9987 - precision: 0.9997 - recall: 0.9977 - auc: 1.0000 - val_loss: 0.0140 - val_tp: 75.0000 - val_fp: 1.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9868 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0022 - tp: 462.9060 - fp: 0.0000e+00 - tn: 480.9573 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0082e-05 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 99ms/step - loss: 6.1305e-05 - tp: 461.3248 - fp: 0.0000e+00 - tn: 482.5385 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.6141e-06 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 1.1888e-04 - tp: 468.9145 - fp: 0.0000e+00 - tn: 474.9487 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.8094e-06 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 7.9611e-05 - tp: 464.7692 - fp: 0.0000e+00 - tn: 479.0940 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.2644e-06 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 5.2243e-05 - tp: 468.8205 - fp: 0.0000e+00 - tn: 475.0427 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.0588e-06 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 99ms/step - loss: 1.9890e-04 - tp: 464.3077 - fp: 0.0000e+00 - tn: 479.5556 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.4731e-06 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 8.3811e-05 - tp: 470.6068 - fp: 0.0000e+00 - tn: 473.2564 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.2855e-06 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 12s 99ms/step - loss: 3.1547e-05 - tp: 463.1966 - fp: 0.0000e+00 - tn: 480.6667 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.5445e-06 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "FOLD 8\n",
      "{0: 0.994092373791622, 1: 1.0059782608695653}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 78s 585ms/step - loss: 1.0437 - tp: 410.0855 - fp: 57.1111 - tn: 414.5641 - fn: 62.1026 - accuracy: 0.8030 - precision: 0.8016 - recall: 0.7898 - auc: 0.8415 - val_loss: 1.6366 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.5812\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0168 - tp: 466.8291 - fp: 3.1026 - tn: 470.9915 - fn: 2.9402 - accuracy: 0.9935 - precision: 0.9940 - recall: 0.9930 - auc: 0.9996 - val_loss: 3.4523 - val_tp: 69.0000 - val_fp: 75.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4792 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0373 - tp: 470.6581 - fp: 4.2222 - tn: 463.2906 - fn: 5.6923 - accuracy: 0.9902 - precision: 0.9919 - recall: 0.9887 - auc: 0.9977 - val_loss: 7.8720 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0101 - tp: 479.5470 - fp: 2.1368 - tn: 461.0000 - fn: 1.1795 - accuracy: 0.9961 - precision: 0.9938 - recall: 0.9985 - auc: 0.9998 - val_loss: 0.3439 - val_tp: 73.0000 - val_fp: 10.0000 - val_tn: 61.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9306 - val_precision: 0.8795 - val_recall: 1.0000 - val_auc: 0.9718\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 0.0089 - tp: 473.7009 - fp: 1.4530 - tn: 466.7863 - fn: 1.9231 - accuracy: 0.9957 - precision: 0.9978 - recall: 0.9941 - auc: 1.0000 - val_loss: 1.0855 - val_tp: 72.0000 - val_fp: 29.0000 - val_tn: 43.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7986 - val_precision: 0.7129 - val_recall: 1.0000 - val_auc: 0.9097\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0021 - tp: 464.9060 - fp: 0.7436 - tn: 477.6410 - fn: 0.5726 - accuracy: 0.9989 - precision: 0.9988 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0696 - val_tp: 70.0000 - val_fp: 2.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9722 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 1.3787e-04 - tp: 469.0598 - fp: 0.2393 - tn: 474.5641 - fn: 0.0000e+00 - accuracy: 0.9999 - precision: 0.9997 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0707 - val_tp: 70.0000 - val_fp: 2.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9722 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 3.2701e-04 - tp: 469.1624 - fp: 0.0000e+00 - tn: 474.7009 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0713 - val_tp: 77.0000 - val_fp: 1.0000 - val_tn: 66.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9872 - val_recall: 1.0000 - val_auc: 0.9925\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.9725e-04 - tp: 476.6496 - fp: 0.0000e+00 - tn: 467.2137 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0733 - val_tp: 74.0000 - val_fp: 1.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9867 - val_recall: 1.0000 - val_auc: 0.9929\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 2.4427e-05 - tp: 470.3077 - fp: 0.0000e+00 - tn: 473.5556 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0719 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.7520e-04 - tp: 461.7778 - fp: 0.0000e+00 - tn: 482.0855 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0779 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 0.9930\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 3.5989e-05 - tp: 463.8632 - fp: 0.0000e+00 - tn: 480.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0845 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 0.9930\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 5.4017e-05 - tp: 463.4786 - fp: 0.0000e+00 - tn: 480.3846 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0551 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 2.9208e-05 - tp: 461.4103 - fp: 0.0000e+00 - tn: 482.4530 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0238 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.4489e-05 - tp: 466.3846 - fp: 0.0000e+00 - tn: 477.4786 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0342 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 85ms/step - loss: 1.7417 - tp: 377.6325 - fp: 102.1111 - tn: 366.0085 - fn: 98.1111 - accuracy: 0.7247 - precision: 0.7253 - recall: 0.7352 - auc: 0.7589 - val_loss: 2.3908 - val_tp: 69.0000 - val_fp: 75.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4792 - val_recall: 1.0000 - val_auc: 0.5194\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.1143 - tp: 438.1282 - fp: 22.2393 - tn: 465.9744 - fn: 17.5214 - accuracy: 0.9526 - precision: 0.9420 - recall: 0.9626 - auc: 0.9937 - val_loss: 3.0258 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.5585\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0728 - tp: 456.7350 - fp: 12.1880 - tn: 461.1197 - fn: 13.8205 - accuracy: 0.9710 - precision: 0.9744 - recall: 0.9665 - auc: 0.9970 - val_loss: 1.7298 - val_tp: 73.0000 - val_fp: 56.0000 - val_tn: 15.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6111 - val_precision: 0.5659 - val_recall: 1.0000 - val_auc: 0.8592\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0529 - tp: 462.9915 - fp: 10.4615 - tn: 462.7179 - fn: 7.6923 - accuracy: 0.9772 - precision: 0.9739 - recall: 0.9830 - auc: 0.9978 - val_loss: 0.1564 - val_tp: 66.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 6.0000 - val_accuracy: 0.9583 - val_precision: 1.0000 - val_recall: 0.9167 - val_auc: 0.9909\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0273 - tp: 460.5043 - fp: 5.5385 - tn: 472.8547 - fn: 4.9658 - accuracy: 0.9890 - precision: 0.9866 - recall: 0.9910 - auc: 0.9995 - val_loss: 1.3825 - val_tp: 43.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 28.0000 - val_accuracy: 0.8056 - val_precision: 1.0000 - val_recall: 0.6056 - val_auc: 0.8944\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0229 - tp: 469.1880 - fp: 3.3932 - tn: 467.5299 - fn: 3.7521 - accuracy: 0.9920 - precision: 0.9919 - recall: 0.9922 - auc: 0.9990 - val_loss: 0.1121 - val_tp: 70.0000 - val_fp: 3.0000 - val_tn: 70.0000 - val_fn: 1.0000 - val_accuracy: 0.9722 - val_precision: 0.9589 - val_recall: 0.9859 - val_auc: 0.9911\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0116 - tp: 469.3504 - fp: 1.7607 - tn: 471.5812 - fn: 1.1709 - accuracy: 0.9972 - precision: 0.9961 - recall: 0.9983 - auc: 0.9998 - val_loss: 0.1322 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 4.0000 - val_accuracy: 0.9722 - val_precision: 1.0000 - val_recall: 0.9444 - val_auc: 0.9926\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0234 - tp: 470.5470 - fp: 4.1111 - tn: 465.8632 - fn: 3.3419 - accuracy: 0.9927 - precision: 0.9924 - recall: 0.9933 - auc: 0.9985 - val_loss: 0.2835 - val_tp: 67.0000 - val_fp: 6.0000 - val_tn: 68.0000 - val_fn: 3.0000 - val_accuracy: 0.9375 - val_precision: 0.9178 - val_recall: 0.9571 - val_auc: 0.9701\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0110 - tp: 456.1282 - fp: 3.7179 - tn: 483.4444 - fn: 0.5726 - accuracy: 0.9936 - precision: 0.9879 - recall: 0.9993 - auc: 0.9999 - val_loss: 1.0805 - val_tp: 41.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 28.0000 - val_accuracy: 0.8056 - val_precision: 1.0000 - val_recall: 0.5942 - val_auc: 0.9348\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0056 - tp: 472.5385 - fp: 0.6068 - tn: 470.1197 - fn: 0.5983 - accuracy: 0.9992 - precision: 0.9992 - recall: 0.9992 - auc: 0.9998 - val_loss: 0.1732 - val_tp: 65.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 5.0000 - val_accuracy: 0.9653 - val_precision: 1.0000 - val_recall: 0.9286 - val_auc: 0.9773\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0218 - tp: 459.6752 - fp: 4.1709 - tn: 475.3932 - fn: 4.6239 - accuracy: 0.9912 - precision: 0.9905 - recall: 0.9917 - auc: 0.9996 - val_loss: 3.6426 - val_tp: 16.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 53.0000 - val_accuracy: 0.6319 - val_precision: 1.0000 - val_recall: 0.2319 - val_auc: 0.7319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0139 - tp: 459.9829 - fp: 2.6410 - tn: 477.9060 - fn: 3.3333 - accuracy: 0.9952 - precision: 0.9957 - recall: 0.9945 - auc: 0.9997 - val_loss: 5.2888 - val_tp: 8.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 64.0000 - val_accuracy: 0.5556 - val_precision: 1.0000 - val_recall: 0.1111 - val_auc: 0.6736\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0224 - tp: 462.0855 - fp: 4.6239 - tn: 473.1111 - fn: 4.0427 - accuracy: 0.9913 - precision: 0.9903 - recall: 0.9922 - auc: 0.9997 - val_loss: 0.3874 - val_tp: 62.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 8.0000 - val_accuracy: 0.9444 - val_precision: 1.0000 - val_recall: 0.8857 - val_auc: 0.9714\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0156 - tp: 463.4872 - fp: 0.8547 - tn: 476.8376 - fn: 2.6838 - accuracy: 0.9949 - precision: 0.9979 - recall: 0.9918 - auc: 0.9998 - val_loss: 0.4140 - val_tp: 66.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 8.0000 - val_accuracy: 0.9444 - val_precision: 1.0000 - val_recall: 0.8919 - val_auc: 0.9662\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0013 - tp: 468.4103 - fp: 0.5897 - tn: 474.4274 - fn: 0.4359 - accuracy: 0.9994 - precision: 0.9993 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.2086 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 6.0000 - val_accuracy: 0.9583 - val_precision: 1.0000 - val_recall: 0.9178 - val_auc: 0.9726\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 89ms/step - loss: 1.1328 - tp: 413.6667 - fp: 41.9231 - tn: 435.8291 - fn: 52.4444 - accuracy: 0.8414 - precision: 0.8488 - recall: 0.8233 - auc: 0.8809 - val_loss: 0.6500 - val_tp: 19.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 54.0000 - val_accuracy: 0.6250 - val_precision: 1.0000 - val_recall: 0.2603 - val_auc: 0.9495\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0175 - tp: 462.0769 - fp: 2.4188 - tn: 478.8120 - fn: 0.5556 - accuracy: 0.9967 - precision: 0.9938 - recall: 0.9993 - auc: 0.9995 - val_loss: 0.1268 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9861 - val_recall: 0.9861 - val_auc: 0.9998\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0065 - tp: 471.8547 - fp: 1.6239 - tn: 470.1026 - fn: 0.2821 - accuracy: 0.9982 - precision: 0.9968 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0115 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0510 - tp: 464.4615 - fp: 3.9487 - tn: 470.8803 - fn: 4.5726 - accuracy: 0.9880 - precision: 0.9889 - recall: 0.9871 - auc: 0.9967 - val_loss: 0.4995 - val_tp: 55.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 14.0000 - val_accuracy: 0.9028 - val_precision: 1.0000 - val_recall: 0.7971 - val_auc: 0.9783\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0070 - tp: 467.1880 - fp: 0.5897 - tn: 474.1709 - fn: 1.9145 - accuracy: 0.9973 - precision: 0.9990 - recall: 0.9956 - auc: 1.0000 - val_loss: 0.0553 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9730 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0019 - tp: 479.4444 - fp: 0.0000e+00 - tn: 463.5385 - fn: 0.8803 - accuracy: 0.9989 - precision: 1.0000 - recall: 0.9978 - auc: 1.0000 - val_loss: 8.6118e-04 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 6.3890e-04 - tp: 475.7094 - fp: 0.0000e+00 - tn: 468.1538 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.2193e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 1.0611e-04 - tp: 476.4957 - fp: 0.0000e+00 - tn: 467.3675 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0015 - val_tp: 76.0000 - val_fp: 0.0000e+00 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 1.0641e-04 - tp: 474.3761 - fp: 0.0000e+00 - tn: 469.4872 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0013 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 5.3775e-05 - tp: 467.4017 - fp: 0.0000e+00 - tn: 476.4615 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0014 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 3.4518e-05 - tp: 476.4701 - fp: 0.0000e+00 - tn: 467.3932 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0017 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 3.8788e-05 - tp: 470.7778 - fp: 0.0000e+00 - tn: 473.0855 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0013 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 3.0284e-05 - tp: 475.5043 - fp: 0.0000e+00 - tn: 468.3590 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0013 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 2.2518e-05 - tp: 471.1197 - fp: 0.0000e+00 - tn: 472.7436 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0035 - val_tp: 76.0000 - val_fp: 0.0000e+00 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 1.3302e-05 - tp: 467.5385 - fp: 0.0000e+00 - tn: 476.3248 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0026 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 77.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 109ms/step - loss: 2.1510 - tp: 406.3248 - fp: 64.5812 - tn: 404.5641 - fn: 68.3932 - accuracy: 0.7937 - precision: 0.7976 - recall: 0.7883 - auc: 0.8309 - val_loss: 0.9065 - val_tp: 69.0000 - val_fp: 75.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4792 - val_recall: 1.0000 - val_auc: 0.9182\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0125 - tp: 457.8889 - fp: 2.1624 - tn: 481.6838 - fn: 2.1282 - accuracy: 0.9964 - precision: 0.9960 - recall: 0.9965 - auc: 0.9997 - val_loss: 1.7619 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.9746\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 12s 99ms/step - loss: 0.0059 - tp: 462.4359 - fp: 0.6154 - tn: 479.1111 - fn: 1.7009 - accuracy: 0.9969 - precision: 0.9991 - recall: 0.9945 - auc: 1.0000 - val_loss: 1.7581 - val_tp: 74.0000 - val_fp: 67.0000 - val_tn: 3.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5347 - val_precision: 0.5248 - val_recall: 1.0000 - val_auc: 0.8786\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0234 - tp: 459.5299 - fp: 3.6581 - tn: 477.7179 - fn: 2.9573 - accuracy: 0.9929 - precision: 0.9911 - recall: 0.9942 - auc: 0.9993 - val_loss: 0.1068 - val_tp: 74.0000 - val_fp: 8.0000 - val_tn: 62.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9444 - val_precision: 0.9024 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 5.4291e-04 - tp: 468.1453 - fp: 0.0000e+00 - tn: 475.7179 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0010 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 3.0056e-05 - tp: 475.5897 - fp: 0.0000e+00 - tn: 468.2735 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.3591e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 1.0043e-05 - tp: 472.0427 - fp: 0.0000e+00 - tn: 471.8205 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.8553e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 1.4448e-04 - tp: 471.7863 - fp: 0.0000e+00 - tn: 472.0769 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.4245e-06 - val_tp: 76.0000 - val_fp: 0.0000e+00 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 2.3129e-04 - tp: 481.1026 - fp: 0.0000e+00 - tn: 462.7607 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.6555e-05 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 3.4251e-06 - tp: 472.1026 - fp: 0.0000e+00 - tn: 471.7607 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.5400e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 3.5996e-05 - tp: 464.9829 - fp: 0.0000e+00 - tn: 478.8803 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.3074e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0178 - tp: 460.2564 - fp: 2.6410 - tn: 478.0427 - fn: 2.9231 - accuracy: 0.9966 - precision: 0.9967 - recall: 0.9964 - auc: 0.9988 - val_loss: 0.0735 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9726 - val_auc: 0.9932\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0214 - tp: 461.6838 - fp: 2.0427 - tn: 478.2308 - fn: 1.9060 - accuracy: 0.9972 - precision: 0.9970 - recall: 0.9972 - auc: 0.9979 - val_loss: 7.6194e-06 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0208 - tp: 464.0342 - fp: 0.9487 - tn: 477.9231 - fn: 0.9573 - accuracy: 0.9980 - precision: 0.9983 - recall: 0.9976 - auc: 0.9987 - val_loss: 1.1037e-06 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0034 - tp: 474.7265 - fp: 0.0000e+00 - tn: 468.5299 - fn: 0.6068 - accuracy: 0.9995 - precision: 1.0000 - recall: 0.9990 - auc: 0.9995 - val_loss: 1.3636e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "FOLD 9\n",
      "{0: 0.994092373791622, 1: 1.0059782608695653}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 81s 605ms/step - loss: 2.0005 - tp: 385.2393 - fp: 83.5897 - tn: 392.4786 - fn: 82.5556 - accuracy: 0.7414 - precision: 0.7332 - recall: 0.7346 - auc: 0.7709 - val_loss: 2.8040 - val_tp: 75.0000 - val_fp: 69.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5208 - val_precision: 0.5208 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 97ms/step - loss: 0.0144 - tp: 474.7094 - fp: 1.5043 - tn: 466.7009 - fn: 0.9487 - accuracy: 0.9973 - precision: 0.9977 - recall: 0.9969 - auc: 0.9996 - val_loss: 6.5328 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0073 - tp: 464.7009 - fp: 1.6496 - tn: 477.1197 - fn: 0.3932 - accuracy: 0.9985 - precision: 0.9974 - recall: 0.9995 - auc: 1.0000 - val_loss: 6.9293 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0026 - tp: 479.5556 - fp: 0.1197 - tn: 464.1880 - fn: 0.0000e+00 - accuracy: 0.9999 - precision: 0.9999 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.7385 - val_tp: 68.0000 - val_fp: 75.0000 - val_tn: 1.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4755 - val_recall: 1.0000 - val_auc: 0.6250\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0050 - tp: 479.8034 - fp: 0.0000e+00 - tn: 463.6581 - fn: 0.4017 - accuracy: 0.9997 - precision: 1.0000 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.6113 - val_tp: 71.0000 - val_fp: 29.0000 - val_tn: 44.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7986 - val_precision: 0.7100 - val_recall: 1.0000 - val_auc: 0.9795\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 7s 63ms/step - loss: 3.2879e-04 - tp: 461.6838 - fp: 0.0000e+00 - tn: 482.1795 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0189 - val_tp: 76.0000 - val_fp: 1.0000 - val_tn: 67.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9870 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 4.6366e-04 - tp: 476.9145 - fp: 0.0000e+00 - tn: 466.9487 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0016 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 1.8914e-04 - tp: 470.0513 - fp: 0.0000e+00 - tn: 473.8120 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0019 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.0749e-04 - tp: 478.1624 - fp: 0.0000e+00 - tn: 465.7009 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.6026e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.4321e-04 - tp: 483.8632 - fp: 0.0000e+00 - tn: 460.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.6270e-04 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 77.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 2.2519e-04 - tp: 475.5214 - fp: 0.0000e+00 - tn: 468.3419 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.5005e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0029 - tp: 453.3761 - fp: 1.3077 - tn: 488.7094 - fn: 0.4701 - accuracy: 0.9988 - precision: 0.9980 - recall: 0.9994 - auc: 1.0000 - val_loss: 5.3608 - val_tp: 7.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 65.0000 - val_accuracy: 0.5486 - val_precision: 1.0000 - val_recall: 0.0972 - val_auc: 0.5972\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0136 - tp: 461.4615 - fp: 1.2991 - tn: 479.3333 - fn: 1.7692 - accuracy: 0.9972 - precision: 0.9974 - recall: 0.9968 - auc: 0.9989 - val_loss: 0.1049 - val_tp: 69.0000 - val_fp: 3.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9583 - val_recall: 1.0000 - val_auc: 0.9933\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 2.6416e-04 - tp: 470.4615 - fp: 0.0000e+00 - tn: 473.4017 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0545 - val_tp: 73.0000 - val_fp: 2.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9733 - val_recall: 1.0000 - val_auc: 0.9930\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.1689e-04 - tp: 465.2137 - fp: 0.0000e+00 - tn: 478.6496 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.3814e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 86ms/step - loss: 2.0452 - tp: 344.4615 - fp: 128.8718 - tn: 345.0000 - fn: 125.5299 - accuracy: 0.6636 - precision: 0.6531 - recall: 0.6625 - auc: 0.7065 - val_loss: 1.0530 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.4683\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.1023 - tp: 453.9744 - fp: 17.5812 - tn: 457.0513 - fn: 15.2564 - accuracy: 0.9669 - precision: 0.9658 - recall: 0.9672 - auc: 0.9923 - val_loss: 1.4267 - val_tp: 69.0000 - val_fp: 75.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4792 - val_recall: 1.0000 - val_auc: 0.7667\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.1121 - tp: 453.0855 - fp: 19.0598 - tn: 454.8376 - fn: 16.8803 - accuracy: 0.9557 - precision: 0.9514 - recall: 0.9626 - auc: 0.9930 - val_loss: 0.3522 - val_tp: 72.0000 - val_fp: 6.0000 - val_tn: 66.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9583 - val_precision: 0.9231 - val_recall: 1.0000 - val_auc: 0.9809\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0633 - tp: 463.5128 - fp: 10.7863 - tn: 456.0000 - fn: 13.5641 - accuracy: 0.9758 - precision: 0.9804 - recall: 0.9721 - auc: 0.9967 - val_loss: 0.2110 - val_tp: 62.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 11.0000 - val_accuracy: 0.9167 - val_precision: 0.9841 - val_recall: 0.8493 - val_auc: 0.9871\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0424 - tp: 466.6410 - fp: 5.8376 - tn: 465.4359 - fn: 5.9487 - accuracy: 0.9868 - precision: 0.9891 - recall: 0.9843 - auc: 0.9987 - val_loss: 0.0635 - val_tp: 75.0000 - val_fp: 1.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9868 - val_recall: 1.0000 - val_auc: 0.9988\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0165 - tp: 461.9316 - fp: 2.3333 - tn: 477.3846 - fn: 2.2137 - accuracy: 0.9932 - precision: 0.9922 - recall: 0.9934 - auc: 0.9998 - val_loss: 0.1003 - val_tp: 68.0000 - val_fp: 5.0000 - val_tn: 69.0000 - val_fn: 2.0000 - val_accuracy: 0.9514 - val_precision: 0.9315 - val_recall: 0.9714 - val_auc: 0.9965\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0088 - tp: 475.1368 - fp: 1.7778 - tn: 465.4786 - fn: 1.4701 - accuracy: 0.9975 - precision: 0.9974 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0227 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9857 - val_recall: 0.9857 - val_auc: 0.9998\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0353 - tp: 454.1026 - fp: 4.7265 - tn: 480.1111 - fn: 4.9231 - accuracy: 0.9887 - precision: 0.9876 - recall: 0.9889 - auc: 0.9995 - val_loss: 0.0285 - val_tp: 76.0000 - val_fp: 0.0000e+00 - val_tn: 67.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9870 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0020 - tp: 475.8547 - fp: 0.5726 - tn: 466.7778 - fn: 0.6581 - accuracy: 0.9992 - precision: 0.9993 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.1041 - val_tp: 61.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 8.0000 - val_accuracy: 0.9444 - val_precision: 1.0000 - val_recall: 0.8841 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0114 - tp: 478.1368 - fp: 1.3248 - tn: 462.2308 - fn: 2.1709 - accuracy: 0.9947 - precision: 0.9974 - recall: 0.9922 - auc: 0.9999 - val_loss: 0.0688 - val_tp: 72.0000 - val_fp: 3.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9600 - val_recall: 1.0000 - val_auc: 0.9994\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0012 - tp: 465.9231 - fp: 0.0000e+00 - tn: 477.9402 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0279 - val_tp: 72.0000 - val_fp: 3.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9600 - val_recall: 1.0000 - val_auc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 2.3744e-04 - tp: 481.6068 - fp: 0.0000e+00 - tn: 462.2564 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0200 - val_tp: 72.0000 - val_fp: 2.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9730 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 7.6003e-04 - tp: 465.7863 - fp: 0.2564 - tn: 477.4957 - fn: 0.3248 - accuracy: 0.9997 - precision: 0.9997 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.1508 - val_tp: 61.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 11.0000 - val_accuracy: 0.9236 - val_precision: 1.0000 - val_recall: 0.8472 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0681 - tp: 442.4359 - fp: 10.5214 - tn: 476.4444 - fn: 14.4615 - accuracy: 0.9778 - precision: 0.9810 - recall: 0.9729 - auc: 0.9957 - val_loss: 0.0376 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 0.9995\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0274 - tp: 469.5043 - fp: 4.9145 - tn: 464.8974 - fn: 4.5470 - accuracy: 0.9879 - precision: 0.9877 - recall: 0.9886 - auc: 0.9996 - val_loss: 0.0431 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9863 - val_recall: 0.9863 - val_auc: 0.9998\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 90ms/step - loss: 1.1135 - tp: 410.1197 - fp: 53.2735 - tn: 416.3590 - fn: 64.1111 - accuracy: 0.8185 - precision: 0.8265 - recall: 0.8034 - auc: 0.8629 - val_loss: 0.5691 - val_tp: 22.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 49.0000 - val_accuracy: 0.6597 - val_precision: 1.0000 - val_recall: 0.3099 - val_auc: 0.8918\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0222 - tp: 463.2735 - fp: 3.3162 - tn: 476.1368 - fn: 1.1368 - accuracy: 0.9961 - precision: 0.9939 - recall: 0.9982 - auc: 0.9985 - val_loss: 0.3204 - val_tp: 62.0000 - val_fp: 4.0000 - val_tn: 68.0000 - val_fn: 10.0000 - val_accuracy: 0.9028 - val_precision: 0.9394 - val_recall: 0.8611 - val_auc: 0.9255\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0224 - tp: 464.4444 - fp: 2.8120 - tn: 474.4701 - fn: 2.1368 - accuracy: 0.9945 - precision: 0.9936 - recall: 0.9953 - auc: 0.9981 - val_loss: 0.3433 - val_tp: 59.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 10.0000 - val_accuracy: 0.9236 - val_precision: 0.9833 - val_recall: 0.8551 - val_auc: 0.9444\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0098 - tp: 465.2137 - fp: 0.8889 - tn: 477.7607 - fn: 0.0000e+00 - accuracy: 0.9989 - precision: 0.9979 - recall: 1.0000 - auc: 0.9989 - val_loss: 0.2827 - val_tp: 64.0000 - val_fp: 1.0000 - val_tn: 68.0000 - val_fn: 11.0000 - val_accuracy: 0.9167 - val_precision: 0.9846 - val_recall: 0.8533 - val_auc: 0.9887\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0076 - tp: 469.6581 - fp: 0.7265 - tn: 471.9915 - fn: 1.4872 - accuracy: 0.9978 - precision: 0.9986 - recall: 0.9970 - auc: 1.0000 - val_loss: 0.2433 - val_tp: 59.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 10.0000 - val_accuracy: 0.9306 - val_precision: 1.0000 - val_recall: 0.8551 - val_auc: 0.9840\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0109 - tp: 461.8718 - fp: 1.0342 - tn: 480.9573 - fn: 0.0000e+00 - accuracy: 0.9989 - precision: 0.9976 - recall: 1.0000 - auc: 0.9989 - val_loss: 1.0431 - val_tp: 61.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 13.0000 - val_accuracy: 0.9097 - val_precision: 1.0000 - val_recall: 0.8243 - val_auc: 0.9122\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0118 - tp: 478.5812 - fp: 1.7949 - tn: 463.2735 - fn: 0.2137 - accuracy: 0.9981 - precision: 0.9965 - recall: 0.9997 - auc: 0.9987 - val_loss: 0.0237 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0035 - tp: 473.4957 - fp: 0.4017 - tn: 469.3761 - fn: 0.5897 - accuracy: 0.9993 - precision: 0.9995 - recall: 0.9991 - auc: 0.9997 - val_loss: 0.0312 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9861 - val_recall: 0.9861 - val_auc: 0.9996\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0046 - tp: 479.4359 - fp: 1.3077 - tn: 462.5043 - fn: 0.6154 - accuracy: 0.9982 - precision: 0.9975 - recall: 0.9990 - auc: 1.0000 - val_loss: 0.0635 - val_tp: 66.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 5.0000 - val_accuracy: 0.9653 - val_precision: 1.0000 - val_recall: 0.9296 - val_auc: 0.9996\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 2.2167e-04 - tp: 461.9231 - fp: 0.0000e+00 - tn: 481.9402 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0267 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9718 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 6.2843e-05 - tp: 454.5470 - fp: 0.0000e+00 - tn: 489.3162 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0233 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9861 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 5.3038e-05 - tp: 476.9829 - fp: 0.0000e+00 - tn: 466.8803 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0191 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9861 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 3.6339e-05 - tp: 477.1966 - fp: 0.0000e+00 - tn: 466.6667 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0534 - val_tp: 65.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9559 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 1.5919e-04 - tp: 480.5897 - fp: 0.0000e+00 - tn: 463.2735 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0181 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9714 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 9.0858e-05 - tp: 465.7009 - fp: 0.0000e+00 - tn: 478.1624 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0082 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 109ms/step - loss: 0.9675 - tp: 408.7521 - fp: 54.1795 - tn: 427.8803 - fn: 53.0513 - accuracy: 0.8277 - precision: 0.8268 - recall: 0.8276 - auc: 0.8804 - val_loss: 0.7421 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.9757\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0325 - tp: 468.1966 - fp: 2.5470 - tn: 469.3846 - fn: 3.7350 - accuracy: 0.9917 - precision: 0.9942 - recall: 0.9887 - auc: 0.9984 - val_loss: 4.3233 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.5070\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0428 - tp: 456.9658 - fp: 4.8889 - tn: 479.1966 - fn: 2.8120 - accuracy: 0.9881 - precision: 0.9832 - recall: 0.9926 - auc: 0.9979 - val_loss: 0.0366 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0073 - tp: 457.2479 - fp: 1.0684 - tn: 483.7265 - fn: 1.8205 - accuracy: 0.9971 - precision: 0.9985 - recall: 0.9952 - auc: 1.0000 - val_loss: 0.0020 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0020 - tp: 472.2735 - fp: 0.9573 - tn: 470.6325 - fn: 0.0000e+00 - accuracy: 0.9984 - precision: 0.9966 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.8426e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 2.7197e-04 - tp: 459.9915 - fp: 0.0171 - tn: 483.7265 - fn: 0.1282 - accuracy: 0.9999 - precision: 1.0000 - recall: 0.9999 - auc: 1.0000 - val_loss: 1.5250e-05 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0024 - tp: 479.7521 - fp: 0.4274 - tn: 463.0940 - fn: 0.5897 - accuracy: 0.9992 - precision: 0.9994 - recall: 0.9991 - auc: 1.0000 - val_loss: 5.6949e-05 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 5.7508e-04 - tp: 471.4103 - fp: 0.2222 - tn: 472.1111 - fn: 0.1197 - accuracy: 0.9998 - precision: 0.9997 - recall: 0.9999 - auc: 1.0000 - val_loss: 4.6484e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0301 - tp: 467.9658 - fp: 5.0769 - tn: 466.7436 - fn: 4.0769 - accuracy: 0.9913 - precision: 0.9901 - recall: 0.9924 - auc: 0.9985 - val_loss: 3.5112e-04 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0063 - tp: 461.6154 - fp: 0.3419 - tn: 481.4359 - fn: 0.4701 - accuracy: 0.9995 - precision: 0.9996 - recall: 0.9994 - auc: 0.9996 - val_loss: 2.0743e-12 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0056 - tp: 474.2735 - fp: 1.2137 - tn: 467.5983 - fn: 0.7778 - accuracy: 0.9986 - precision: 0.9981 - recall: 0.9990 - auc: 0.9998 - val_loss: 5.2863e-11 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0335 - tp: 480.9231 - fp: 2.2991 - tn: 459.6923 - fn: 0.9487 - accuracy: 0.9956 - precision: 0.9926 - recall: 0.9987 - auc: 0.9965 - val_loss: 4.3525 - val_tp: 26.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 47.0000 - val_accuracy: 0.6736 - val_precision: 1.0000 - val_recall: 0.3562 - val_auc: 0.7534\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 8.7896e-05 - tp: 465.8889 - fp: 0.0000e+00 - tn: 477.9744 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.9801e-05 - val_tp: 78.0000 - val_fp: 0.0000e+00 - val_tn: 66.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0026 - tp: 477.1282 - fp: 0.2821 - tn: 466.1624 - fn: 0.2906 - accuracy: 0.9997 - precision: 0.9996 - recall: 0.9997 - auc: 0.9998 - val_loss: 6.5863 - val_tp: 14.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 60.0000 - val_accuracy: 0.5833 - val_precision: 1.0000 - val_recall: 0.1892 - val_auc: 0.6486\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0305 - tp: 467.4701 - fp: 2.2906 - tn: 473.2051 - fn: 0.8974 - accuracy: 0.9955 - precision: 0.9924 - recall: 0.9985 - auc: 0.9984 - val_loss: 6.3866e-14 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "FOLD 10\n",
      "{0: 0.994092373791622, 1: 1.0059782608695653}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 82s 603ms/step - loss: 0.9843 - tp: 437.8462 - fp: 33.0427 - tn: 440.3846 - fn: 32.5897 - accuracy: 0.8857 - precision: 0.8752 - recall: 0.8735 - auc: 0.9148 - val_loss: 3.0233 - val_tp: 68.0000 - val_fp: 76.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4722 - val_precision: 0.4722 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 94ms/step - loss: 0.0127 - tp: 468.4359 - fp: 1.2393 - tn: 472.2479 - fn: 1.9402 - accuracy: 0.9967 - precision: 0.9974 - recall: 0.9960 - auc: 0.9997 - val_loss: 3.6359 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 8s 66ms/step - loss: 0.0202 - tp: 467.4786 - fp: 2.8889 - tn: 470.6667 - fn: 2.8291 - accuracy: 0.9936 - precision: 0.9942 - recall: 0.9929 - auc: 0.9998 - val_loss: 3.1376 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.5694\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0138 - tp: 471.6581 - fp: 1.9231 - tn: 468.3590 - fn: 1.9231 - accuracy: 0.9964 - precision: 0.9964 - recall: 0.9965 - auc: 0.9999 - val_loss: 0.0675 - val_tp: 74.0000 - val_fp: 4.0000 - val_tn: 66.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9722 - val_precision: 0.9487 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0110 - tp: 465.9231 - fp: 1.1538 - tn: 475.6496 - fn: 1.1368 - accuracy: 0.9971 - precision: 0.9974 - recall: 0.9967 - auc: 1.0000 - val_loss: 4.6845e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0020 - tp: 475.1966 - fp: 0.4701 - tn: 468.1966 - fn: 0.0000e+00 - accuracy: 0.9997 - precision: 0.9993 - recall: 1.0000 - auc: 1.0000 - val_loss: 9.9980e-04 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0014 - tp: 478.7692 - fp: 0.6581 - tn: 463.9060 - fn: 0.5299 - accuracy: 0.9990 - precision: 0.9989 - recall: 0.9992 - auc: 1.0000 - val_loss: 1.5365e-04 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 5.3153e-04 - tp: 455.8803 - fp: 0.0000e+00 - tn: 487.9829 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.1272e-05 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0013 - tp: 469.8547 - fp: 0.2735 - tn: 473.7350 - fn: 0.0000e+00 - accuracy: 0.9998 - precision: 0.9997 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.4163 - val_tp: 28.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 42.0000 - val_accuracy: 0.7083 - val_precision: 1.0000 - val_recall: 0.4000 - val_auc: 0.8786\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 2.3798e-04 - tp: 470.6752 - fp: 0.0000e+00 - tn: 473.1880 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0429 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9577 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0149 - tp: 462.7949 - fp: 3.5299 - tn: 475.2821 - fn: 2.2564 - accuracy: 0.9947 - precision: 0.9946 - recall: 0.9948 - auc: 0.9994 - val_loss: 0.5073 - val_tp: 61.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 12.0000 - val_accuracy: 0.9167 - val_precision: 1.0000 - val_recall: 0.8356 - val_auc: 0.9589\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0081 - tp: 471.8803 - fp: 2.8120 - tn: 467.8376 - fn: 1.3333 - accuracy: 0.9967 - precision: 0.9955 - recall: 0.9980 - auc: 0.9998 - val_loss: 0.5811 - val_tp: 54.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 19.0000 - val_accuracy: 0.8681 - val_precision: 1.0000 - val_recall: 0.7397 - val_auc: 0.9726\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0035 - tp: 461.5726 - fp: 0.7778 - tn: 480.9744 - fn: 0.5385 - accuracy: 0.9989 - precision: 0.9987 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.5892 - val_tp: 50.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 23.0000 - val_accuracy: 0.8403 - val_precision: 1.0000 - val_recall: 0.6849 - val_auc: 0.9726\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0037 - tp: 474.3419 - fp: 0.3846 - tn: 468.8034 - fn: 0.3333 - accuracy: 0.9995 - precision: 0.9995 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0191 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 3.0735e-04 - tp: 464.1111 - fp: 0.0000e+00 - tn: 479.7521 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0250 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9718 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 85ms/step - loss: 1.7555 - tp: 342.5385 - fp: 129.1453 - tn: 343.1709 - fn: 129.0085 - accuracy: 0.6821 - precision: 0.6827 - recall: 0.6801 - auc: 0.7080 - val_loss: 1.0932 - val_tp: 68.0000 - val_fp: 76.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4722 - val_precision: 0.4722 - val_recall: 1.0000 - val_auc: 0.4853\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.1139 - tp: 437.0940 - fp: 23.5385 - tn: 461.9744 - fn: 21.2564 - accuracy: 0.9553 - precision: 0.9536 - recall: 0.9534 - auc: 0.9922 - val_loss: 0.9913 - val_tp: 77.0000 - val_fp: 67.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5347 - val_precision: 0.5347 - val_recall: 1.0000 - val_auc: 0.6565\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0627 - tp: 466.7009 - fp: 10.2051 - tn: 455.7179 - fn: 11.2393 - accuracy: 0.9738 - precision: 0.9763 - recall: 0.9738 - auc: 0.9977 - val_loss: 0.7570 - val_tp: 72.0000 - val_fp: 51.0000 - val_tn: 21.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6458 - val_precision: 0.5854 - val_recall: 1.0000 - val_auc: 0.9858\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0632 - tp: 462.6752 - fp: 8.4359 - tn: 466.8889 - fn: 5.8632 - accuracy: 0.9782 - precision: 0.9742 - recall: 0.9850 - auc: 0.9965 - val_loss: 0.2361 - val_tp: 64.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 10.0000 - val_accuracy: 0.9306 - val_precision: 1.0000 - val_recall: 0.8649 - val_auc: 0.9786\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0358 - tp: 458.8034 - fp: 3.3675 - tn: 475.3077 - fn: 6.3846 - accuracy: 0.9894 - precision: 0.9938 - recall: 0.9844 - auc: 0.9971 - val_loss: 0.1699 - val_tp: 70.0000 - val_fp: 10.0000 - val_tn: 64.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9306 - val_precision: 0.8750 - val_recall: 1.0000 - val_auc: 0.9983\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0713 - tp: 458.8462 - fp: 11.3675 - tn: 463.4957 - fn: 10.1538 - accuracy: 0.9742 - precision: 0.9684 - recall: 0.9783 - auc: 0.9976 - val_loss: 0.0706 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9710 - val_auc: 0.9926\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0106 - tp: 476.7949 - fp: 0.9573 - tn: 465.9487 - fn: 0.1624 - accuracy: 0.9983 - precision: 0.9967 - recall: 0.9998 - auc: 0.9999 - val_loss: 0.0614 - val_tp: 75.0000 - val_fp: 2.0000 - val_tn: 67.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9740 - val_recall: 1.0000 - val_auc: 0.9996\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0025 - tp: 461.6496 - fp: 0.1709 - tn: 481.7692 - fn: 0.2735 - accuracy: 0.9997 - precision: 0.9998 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.6706 - val_tp: 50.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 23.0000 - val_accuracy: 0.8403 - val_precision: 1.0000 - val_recall: 0.6849 - val_auc: 0.9452\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0150 - tp: 462.6239 - fp: 2.8291 - tn: 476.4530 - fn: 1.9573 - accuracy: 0.9939 - precision: 0.9932 - recall: 0.9944 - auc: 0.9999 - val_loss: 0.0152 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0013 - tp: 484.6239 - fp: 0.0000e+00 - tn: 459.2393 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0229 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9710 - val_auc: 0.9996\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0018 - tp: 468.3590 - fp: 0.4530 - tn: 474.6581 - fn: 0.3932 - accuracy: 0.9995 - precision: 0.9995 - recall: 0.9996 - auc: 0.9999 - val_loss: 0.1593 - val_tp: 69.0000 - val_fp: 2.0000 - val_tn: 71.0000 - val_fn: 2.0000 - val_accuracy: 0.9722 - val_precision: 0.9718 - val_recall: 0.9718 - val_auc: 0.9916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0213 - tp: 462.7863 - fp: 3.2650 - tn: 474.5641 - fn: 3.2479 - accuracy: 0.9932 - precision: 0.9929 - recall: 0.9933 - auc: 0.9993 - val_loss: 0.0011 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0036 - tp: 471.9316 - fp: 0.6410 - tn: 470.8034 - fn: 0.4872 - accuracy: 0.9993 - precision: 0.9992 - recall: 0.9993 - auc: 1.0000 - val_loss: 0.0018 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0252 - tp: 461.8632 - fp: 6.0855 - tn: 472.0684 - fn: 3.8462 - accuracy: 0.9890 - precision: 0.9849 - recall: 0.9928 - auc: 0.9991 - val_loss: 0.1339 - val_tp: 66.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 6.0000 - val_accuracy: 0.9583 - val_precision: 1.0000 - val_recall: 0.9167 - val_auc: 0.9931\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0150 - tp: 468.4701 - fp: 2.6752 - tn: 470.4017 - fn: 2.3162 - accuracy: 0.9937 - precision: 0.9924 - recall: 0.9948 - auc: 0.9999 - val_loss: 0.0013 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 89ms/step - loss: 1.0540 - tp: 421.2137 - fp: 39.8974 - tn: 431.6154 - fn: 51.1368 - accuracy: 0.8413 - precision: 0.8555 - recall: 0.8307 - auc: 0.8851 - val_loss: 0.4181 - val_tp: 71.0000 - val_fp: 22.0000 - val_tn: 51.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8472 - val_precision: 0.7634 - val_recall: 1.0000 - val_auc: 0.9795\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0167 - tp: 470.2735 - fp: 3.3077 - tn: 468.9573 - fn: 1.3248 - accuracy: 0.9957 - precision: 0.9935 - recall: 0.9980 - auc: 0.9997 - val_loss: 1.6457 - val_tp: 70.0000 - val_fp: 72.0000 - val_tn: 2.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.4930 - val_recall: 1.0000 - val_auc: 0.9740\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0171 - tp: 473.5812 - fp: 1.6838 - tn: 465.3761 - fn: 3.2222 - accuracy: 0.9935 - precision: 0.9970 - recall: 0.9903 - auc: 0.9996 - val_loss: 0.5007 - val_tp: 74.0000 - val_fp: 23.0000 - val_tn: 47.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8403 - val_precision: 0.7629 - val_recall: 1.0000 - val_auc: 0.9643\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 7.6684e-04 - tp: 474.8034 - fp: 0.0000e+00 - tn: 469.0598 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0306 - val_tp: 74.0000 - val_fp: 1.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9867 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0110 - tp: 463.8205 - fp: 1.4103 - tn: 476.6239 - fn: 2.0085 - accuracy: 0.9964 - precision: 0.9971 - recall: 0.9956 - auc: 0.9993 - val_loss: 0.0066 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0163 - tp: 465.4274 - fp: 2.6410 - tn: 473.5385 - fn: 2.2564 - accuracy: 0.9958 - precision: 0.9954 - recall: 0.9961 - auc: 0.9988 - val_loss: 0.0171 - val_tp: 75.0000 - val_fp: 1.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9868 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0044 - tp: 465.8205 - fp: 0.7778 - tn: 475.7692 - fn: 1.4957 - accuracy: 0.9977 - precision: 0.9984 - recall: 0.9971 - auc: 1.0000 - val_loss: 1.6536e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 5.5997e-05 - tp: 474.6239 - fp: 0.0000e+00 - tn: 469.2393 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.7379e-06 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 3.2529e-05 - tp: 471.1282 - fp: 0.0000e+00 - tn: 472.7350 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0398e-06 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 1.6025e-05 - tp: 458.2991 - fp: 0.0000e+00 - tn: 485.5641 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.0300e-07 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 3.4455e-06 - tp: 468.0427 - fp: 0.0000e+00 - tn: 475.8205 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.5094e-07 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 10s 83ms/step - loss: 1.1401e-05 - tp: 470.6923 - fp: 0.0000e+00 - tn: 473.1709 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.8601e-07 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 1.1827e-05 - tp: 461.3333 - fp: 0.0000e+00 - tn: 482.5299 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.8373e-06 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 77.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 2.4434e-05 - tp: 468.1538 - fp: 0.0000e+00 - tn: 475.7094 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 9.9677e-07 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 7.8387e-06 - tp: 468.4701 - fp: 0.0000e+00 - tn: 475.3932 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.7538e-07 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 109ms/step - loss: 1.9790 - tp: 407.7350 - fp: 58.6496 - tn: 415.4017 - fn: 62.0769 - accuracy: 0.8141 - precision: 0.8195 - recall: 0.8110 - auc: 0.8671 - val_loss: 1.5820 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.8800\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 100ms/step - loss: 0.0225 - tp: 471.2821 - fp: 4.5128 - tn: 465.0256 - fn: 3.0427 - accuracy: 0.9880 - precision: 0.9823 - recall: 0.9930 - auc: 0.9995 - val_loss: 1.1058 - val_tp: 76.0000 - val_fp: 66.0000 - val_tn: 2.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5417 - val_precision: 0.5352 - val_recall: 1.0000 - val_auc: 0.9853\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0026 - tp: 453.8291 - fp: 0.0000e+00 - tn: 489.6325 - fn: 0.4017 - accuracy: 0.9997 - precision: 1.0000 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.3966 - val_tp: 69.0000 - val_fp: 25.0000 - val_tn: 50.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8264 - val_precision: 0.7340 - val_recall: 1.0000 - val_auc: 0.9992\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0043 - tp: 470.9829 - fp: 1.6838 - tn: 470.4274 - fn: 0.7692 - accuracy: 0.9976 - precision: 0.9967 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0160 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 4.7838e-04 - tp: 473.6667 - fp: 0.0000e+00 - tn: 470.1966 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0045 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0013 - tp: 454.5385 - fp: 0.0000e+00 - tn: 489.3248 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0181 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9859 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 8.0672e-04 - tp: 469.2137 - fp: 0.6496 - tn: 474.0000 - fn: 0.0000e+00 - accuracy: 0.9994 - precision: 0.9989 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0110 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 7.8590e-05 - tp: 464.6410 - fp: 0.0000e+00 - tn: 479.2222 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0202 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9722 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 5.6938e-05 - tp: 470.9402 - fp: 0.0000e+00 - tn: 472.9231 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0228 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9733 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.1245 - tp: 458.2821 - fp: 13.1111 - tn: 461.0085 - fn: 11.4615 - accuracy: 0.9757 - precision: 0.9740 - recall: 0.9775 - auc: 0.9889 - val_loss: 0.2137 - val_tp: 58.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 11.0000 - val_accuracy: 0.9236 - val_precision: 1.0000 - val_recall: 0.8406 - val_auc: 0.9928\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0645 - tp: 472.2137 - fp: 3.3077 - tn: 465.7949 - fn: 2.5470 - accuracy: 0.9932 - precision: 0.9916 - recall: 0.9951 - auc: 0.9983 - val_loss: 0.0209 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9865 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 3.1985e-04 - tp: 463.4017 - fp: 0.0000e+00 - tn: 480.4615 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0156 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 1.6119e-04 - tp: 467.1026 - fp: 0.0000e+00 - tn: 476.7607 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0131 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9865 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 1.2121e-04 - tp: 476.6068 - fp: 0.0000e+00 - tn: 467.2564 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0062 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 1.0409e-04 - tp: 473.8974 - fp: 0.0000e+00 - tn: 469.9658 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0045 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "FOLD 11\n",
      "{0: 0.994092373791622, 1: 1.0059782608695653}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 79s 579ms/step - loss: 1.8592 - tp: 397.1709 - fp: 77.7863 - tn: 394.6325 - fn: 74.2735 - accuracy: 0.7722 - precision: 0.7590 - recall: 0.7682 - auc: 0.8147 - val_loss: 3.2635 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 93ms/step - loss: 0.0272 - tp: 469.6325 - fp: 5.0342 - tn: 466.9231 - fn: 2.2735 - accuracy: 0.9931 - precision: 0.9908 - recall: 0.9954 - auc: 0.9977 - val_loss: 3.8842 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0058 - tp: 455.8547 - fp: 1.0855 - tn: 486.1453 - fn: 0.7778 - accuracy: 0.9984 - precision: 0.9983 - recall: 0.9983 - auc: 1.0000 - val_loss: 4.1819 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0088 - tp: 467.9829 - fp: 2.3248 - tn: 472.7350 - fn: 0.8205 - accuracy: 0.9977 - precision: 0.9965 - recall: 0.9989 - auc: 0.9999 - val_loss: 1.9621 - val_tp: 72.0000 - val_fp: 54.0000 - val_tn: 18.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6250 - val_precision: 0.5714 - val_recall: 1.0000 - val_auc: 0.8194\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0276 - tp: 465.0513 - fp: 3.6068 - tn: 471.2991 - fn: 3.9060 - accuracy: 0.9915 - precision: 0.9922 - recall: 0.9909 - auc: 0.9979 - val_loss: 0.3727 - val_tp: 70.0000 - val_fp: 20.0000 - val_tn: 54.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8611 - val_precision: 0.7778 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0025 - tp: 472.3162 - fp: 0.7265 - tn: 470.8205 - fn: 0.0000e+00 - accuracy: 0.9993 - precision: 0.9986 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0607 - val_tp: 71.0000 - val_fp: 2.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9726 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0011 - tp: 473.6752 - fp: 0.4444 - tn: 469.7436 - fn: 0.0000e+00 - accuracy: 0.9997 - precision: 0.9994 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0189 - val_tp: 75.0000 - val_fp: 1.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9868 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0107 - tp: 472.0342 - fp: 3.1368 - tn: 467.0171 - fn: 1.6752 - accuracy: 0.9965 - precision: 0.9953 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0061 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0028 - tp: 468.4359 - fp: 0.0000e+00 - tn: 475.3333 - fn: 0.0940 - accuracy: 0.9999 - precision: 1.0000 - recall: 0.9999 - auc: 0.9999 - val_loss: 0.0469 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9595 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0037 - tp: 466.7094 - fp: 0.6239 - tn: 476.5299 - fn: 0.0000e+00 - accuracy: 0.9995 - precision: 0.9990 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0031 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0352 - tp: 469.4957 - fp: 7.3419 - tn: 461.6667 - fn: 5.3590 - accuracy: 0.9889 - precision: 0.9871 - recall: 0.9909 - auc: 0.9977 - val_loss: 0.8399 - val_tp: 14.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 59.0000 - val_accuracy: 0.5903 - val_precision: 1.0000 - val_recall: 0.1918 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0129 - tp: 470.1368 - fp: 1.8120 - tn: 471.9145 - fn: 0.0000e+00 - accuracy: 0.9968 - precision: 0.9937 - recall: 1.0000 - auc: 0.9999 - val_loss: 0.0087 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0049 - tp: 473.2137 - fp: 1.5726 - tn: 469.0769 - fn: 0.0000e+00 - accuracy: 0.9985 - precision: 0.9970 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.0144e-04 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0099 - tp: 461.3761 - fp: 1.5641 - tn: 479.9316 - fn: 0.9915 - accuracy: 0.9958 - precision: 0.9961 - recall: 0.9951 - auc: 1.0000 - val_loss: 0.0341 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9722 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 6.4653e-04 - tp: 464.5385 - fp: 0.2393 - tn: 479.0855 - fn: 0.0000e+00 - accuracy: 0.9999 - precision: 0.9997 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0149 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 13s 91ms/step - loss: 1.5251 - tp: 379.0256 - fp: 87.4615 - tn: 381.5556 - fn: 95.8205 - accuracy: 0.7449 - precision: 0.7612 - recall: 0.7484 - auc: 0.7872 - val_loss: 1.1637 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.7057\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0727 - tp: 463.6923 - fp: 10.7949 - tn: 457.8803 - fn: 11.4957 - accuracy: 0.9758 - precision: 0.9782 - recall: 0.9740 - auc: 0.9959 - val_loss: 1.9924 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.7590\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0320 - tp: 465.3333 - fp: 4.8547 - tn: 468.0769 - fn: 5.5983 - accuracy: 0.9891 - precision: 0.9872 - recall: 0.9901 - auc: 0.9995 - val_loss: 1.5582 - val_tp: 72.0000 - val_fp: 64.0000 - val_tn: 8.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5556 - val_precision: 0.5294 - val_recall: 1.0000 - val_auc: 0.9787\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0076 - tp: 470.2308 - fp: 0.9829 - tn: 471.8205 - fn: 0.8291 - accuracy: 0.9970 - precision: 0.9961 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0924 - val_tp: 73.0000 - val_fp: 5.0000 - val_tn: 66.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9653 - val_precision: 0.9359 - val_recall: 1.0000 - val_auc: 0.9990\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0036 - tp: 468.4103 - fp: 0.3761 - tn: 473.7179 - fn: 1.3590 - accuracy: 0.9988 - precision: 0.9995 - recall: 0.9981 - auc: 0.9999 - val_loss: 0.0385 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9859 - val_auc: 0.9986\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0252 - tp: 463.8803 - fp: 5.3333 - tn: 470.6496 - fn: 4.0000 - accuracy: 0.9927 - precision: 0.9912 - recall: 0.9941 - auc: 0.9985 - val_loss: 1.0484 - val_tp: 73.0000 - val_fp: 19.0000 - val_tn: 52.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8681 - val_precision: 0.7935 - val_recall: 1.0000 - val_auc: 0.9071\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0288 - tp: 468.4017 - fp: 4.0855 - tn: 468.9402 - fn: 2.4359 - accuracy: 0.9901 - precision: 0.9867 - recall: 0.9936 - auc: 0.9990 - val_loss: 1.3787 - val_tp: 75.0000 - val_fp: 30.0000 - val_tn: 39.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7917 - val_precision: 0.7143 - val_recall: 1.0000 - val_auc: 0.8913\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0163 - tp: 469.5128 - fp: 2.2650 - tn: 469.4444 - fn: 2.6410 - accuracy: 0.9944 - precision: 0.9946 - recall: 0.9940 - auc: 0.9999 - val_loss: 1.1472 - val_tp: 44.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 31.0000 - val_accuracy: 0.7847 - val_precision: 1.0000 - val_recall: 0.5867 - val_auc: 0.9133\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0236 - tp: 455.2906 - fp: 2.4017 - tn: 482.0171 - fn: 4.1538 - accuracy: 0.9944 - precision: 0.9963 - recall: 0.9922 - auc: 0.9980 - val_loss: 9.8043 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 72.0000 - val_accuracy: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5069\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0112 - tp: 470.0684 - fp: 2.8632 - tn: 469.7949 - fn: 1.1368 - accuracy: 0.9967 - precision: 0.9952 - recall: 0.9981 - auc: 0.9994 - val_loss: 0.0681 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 67.0000 - val_fn: 4.0000 - val_accuracy: 0.9722 - val_precision: 1.0000 - val_recall: 0.9481 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0038 - tp: 481.4103 - fp: 0.0000e+00 - tn: 461.3590 - fn: 1.0940 - accuracy: 0.9988 - precision: 1.0000 - recall: 0.9977 - auc: 1.0000 - val_loss: 0.1640 - val_tp: 66.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 4.0000 - val_accuracy: 0.9722 - val_precision: 1.0000 - val_recall: 0.9429 - val_auc: 0.9786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0025 - tp: 461.1368 - fp: 0.0000e+00 - tn: 482.7265 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0230 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9861 - val_auc: 0.9996\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 2.6895e-04 - tp: 470.2650 - fp: 0.0171 - tn: 473.5812 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0079 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9867 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0012 - tp: 468.7863 - fp: 0.1368 - tn: 474.9060 - fn: 0.0342 - accuracy: 0.9999 - precision: 0.9998 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2258 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 4.0000 - val_accuracy: 0.9722 - val_precision: 1.0000 - val_recall: 0.9467 - val_auc: 0.9733\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0027 - tp: 468.7949 - fp: 0.9658 - tn: 474.1026 - fn: 0.0000e+00 - accuracy: 0.9983 - precision: 0.9964 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0655 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9730 - val_auc: 0.9930\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 89ms/step - loss: 0.8319 - tp: 421.8291 - fp: 48.2137 - tn: 429.6410 - fn: 44.1795 - accuracy: 0.8438 - precision: 0.8422 - recall: 0.8507 - auc: 0.8963 - val_loss: 0.3646 - val_tp: 72.0000 - val_fp: 9.0000 - val_tn: 62.0000 - val_fn: 1.0000 - val_accuracy: 0.9306 - val_precision: 0.8889 - val_recall: 0.9863 - val_auc: 0.9580\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0341 - tp: 468.8205 - fp: 5.3077 - tn: 464.3846 - fn: 5.3504 - accuracy: 0.9884 - precision: 0.9889 - recall: 0.9883 - auc: 0.9987 - val_loss: 1.2235 - val_tp: 73.0000 - val_fp: 68.0000 - val_tn: 3.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5278 - val_precision: 0.5177 - val_recall: 1.0000 - val_auc: 0.9783\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0047 - tp: 475.4786 - fp: 0.4103 - tn: 467.5128 - fn: 0.4615 - accuracy: 0.9994 - precision: 0.9994 - recall: 0.9994 - auc: 0.9998 - val_loss: 2.1191 - val_tp: 72.0000 - val_fp: 67.0000 - val_tn: 5.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5347 - val_precision: 0.5180 - val_recall: 1.0000 - val_auc: 0.8333\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0016 - tp: 473.8034 - fp: 0.1624 - tn: 469.6581 - fn: 0.2393 - accuracy: 0.9998 - precision: 0.9998 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0139 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9867 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 4.6023e-04 - tp: 473.2991 - fp: 0.0171 - tn: 470.4274 - fn: 0.1197 - accuracy: 0.9999 - precision: 1.0000 - recall: 0.9999 - auc: 1.0000 - val_loss: 0.0940 - val_tp: 73.0000 - val_fp: 8.0000 - val_tn: 63.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9444 - val_precision: 0.9012 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0042 - tp: 464.5470 - fp: 0.7094 - tn: 477.1880 - fn: 1.4188 - accuracy: 0.9980 - precision: 0.9987 - recall: 0.9973 - auc: 1.0000 - val_loss: 0.5445 - val_tp: 74.0000 - val_fp: 27.0000 - val_tn: 43.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8125 - val_precision: 0.7327 - val_recall: 1.0000 - val_auc: 0.9643\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 8.0277e-05 - tp: 472.9402 - fp: 0.0000e+00 - tn: 470.9231 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.7949e-05 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 3.9306e-05 - tp: 469.9145 - fp: 0.0000e+00 - tn: 473.9487 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.5936e-06 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 4.4745e-05 - tp: 472.1282 - fp: 0.0000e+00 - tn: 471.7350 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.3939e-06 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 5.2328e-05 - tp: 471.4957 - fp: 0.0000e+00 - tn: 472.3675 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.3193e-06 - val_tp: 76.0000 - val_fp: 0.0000e+00 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 1.4585e-05 - tp: 462.4188 - fp: 0.0000e+00 - tn: 481.4444 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.6696e-06 - val_tp: 76.0000 - val_fp: 0.0000e+00 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 1.6088e-05 - tp: 469.8034 - fp: 0.0000e+00 - tn: 474.0598 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.1740e-06 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 1.5599e-05 - tp: 482.9145 - fp: 0.0000e+00 - tn: 460.9487 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.1819e-06 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 1.4202e-05 - tp: 466.4786 - fp: 0.0000e+00 - tn: 477.3846 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.3787e-06 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 2.1485e-05 - tp: 472.3162 - fp: 0.0000e+00 - tn: 471.5470 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.6350e-07 - val_tp: 78.0000 - val_fp: 0.0000e+00 - val_tn: 66.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 111ms/step - loss: 2.4666 - tp: 372.8632 - fp: 91.3675 - tn: 387.5128 - fn: 92.1197 - accuracy: 0.7301 - precision: 0.7127 - recall: 0.7148 - auc: 0.7767 - val_loss: 0.7274 - val_tp: 67.0000 - val_fp: 77.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4653 - val_precision: 0.4653 - val_recall: 1.0000 - val_auc: 0.9355\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0276 - tp: 474.9573 - fp: 3.3590 - tn: 464.4872 - fn: 1.0598 - accuracy: 0.9953 - precision: 0.9939 - recall: 0.9969 - auc: 0.9990 - val_loss: 1.2466 - val_tp: 69.0000 - val_fp: 75.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4792 - val_recall: 1.0000 - val_auc: 0.9889\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0121 - tp: 470.4615 - fp: 2.2479 - tn: 470.1795 - fn: 0.9744 - accuracy: 0.9958 - precision: 0.9954 - recall: 0.9964 - auc: 1.0000 - val_loss: 1.0965 - val_tp: 74.0000 - val_fp: 67.0000 - val_tn: 3.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5347 - val_precision: 0.5248 - val_recall: 1.0000 - val_auc: 0.9996\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0057 - tp: 476.0684 - fp: 0.7094 - tn: 467.0855 - fn: 0.0000e+00 - accuracy: 0.9993 - precision: 0.9987 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0312 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9857 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0026 - tp: 462.3504 - fp: 0.0000e+00 - tn: 481.2735 - fn: 0.2393 - accuracy: 0.9999 - precision: 1.0000 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0042 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0013 - tp: 462.7350 - fp: 0.0000e+00 - tn: 481.1282 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0064 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0026 - tp: 472.8974 - fp: 0.4530 - tn: 470.5128 - fn: 0.0000e+00 - accuracy: 0.9997 - precision: 0.9994 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0096 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9865 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 12s 102ms/step - loss: 0.0017 - tp: 473.4017 - fp: 0.0000e+00 - tn: 470.4615 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0082 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0014 - tp: 466.7436 - fp: 0.0000e+00 - tn: 477.1197 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.6137e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 9.3148e-04 - tp: 466.5043 - fp: 0.0000e+00 - tn: 477.3590 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.5916e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 12s 102ms/step - loss: 5.1714e-04 - tp: 464.6752 - fp: 0.0000e+00 - tn: 479.1880 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.7167e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0022 - tp: 468.8547 - fp: 0.0000e+00 - tn: 475.0085 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0032 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 4.1034e-04 - tp: 472.7265 - fp: 0.0000e+00 - tn: 471.1368 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0050 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 9.7240e-05 - tp: 464.5214 - fp: 0.0000e+00 - tn: 479.3419 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0029 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 1.0104e-04 - tp: 481.0085 - fp: 0.0000e+00 - tn: 462.8547 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0028 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "FOLD 12\n",
      "{0: 0.9951612903225806, 1: 1.004885993485342}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 79s 581ms/step - loss: 1.7720 - tp: 394.5043 - fp: 82.7778 - tn: 389.4103 - fn: 77.1709 - accuracy: 0.7431 - precision: 0.7492 - recall: 0.7628 - auc: 0.7623 - val_loss: 1.9569 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.5076\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 11s 97ms/step - loss: 0.0253 - tp: 464.4957 - fp: 3.0085 - tn: 474.6752 - fn: 1.6838 - accuracy: 0.9925 - precision: 0.9884 - recall: 0.9958 - auc: 0.9999 - val_loss: 4.0034 - val_tp: 69.0000 - val_fp: 75.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4792 - val_precision: 0.4792 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0096 - tp: 454.7692 - fp: 2.8803 - tn: 484.8632 - fn: 1.3504 - accuracy: 0.9965 - precision: 0.9946 - recall: 0.9982 - auc: 0.9999 - val_loss: 4.5306 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0047 - tp: 469.1453 - fp: 0.7436 - tn: 473.9744 - fn: 0.0000e+00 - accuracy: 0.9994 - precision: 0.9988 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.3198 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.6486\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0167 - tp: 467.1111 - fp: 1.2906 - tn: 473.7350 - fn: 1.7265 - accuracy: 0.9959 - precision: 0.9973 - recall: 0.9945 - auc: 0.9999 - val_loss: 0.9312 - val_tp: 66.0000 - val_fp: 33.0000 - val_tn: 45.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7708 - val_precision: 0.6667 - val_recall: 1.0000 - val_auc: 0.9359\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 5.2197e-04 - tp: 474.8034 - fp: 0.0000e+00 - tn: 469.0598 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0996 - val_tp: 75.0000 - val_fp: 5.0000 - val_tn: 64.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9653 - val_precision: 0.9375 - val_recall: 1.0000 - val_auc: 0.9928\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 5.6511e-04 - tp: 468.9487 - fp: 0.0000e+00 - tn: 474.6667 - fn: 0.2479 - accuracy: 0.9998 - precision: 1.0000 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0495 - val_tp: 71.0000 - val_fp: 3.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9595 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0070 - tp: 461.4872 - fp: 1.2735 - tn: 480.2222 - fn: 0.8803 - accuracy: 0.9978 - precision: 0.9978 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0567 - val_tp: 66.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 5.0000 - val_accuracy: 0.9653 - val_precision: 1.0000 - val_recall: 0.9296 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 3.5595e-04 - tp: 481.9487 - fp: 0.0000e+00 - tn: 461.9145 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.1591e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 2.8474e-04 - tp: 478.7350 - fp: 0.0000e+00 - tn: 465.1282 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0649e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 1.4575e-04 - tp: 466.2222 - fp: 0.0000e+00 - tn: 477.6410 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.7709e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0011 - tp: 464.8718 - fp: 0.0000e+00 - tn: 478.2393 - fn: 0.7521 - accuracy: 0.9993 - precision: 1.0000 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.0058 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 77.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 1.1170e-04 - tp: 461.9915 - fp: 0.0000e+00 - tn: 481.8718 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0043 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 8.1166e-05 - tp: 476.5299 - fp: 0.0000e+00 - tn: 467.3333 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.9247e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0055 - tp: 456.2821 - fp: 0.9658 - tn: 485.7179 - fn: 0.8974 - accuracy: 0.9971 - precision: 0.9964 - recall: 0.9975 - auc: 1.0000 - val_loss: 0.0011 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 87ms/step - loss: 1.4177 - tp: 396.9316 - fp: 81.6923 - tn: 386.5128 - fn: 78.7265 - accuracy: 0.7751 - precision: 0.7787 - recall: 0.7861 - auc: 0.8163 - val_loss: 1.1091 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.5624\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0654 - tp: 466.2479 - fp: 8.1795 - tn: 461.7436 - fn: 7.6923 - accuracy: 0.9828 - precision: 0.9836 - recall: 0.9821 - auc: 0.9957 - val_loss: 1.2864 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.7184\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0429 - tp: 454.6581 - fp: 7.6838 - tn: 474.7094 - fn: 6.8120 - accuracy: 0.9842 - precision: 0.9818 - recall: 0.9858 - auc: 0.9987 - val_loss: 0.9879 - val_tp: 73.0000 - val_fp: 57.0000 - val_tn: 14.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6042 - val_precision: 0.5615 - val_recall: 1.0000 - val_auc: 0.9831\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0316 - tp: 458.5897 - fp: 5.2393 - tn: 474.0684 - fn: 5.9658 - accuracy: 0.9871 - precision: 0.9893 - recall: 0.9850 - auc: 0.9994 - val_loss: 11.4249 - val_tp: 75.0000 - val_fp: 69.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5208 - val_precision: 0.5208 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0386 - tp: 472.7521 - fp: 5.5043 - tn: 461.9231 - fn: 3.6838 - accuracy: 0.9877 - precision: 0.9857 - recall: 0.9889 - auc: 0.9978 - val_loss: 0.0527 - val_tp: 68.0000 - val_fp: 4.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9722 - val_precision: 0.9444 - val_recall: 1.0000 - val_auc: 0.9995\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0090 - tp: 459.7179 - fp: 2.1795 - tn: 480.6068 - fn: 1.3590 - accuracy: 0.9969 - precision: 0.9955 - recall: 0.9980 - auc: 0.9999 - val_loss: 0.2061 - val_tp: 59.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 10.0000 - val_accuracy: 0.9306 - val_precision: 1.0000 - val_recall: 0.8551 - val_auc: 0.9928\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0257 - tp: 465.3333 - fp: 3.9658 - tn: 471.5214 - fn: 3.0427 - accuracy: 0.9922 - precision: 0.9905 - recall: 0.9934 - auc: 0.9990 - val_loss: 0.0536 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9722 - val_auc: 0.9988\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0026 - tp: 483.1709 - fp: 0.7350 - tn: 459.1282 - fn: 0.8291 - accuracy: 0.9989 - precision: 0.9990 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.1822 - val_tp: 67.0000 - val_fp: 8.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9444 - val_precision: 0.8933 - val_recall: 1.0000 - val_auc: 0.9984\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0029 - tp: 462.7094 - fp: 0.0940 - tn: 480.4786 - fn: 0.5812 - accuracy: 0.9996 - precision: 0.9999 - recall: 0.9992 - auc: 1.0000 - val_loss: 0.2168 - val_tp: 70.0000 - val_fp: 8.0000 - val_tn: 66.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9444 - val_precision: 0.8974 - val_recall: 1.0000 - val_auc: 0.9792\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0591 - tp: 460.7692 - fp: 6.9402 - tn: 469.9145 - fn: 6.2393 - accuracy: 0.9847 - precision: 0.9852 - recall: 0.9840 - auc: 0.9954 - val_loss: 0.2664 - val_tp: 69.0000 - val_fp: 15.0000 - val_tn: 60.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8958 - val_precision: 0.8214 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0164 - tp: 461.8291 - fp: 3.4103 - tn: 476.9744 - fn: 1.6496 - accuracy: 0.9927 - precision: 0.9897 - recall: 0.9961 - auc: 0.9987 - val_loss: 1.9601 - val_tp: 16.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 56.0000 - val_accuracy: 0.6111 - val_precision: 1.0000 - val_recall: 0.2222 - val_auc: 0.8472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 3.6561e-04 - tp: 467.4615 - fp: 0.0000e+00 - tn: 476.4017 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0061 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 3.8299e-04 - tp: 468.3675 - fp: 0.0000e+00 - tn: 475.4957 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0110 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 1.2177e-04 - tp: 472.2308 - fp: 0.0000e+00 - tn: 471.6325 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0114 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9861 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 4.3536e-05 - tp: 474.1624 - fp: 0.0000e+00 - tn: 469.7009 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0124 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 90ms/step - loss: 0.4711 - tp: 449.2991 - fp: 25.4103 - tn: 443.0171 - fn: 26.1368 - accuracy: 0.9010 - precision: 0.9051 - recall: 0.9051 - auc: 0.9427 - val_loss: 0.7209 - val_tp: 68.0000 - val_fp: 76.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4722 - val_precision: 0.4722 - val_recall: 1.0000 - val_auc: 0.9640\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0191 - tp: 466.4017 - fp: 2.2735 - tn: 471.6496 - fn: 3.5385 - accuracy: 0.9930 - precision: 0.9937 - recall: 0.9920 - auc: 0.9982 - val_loss: 0.6824 - val_tp: 72.0000 - val_fp: 69.0000 - val_tn: 3.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5208 - val_precision: 0.5106 - val_recall: 1.0000 - val_auc: 0.9603\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0138 - tp: 468.0256 - fp: 2.0171 - tn: 473.0171 - fn: 0.8034 - accuracy: 0.9970 - precision: 0.9958 - recall: 0.9982 - auc: 0.9996 - val_loss: 0.2546 - val_tp: 66.0000 - val_fp: 2.0000 - val_tn: 68.0000 - val_fn: 8.0000 - val_accuracy: 0.9306 - val_precision: 0.9706 - val_recall: 0.8919 - val_auc: 0.9819\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0107 - tp: 459.9231 - fp: 2.2906 - tn: 479.6154 - fn: 2.0342 - accuracy: 0.9964 - precision: 0.9961 - recall: 0.9966 - auc: 0.9998 - val_loss: 0.0326 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 0.9998\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0043 - tp: 467.7778 - fp: 0.7094 - tn: 474.6068 - fn: 0.7692 - accuracy: 0.9986 - precision: 0.9987 - recall: 0.9984 - auc: 1.0000 - val_loss: 0.0608 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 4.3637e-04 - tp: 470.4017 - fp: 0.0000e+00 - tn: 473.4615 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0867 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9857 - val_recall: 1.0000 - val_auc: 0.9933\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 1.7364e-04 - tp: 465.4103 - fp: 0.0000e+00 - tn: 478.4530 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2029 - val_tp: 71.0000 - val_fp: 2.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9726 - val_recall: 1.0000 - val_auc: 0.9863\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 7.3119e-05 - tp: 470.7692 - fp: 0.0000e+00 - tn: 473.0940 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1030 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 3.5179e-04 - tp: 463.5470 - fp: 0.0000e+00 - tn: 480.3162 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1222 - val_tp: 76.0000 - val_fp: 1.0000 - val_tn: 67.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9870 - val_recall: 1.0000 - val_auc: 0.9926\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 7.2939e-05 - tp: 460.0940 - fp: 0.0000e+00 - tn: 483.7692 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1210 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 7.5530e-05 - tp: 471.2393 - fp: 0.0000e+00 - tn: 472.6239 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.2378 - val_tp: 68.0000 - val_fp: 2.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9714 - val_recall: 1.0000 - val_auc: 0.9868\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 4.4978e-06 - tp: 457.7094 - fp: 0.0000e+00 - tn: 486.1538 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1182 - val_tp: 74.0000 - val_fp: 1.0000 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9867 - val_recall: 1.0000 - val_auc: 0.9929\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 2.7604e-05 - tp: 471.2906 - fp: 0.0000e+00 - tn: 472.5726 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0993 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9861 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 6.5482e-06 - tp: 481.2137 - fp: 0.0000e+00 - tn: 462.6496 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1024 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 1.1147e-05 - tp: 475.0769 - fp: 0.0000e+00 - tn: 468.7863 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1051 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 0.9930\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 114ms/step - loss: 3.0931 - tp: 356.3590 - fp: 105.9316 - tn: 374.3675 - fn: 107.2051 - accuracy: 0.7082 - precision: 0.6970 - recall: 0.6945 - auc: 0.7436 - val_loss: 0.8191 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.7527\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0166 - tp: 456.7179 - fp: 2.0769 - tn: 482.6667 - fn: 2.4017 - accuracy: 0.9950 - precision: 0.9963 - recall: 0.9934 - auc: 0.9999 - val_loss: 1.2669 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.8109\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0186 - tp: 467.8034 - fp: 1.2308 - tn: 472.7094 - fn: 2.1197 - accuracy: 0.9958 - precision: 0.9977 - recall: 0.9936 - auc: 0.9997 - val_loss: 1.2677 - val_tp: 68.0000 - val_fp: 70.0000 - val_tn: 6.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.4928 - val_recall: 1.0000 - val_auc: 0.9438\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0029 - tp: 477.0171 - fp: 0.8803 - tn: 465.9658 - fn: 0.0000e+00 - accuracy: 0.9989 - precision: 0.9978 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.3378 - val_tp: 70.0000 - val_fp: 19.0000 - val_tn: 55.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8681 - val_precision: 0.7865 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0017 - tp: 471.0855 - fp: 0.0000e+00 - tn: 472.7778 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0967 - val_tp: 69.0000 - val_fp: 2.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9718 - val_recall: 1.0000 - val_auc: 0.9933\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0013 - tp: 481.6239 - fp: 0.0000e+00 - tn: 462.2393 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0708 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0015 - tp: 467.3846 - fp: 0.0000e+00 - tn: 476.4786 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0555 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9861 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 2.5794e-04 - tp: 461.0598 - fp: 0.0000e+00 - tn: 482.8034 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0540 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 0.9930\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 2.6157e-04 - tp: 475.9145 - fp: 0.0000e+00 - tn: 467.9487 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0731 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 5.0105e-04 - tp: 478.2479 - fp: 0.0000e+00 - tn: 465.6154 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0689 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 3.4502e-04 - tp: 468.8718 - fp: 0.0000e+00 - tn: 474.9915 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0559 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9857 - val_recall: 1.0000 - val_auc: 0.9933\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 4.3596e-04 - tp: 462.7436 - fp: 0.0000e+00 - tn: 481.1197 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0633 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 1.0542e-04 - tp: 469.8205 - fp: 0.0000e+00 - tn: 474.0427 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0538 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 0.9930\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 1.2350e-04 - tp: 470.1282 - fp: 0.0000e+00 - tn: 473.7350 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0585 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9857 - val_recall: 1.0000 - val_auc: 0.9933\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 1.5450e-04 - tp: 475.3590 - fp: 0.0000e+00 - tn: 468.5043 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0576 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9857 - val_recall: 1.0000 - val_auc: 0.9933\n",
      "FOLD 13\n",
      "{0: 0.9951612903225806, 1: 1.004885993485342}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 81s 595ms/step - loss: 1.8158 - tp: 397.3162 - fp: 69.8803 - tn: 407.1624 - fn: 69.5043 - accuracy: 0.7689 - precision: 0.7766 - recall: 0.7717 - auc: 0.8059 - val_loss: 1.3034 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.7729\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 10s 89ms/step - loss: 0.0249 - tp: 463.2906 - fp: 4.2991 - tn: 473.5470 - fn: 2.7265 - accuracy: 0.9934 - precision: 0.9918 - recall: 0.9947 - auc: 0.9990 - val_loss: 1.5163 - val_tp: 75.0000 - val_fp: 69.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5208 - val_precision: 0.5208 - val_recall: 1.0000 - val_auc: 0.9111\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0287 - tp: 463.1966 - fp: 4.1538 - tn: 474.6410 - fn: 1.8718 - accuracy: 0.9923 - precision: 0.9888 - recall: 0.9955 - auc: 0.9991 - val_loss: 1.1403 - val_tp: 68.0000 - val_fp: 65.0000 - val_tn: 11.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5486 - val_precision: 0.5113 - val_recall: 1.0000 - val_auc: 0.9988\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0130 - tp: 470.2821 - fp: 2.4359 - tn: 470.1111 - fn: 1.0342 - accuracy: 0.9961 - precision: 0.9942 - recall: 0.9979 - auc: 0.9999 - val_loss: 0.0783 - val_tp: 74.0000 - val_fp: 2.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9737 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0090 - tp: 460.9915 - fp: 3.2051 - tn: 479.4103 - fn: 0.2564 - accuracy: 0.9945 - precision: 0.9891 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0046 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0131 - tp: 474.1795 - fp: 1.9658 - tn: 464.7265 - fn: 2.9915 - accuracy: 0.9942 - precision: 0.9970 - recall: 0.9915 - auc: 0.9999 - val_loss: 0.0282 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9722 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0119 - tp: 455.0769 - fp: 2.8632 - tn: 484.6068 - fn: 1.3162 - accuracy: 0.9959 - precision: 0.9934 - recall: 0.9978 - auc: 0.9999 - val_loss: 0.0217 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 8s 66ms/step - loss: 0.0030 - tp: 463.9487 - fp: 0.1282 - tn: 479.7863 - fn: 0.0000e+00 - accuracy: 0.9999 - precision: 0.9999 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0011 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0101 - tp: 465.9829 - fp: 0.0000e+00 - tn: 476.8889 - fn: 0.9915 - accuracy: 0.9977 - precision: 1.0000 - recall: 0.9954 - auc: 1.0000 - val_loss: 4.0677e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0014 - tp: 460.5641 - fp: 0.0000e+00 - tn: 483.2991 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0188 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9857 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 3.9849e-04 - tp: 461.1624 - fp: 0.0000e+00 - tn: 482.7009 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0648 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 0.9932\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 2.6109e-04 - tp: 467.9658 - fp: 0.0000e+00 - tn: 475.8974 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0722 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9853 - val_auc: 0.9926\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 1.1036e-04 - tp: 462.2137 - fp: 0.0000e+00 - tn: 481.6496 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0492 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9861 - val_auc: 0.9931\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 3.5918e-05 - tp: 462.4701 - fp: 0.0000e+00 - tn: 481.3932 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0540 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9863 - val_auc: 0.9932\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0221 - tp: 455.9402 - fp: 3.8034 - tn: 480.4444 - fn: 3.6752 - accuracy: 0.9947 - precision: 0.9945 - recall: 0.9947 - auc: 0.9985 - val_loss: 0.0014 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 88ms/step - loss: 1.4952 - tp: 373.8547 - fp: 112.1111 - tn: 360.9402 - fn: 96.9573 - accuracy: 0.7277 - precision: 0.7119 - recall: 0.7387 - auc: 0.7814 - val_loss: 1.0074 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.6309\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.1106 - tp: 452.3248 - fp: 20.5470 - tn: 453.2821 - fn: 17.7094 - accuracy: 0.9580 - precision: 0.9545 - recall: 0.9622 - auc: 0.9925 - val_loss: 1.5033 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.8558\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0384 - tp: 455.5470 - fp: 6.3846 - tn: 476.3248 - fn: 5.6068 - accuracy: 0.9860 - precision: 0.9828 - recall: 0.9884 - auc: 0.9992 - val_loss: 0.9873 - val_tp: 70.0000 - val_fp: 61.0000 - val_tn: 13.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5764 - val_precision: 0.5344 - val_recall: 1.0000 - val_auc: 0.9834\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0132 - tp: 471.7436 - fp: 1.6154 - tn: 467.9744 - fn: 2.5299 - accuracy: 0.9958 - precision: 0.9973 - recall: 0.9943 - auc: 0.9999 - val_loss: 0.0588 - val_tp: 68.0000 - val_fp: 3.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9577 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0311 - tp: 455.5214 - fp: 4.2906 - tn: 480.8718 - fn: 3.1795 - accuracy: 0.9909 - precision: 0.9888 - recall: 0.9920 - auc: 0.9980 - val_loss: 0.0862 - val_tp: 69.0000 - val_fp: 3.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9583 - val_recall: 1.0000 - val_auc: 0.9994\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0182 - tp: 474.2564 - fp: 3.4359 - tn: 463.5641 - fn: 2.6068 - accuracy: 0.9938 - precision: 0.9923 - recall: 0.9956 - auc: 0.9998 - val_loss: 0.0652 - val_tp: 71.0000 - val_fp: 6.0000 - val_tn: 67.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9583 - val_precision: 0.9221 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0057 - tp: 471.5470 - fp: 0.0000e+00 - tn: 472.3162 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0156 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9857 - val_auc: 0.9998\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0013 - tp: 454.8462 - fp: 0.0000e+00 - tn: 488.6154 - fn: 0.4017 - accuracy: 0.9997 - precision: 1.0000 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0236 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 0.9998\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 2.4468e-04 - tp: 480.6923 - fp: 0.0000e+00 - tn: 463.1709 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0116 - val_tp: 68.0000 - val_fp: 1.0000 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9855 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 4.3864e-04 - tp: 474.1624 - fp: 0.0000e+00 - tn: 469.7009 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0147 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 2.0000 - val_accuracy: 0.9861 - val_precision: 1.0000 - val_recall: 0.9714 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 8.9590e-05 - tp: 478.6581 - fp: 0.0000e+00 - tn: 465.2051 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0102 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9855 - val_auc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 1.2568e-04 - tp: 461.6496 - fp: 0.0000e+00 - tn: 482.2137 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0106 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9861 - val_recall: 0.9861 - val_auc: 0.9998\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 1.2269e-04 - tp: 464.1368 - fp: 0.0000e+00 - tn: 479.7265 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0117 - val_tp: 67.0000 - val_fp: 1.0000 - val_tn: 75.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9853 - val_recall: 0.9853 - val_auc: 0.9998\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0020 - tp: 478.6752 - fp: 0.5128 - tn: 464.3504 - fn: 0.3248 - accuracy: 0.9995 - precision: 0.9994 - recall: 0.9996 - auc: 0.9999 - val_loss: 0.0467 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9577 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.1131 - tp: 446.7778 - fp: 16.2308 - tn: 465.1966 - fn: 15.6581 - accuracy: 0.9616 - precision: 0.9577 - recall: 0.9633 - auc: 0.9911 - val_loss: 0.5606 - val_tp: 72.0000 - val_fp: 25.0000 - val_tn: 47.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8264 - val_precision: 0.7423 - val_recall: 1.0000 - val_auc: 0.9792\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 89ms/step - loss: 1.3695 - tp: 421.8974 - fp: 41.8889 - tn: 432.2137 - fn: 47.8632 - accuracy: 0.8390 - precision: 0.8381 - recall: 0.8234 - auc: 0.8653 - val_loss: 0.3788 - val_tp: 71.0000 - val_fp: 15.0000 - val_tn: 58.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8958 - val_precision: 0.8256 - val_recall: 1.0000 - val_auc: 0.9963\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0079 - tp: 472.0513 - fp: 1.1111 - tn: 469.7436 - fn: 0.9573 - accuracy: 0.9983 - precision: 0.9979 - recall: 0.9987 - auc: 1.0000 - val_loss: 0.0880 - val_tp: 65.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9559 - val_auc: 0.9992\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0652 - tp: 464.3504 - fp: 7.4957 - tn: 462.3761 - fn: 9.6410 - accuracy: 0.9815 - precision: 0.9860 - recall: 0.9771 - auc: 0.9963 - val_loss: 0.5788 - val_tp: 71.0000 - val_fp: 34.0000 - val_tn: 39.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7639 - val_precision: 0.6762 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0084 - tp: 462.4359 - fp: 0.6838 - tn: 479.4274 - fn: 1.3162 - accuracy: 0.9982 - precision: 0.9988 - recall: 0.9975 - auc: 0.9996 - val_loss: 0.0046 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0220 - tp: 475.4615 - fp: 2.5470 - tn: 462.7009 - fn: 3.1538 - accuracy: 0.9934 - precision: 0.9942 - recall: 0.9931 - auc: 0.9985 - val_loss: 0.0034 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 3.5597e-05 - tp: 484.6838 - fp: 0.0000e+00 - tn: 459.1795 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 6.3965e-05 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 10s 83ms/step - loss: 2.1046e-05 - tp: 462.4786 - fp: 0.0000e+00 - tn: 481.3846 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.0484e-06 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 5.2420e-06 - tp: 467.0342 - fp: 0.0000e+00 - tn: 476.8291 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.1730e-06 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 2.0992e-04 - tp: 474.2308 - fp: 0.0000e+00 - tn: 469.6325 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.3721e-05 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 2.0727e-05 - tp: 471.8205 - fp: 0.0000e+00 - tn: 472.0427 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.5562e-07 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 1.6680e-06 - tp: 469.0085 - fp: 0.0000e+00 - tn: 474.8547 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.9567e-07 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 2.8054e-05 - tp: 476.7521 - fp: 0.0000e+00 - tn: 467.1111 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.2976e-07 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 2.1905e-06 - tp: 462.2735 - fp: 0.0000e+00 - tn: 481.5897 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.6656e-07 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 6.2128e-06 - tp: 465.7094 - fp: 0.0000e+00 - tn: 478.1538 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.1317e-07 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 7.5560e-06 - tp: 462.8376 - fp: 0.0000e+00 - tn: 481.0256 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.2878e-07 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 110ms/step - loss: 3.5493 - tp: 366.3846 - fp: 89.8547 - tn: 396.2650 - fn: 91.3590 - accuracy: 0.7362 - precision: 0.7144 - recall: 0.7149 - auc: 0.7585 - val_loss: 1.2249 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.4666\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0209 - tp: 473.6667 - fp: 3.2308 - tn: 465.7436 - fn: 1.2222 - accuracy: 0.9957 - precision: 0.9947 - recall: 0.9967 - auc: 0.9995 - val_loss: 1.5191 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.8466\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0141 - tp: 461.9487 - fp: 3.5128 - tn: 476.6667 - fn: 1.7350 - accuracy: 0.9945 - precision: 0.9919 - recall: 0.9970 - auc: 0.9999 - val_loss: 1.5014 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 0.0034 - tp: 468.9060 - fp: 0.2735 - tn: 474.6068 - fn: 0.0769 - accuracy: 0.9998 - precision: 0.9997 - recall: 0.9999 - auc: 1.0000 - val_loss: 0.1821 - val_tp: 67.0000 - val_fp: 10.0000 - val_tn: 67.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9306 - val_precision: 0.8701 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0042 - tp: 479.3333 - fp: 0.0000e+00 - tn: 464.5299 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0047 - val_tp: 65.0000 - val_fp: 0.0000e+00 - val_tn: 79.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0023 - tp: 472.2991 - fp: 0.0000e+00 - tn: 471.1538 - fn: 0.4103 - accuracy: 0.9997 - precision: 1.0000 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0037 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 12s 99ms/step - loss: 0.0036 - tp: 465.4274 - fp: 0.0000e+00 - tn: 477.7607 - fn: 0.6752 - accuracy: 0.9994 - precision: 1.0000 - recall: 0.9988 - auc: 1.0000 - val_loss: 0.0027 - val_tp: 66.0000 - val_fp: 0.0000e+00 - val_tn: 78.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0147 - tp: 466.8205 - fp: 3.3419 - tn: 471.7179 - fn: 1.9829 - accuracy: 0.9955 - precision: 0.9944 - recall: 0.9965 - auc: 0.9998 - val_loss: 0.0018 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0013 - tp: 477.0085 - fp: 0.0000e+00 - tn: 466.8547 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0031 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0017 - tp: 465.1111 - fp: 0.0000e+00 - tn: 478.7521 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0032 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 5.0028e-04 - tp: 475.1538 - fp: 0.0000e+00 - tn: 468.7094 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0037 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 11s 99ms/step - loss: 4.4574e-04 - tp: 476.2308 - fp: 0.0000e+00 - tn: 467.6325 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0047 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 2.9174e-04 - tp: 473.5299 - fp: 0.0000e+00 - tn: 470.3333 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0053 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9859 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 1.3625e-04 - tp: 470.4530 - fp: 0.0000e+00 - tn: 473.4103 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0054 - val_tp: 76.0000 - val_fp: 0.0000e+00 - val_tn: 67.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9870 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 3.2031e-04 - tp: 466.3248 - fp: 0.0000e+00 - tn: 477.5385 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0043 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "FOLD 14\n",
      "{0: 0.9951612903225806, 1: 1.004885993485342}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 82s 619ms/step - loss: 1.8251 - tp: 363.2650 - fp: 93.8462 - tn: 377.6239 - fn: 109.1282 - accuracy: 0.7049 - precision: 0.7189 - recall: 0.6838 - auc: 0.7246 - val_loss: 1.2955 - val_tp: 74.0000 - val_fp: 70.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5139 - val_precision: 0.5139 - val_recall: 1.0000 - val_auc: 0.7724\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 10s 89ms/step - loss: 0.0135 - tp: 478.1624 - fp: 2.8974 - tn: 462.1111 - fn: 0.6923 - accuracy: 0.9970 - precision: 0.9953 - recall: 0.9988 - auc: 0.9994 - val_loss: 2.7138 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.6700\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0072 - tp: 471.8462 - fp: 2.1624 - tn: 468.4274 - fn: 1.4274 - accuracy: 0.9960 - precision: 0.9961 - recall: 0.9961 - auc: 1.0000 - val_loss: 0.9602 - val_tp: 66.0000 - val_fp: 49.0000 - val_tn: 29.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6597 - val_precision: 0.5739 - val_recall: 1.0000 - val_auc: 0.9868\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0032 - tp: 461.6068 - fp: 0.0000e+00 - tn: 481.5214 - fn: 0.7350 - accuracy: 0.9993 - precision: 1.0000 - recall: 0.9985 - auc: 1.0000 - val_loss: 0.1443 - val_tp: 75.0000 - val_fp: 6.0000 - val_tn: 63.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9583 - val_precision: 0.9259 - val_recall: 1.0000 - val_auc: 0.9928\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0049 - tp: 470.3932 - fp: 0.5726 - tn: 472.0855 - fn: 0.8120 - accuracy: 0.9987 - precision: 0.9992 - recall: 0.9982 - auc: 1.0000 - val_loss: 0.0098 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0068 - tp: 460.5812 - fp: 1.1795 - tn: 480.8889 - fn: 1.2137 - accuracy: 0.9981 - precision: 0.9982 - recall: 0.9979 - auc: 1.0000 - val_loss: 0.0289 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9853 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0052 - tp: 476.7863 - fp: 0.8120 - tn: 466.2650 - fn: 0.0000e+00 - accuracy: 0.9991 - precision: 0.9983 - recall: 1.0000 - auc: 1.0000 - val_loss: 9.3095e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 3.4506e-04 - tp: 471.2821 - fp: 0.0000e+00 - tn: 472.5812 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.5592e-04 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 9.2777e-04 - tp: 473.1709 - fp: 0.0000e+00 - tn: 470.6923 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.0638e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 1.8747e-04 - tp: 473.8803 - fp: 0.0000e+00 - tn: 469.9829 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.7887e-04 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 4.6755e-04 - tp: 471.1368 - fp: 0.0000e+00 - tn: 472.7265 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.7361e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 1.9113e-04 - tp: 464.2650 - fp: 0.0000e+00 - tn: 479.5983 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.0380e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 7.4364e-05 - tp: 464.7607 - fp: 0.0000e+00 - tn: 479.1026 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.2676e-04 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 8.1747e-04 - tp: 470.0000 - fp: 0.0000e+00 - tn: 473.8632 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0178 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 4.0855e-04 - tp: 468.2051 - fp: 0.2991 - tn: 475.3590 - fn: 0.0000e+00 - accuracy: 0.9998 - precision: 0.9996 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.5377e-04 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 88ms/step - loss: 0.9534 - tp: 397.3162 - fp: 78.3932 - tn: 393.8120 - fn: 74.3419 - accuracy: 0.7789 - precision: 0.7722 - recall: 0.7826 - auc: 0.8387 - val_loss: 0.9222 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 70.0000 - val_accuracy: 0.5139 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5103\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0746 - tp: 458.4701 - fp: 14.5641 - tn: 458.1709 - fn: 12.6581 - accuracy: 0.9719 - precision: 0.9692 - recall: 0.9761 - auc: 0.9965 - val_loss: 0.7270 - val_tp: 69.0000 - val_fp: 69.0000 - val_tn: 6.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5208 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.8757\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0496 - tp: 470.0000 - fp: 7.9231 - tn: 458.5726 - fn: 7.3675 - accuracy: 0.9805 - precision: 0.9798 - recall: 0.9834 - auc: 0.9985 - val_loss: 0.4890 - val_tp: 70.0000 - val_fp: 33.0000 - val_tn: 41.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7708 - val_precision: 0.6796 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0212 - tp: 463.9145 - fp: 2.3248 - tn: 473.2222 - fn: 4.4017 - accuracy: 0.9933 - precision: 0.9959 - recall: 0.9905 - auc: 0.9996 - val_loss: 0.0507 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0252 - tp: 473.8462 - fp: 3.8034 - tn: 462.2821 - fn: 3.9316 - accuracy: 0.9915 - precision: 0.9911 - recall: 0.9924 - auc: 0.9996 - val_loss: 0.0189 - val_tp: 67.0000 - val_fp: 2.0000 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9710 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0303 - tp: 469.4274 - fp: 3.3248 - tn: 466.2821 - fn: 4.8291 - accuracy: 0.9904 - precision: 0.9926 - recall: 0.9881 - auc: 0.9972 - val_loss: 0.0118 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0031 - tp: 469.7521 - fp: 0.4017 - tn: 473.1282 - fn: 0.5812 - accuracy: 0.9993 - precision: 0.9995 - recall: 0.9991 - auc: 1.0000 - val_loss: 0.0051 - val_tp: 75.0000 - val_fp: 0.0000e+00 - val_tn: 69.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0169 - tp: 463.8462 - fp: 2.8120 - tn: 474.3504 - fn: 2.8547 - accuracy: 0.9948 - precision: 0.9945 - recall: 0.9949 - auc: 0.9998 - val_loss: 0.0146 - val_tp: 67.0000 - val_fp: 1.0000 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9853 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0111 - tp: 464.1282 - fp: 2.0000 - tn: 477.0427 - fn: 0.6923 - accuracy: 0.9976 - precision: 0.9963 - recall: 0.9987 - auc: 0.9999 - val_loss: 0.1500 - val_tp: 69.0000 - val_fp: 10.0000 - val_tn: 65.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9306 - val_precision: 0.8734 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0141 - tp: 458.7863 - fp: 3.2222 - tn: 480.2821 - fn: 1.5726 - accuracy: 0.9957 - precision: 0.9939 - recall: 0.9974 - auc: 0.9997 - val_loss: 0.0258 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9861 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0036 - tp: 492.8034 - fp: 0.5897 - tn: 450.4701 - fn: 0.0000e+00 - accuracy: 0.9995 - precision: 0.9991 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0336 - val_tp: 66.0000 - val_fp: 0.0000e+00 - val_tn: 77.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9851 - val_auc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0015 - tp: 470.7009 - fp: 0.3932 - tn: 472.3162 - fn: 0.4530 - accuracy: 0.9994 - precision: 0.9995 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.0096 - val_tp: 66.0000 - val_fp: 0.0000e+00 - val_tn: 77.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9851 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 8.3130e-04 - tp: 471.9829 - fp: 0.0000e+00 - tn: 471.8803 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0095 - val_tp: 65.0000 - val_fp: 0.0000e+00 - val_tn: 78.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9848 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0034 - tp: 453.8376 - fp: 1.0000 - tn: 488.2821 - fn: 0.7436 - accuracy: 0.9989 - precision: 0.9987 - recall: 0.9991 - auc: 0.9998 - val_loss: 1.2451 - val_tp: 44.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 27.0000 - val_accuracy: 0.8125 - val_precision: 1.0000 - val_recall: 0.6197 - val_auc: 0.9085\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0438 - tp: 465.6154 - fp: 4.2137 - tn: 466.9829 - fn: 7.0513 - accuracy: 0.9877 - precision: 0.9920 - recall: 0.9833 - auc: 0.9966 - val_loss: 1.0850 - val_tp: 71.0000 - val_fp: 26.0000 - val_tn: 47.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8194 - val_precision: 0.7320 - val_recall: 1.0000 - val_auc: 0.8973\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 13s 91ms/step - loss: 0.7032 - tp: 435.0513 - fp: 28.2564 - tn: 449.7009 - fn: 30.8547 - accuracy: 0.8926 - precision: 0.8940 - recall: 0.8826 - auc: 0.9262 - val_loss: 0.5962 - val_tp: 23.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 48.0000 - val_accuracy: 0.6667 - val_precision: 1.0000 - val_recall: 0.3239 - val_auc: 0.9715\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0233 - tp: 476.0769 - fp: 1.1709 - tn: 462.4701 - fn: 4.1453 - accuracy: 0.9914 - precision: 0.9979 - recall: 0.9850 - auc: 0.9997 - val_loss: 0.3704 - val_tp: 70.0000 - val_fp: 24.0000 - val_tn: 47.0000 - val_fn: 3.0000 - val_accuracy: 0.8125 - val_precision: 0.7447 - val_recall: 0.9589 - val_auc: 0.9640\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0051 - tp: 459.3419 - fp: 1.0171 - tn: 482.7436 - fn: 0.7607 - accuracy: 0.9985 - precision: 0.9982 - recall: 0.9988 - auc: 0.9993 - val_loss: 0.3761 - val_tp: 71.0000 - val_fp: 18.0000 - val_tn: 55.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8750 - val_precision: 0.7978 - val_recall: 1.0000 - val_auc: 0.9822\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0224 - tp: 461.8376 - fp: 2.2564 - tn: 476.9573 - fn: 2.8120 - accuracy: 0.9963 - precision: 0.9970 - recall: 0.9955 - auc: 0.9988 - val_loss: 0.0513 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9583 - val_auc: 0.9998\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 82ms/step - loss: 0.0261 - tp: 464.1880 - fp: 1.8291 - tn: 475.9829 - fn: 1.8632 - accuracy: 0.9952 - precision: 0.9948 - recall: 0.9956 - auc: 0.9975 - val_loss: 0.5246 - val_tp: 68.0000 - val_fp: 11.0000 - val_tn: 62.0000 - val_fn: 3.0000 - val_accuracy: 0.9028 - val_precision: 0.8608 - val_recall: 0.9577 - val_auc: 0.9505\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0102 - tp: 468.6154 - fp: 1.6239 - tn: 472.3248 - fn: 1.2991 - accuracy: 0.9975 - precision: 0.9966 - recall: 0.9984 - auc: 0.9995 - val_loss: 0.2984 - val_tp: 68.0000 - val_fp: 3.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9792 - val_precision: 0.9577 - val_recall: 1.0000 - val_auc: 0.9794\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0064 - tp: 464.0171 - fp: 0.9915 - tn: 477.1966 - fn: 1.6581 - accuracy: 0.9969 - precision: 0.9985 - recall: 0.9952 - auc: 0.9996 - val_loss: 0.0976 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9577 - val_auc: 0.9930\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 9.1450e-06 - tp: 477.6496 - fp: 0.0000e+00 - tn: 466.2137 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0049 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 8.7042e-06 - tp: 465.2479 - fp: 0.0000e+00 - tn: 478.6154 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.3470e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 3.3560e-05 - tp: 467.1538 - fp: 0.0000e+00 - tn: 476.7094 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.5630e-05 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 77.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 5.8962e-06 - tp: 468.9060 - fp: 0.0000e+00 - tn: 474.9573 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.5008e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 9.7268e-07 - tp: 471.6410 - fp: 0.0000e+00 - tn: 472.2222 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.4652e-04 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 7.6346e-06 - tp: 473.3419 - fp: 0.0000e+00 - tn: 470.5214 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.1006e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 8.7374e-07 - tp: 463.4359 - fp: 0.0000e+00 - tn: 480.4274 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.1302e-04 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 2.6165e-06 - tp: 480.0000 - fp: 0.0000e+00 - tn: 463.8632 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0010 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 111ms/step - loss: 2.8124 - tp: 377.4444 - fp: 88.5043 - tn: 389.3077 - fn: 88.6068 - accuracy: 0.7339 - precision: 0.7271 - recall: 0.7271 - auc: 0.7503 - val_loss: 0.7641 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.9775\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0198 - tp: 468.2821 - fp: 1.5043 - tn: 472.3932 - fn: 1.6838 - accuracy: 0.9969 - precision: 0.9967 - recall: 0.9971 - auc: 0.9998 - val_loss: 1.0138 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0067 - tp: 468.9573 - fp: 0.6838 - tn: 474.2222 - fn: 0.0000e+00 - accuracy: 0.9995 - precision: 0.9991 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.2748 - val_tp: 71.0000 - val_fp: 71.0000 - val_tn: 2.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0073 - tp: 455.2564 - fp: 0.7949 - tn: 487.6923 - fn: 0.1197 - accuracy: 0.9992 - precision: 0.9985 - recall: 0.9999 - auc: 1.0000 - val_loss: 0.3286 - val_tp: 69.0000 - val_fp: 25.0000 - val_tn: 50.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8264 - val_precision: 0.7340 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0186 - tp: 480.6496 - fp: 2.4444 - tn: 457.7778 - fn: 2.9915 - accuracy: 0.9944 - precision: 0.9954 - recall: 0.9937 - auc: 0.9987 - val_loss: 0.0171 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9861 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0038 - tp: 466.0085 - fp: 1.3932 - tn: 475.7778 - fn: 0.6838 - accuracy: 0.9980 - precision: 0.9971 - recall: 0.9988 - auc: 1.0000 - val_loss: 7.7748e-04 - val_tp: 65.0000 - val_fp: 0.0000e+00 - val_tn: 79.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0043 - tp: 478.3590 - fp: 0.0000e+00 - tn: 464.5385 - fn: 0.9658 - accuracy: 0.9983 - precision: 1.0000 - recall: 0.9965 - auc: 1.0000 - val_loss: 1.3278e-04 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 5.2927e-04 - tp: 465.2222 - fp: 0.0000e+00 - tn: 478.6410 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.7764e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 1.7021e-04 - tp: 456.7521 - fp: 0.0000e+00 - tn: 487.1111 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.9518e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 1.0127e-04 - tp: 474.3590 - fp: 0.0000e+00 - tn: 469.5043 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.2571e-05 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 1.1415e-04 - tp: 477.4530 - fp: 0.0000e+00 - tn: 466.4103 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.4973e-05 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 1.6390e-04 - tp: 466.0769 - fp: 0.0000e+00 - tn: 477.7863 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.4906e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 7.9996e-05 - tp: 477.6239 - fp: 0.0000e+00 - tn: 466.2393 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0011 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 77.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 2.7705e-04 - tp: 464.0342 - fp: 0.0000e+00 - tn: 479.8291 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0831e-05 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 8.7620e-05 - tp: 474.6581 - fp: 0.0000e+00 - tn: 469.2051 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.8197e-06 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "FOLD 15\n",
      "{0: 0.9951612903225806, 1: 1.004885993485342}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 83s 617ms/step - loss: 0.9585 - tp: 439.6838 - fp: 30.0684 - tn: 442.8889 - fn: 31.2222 - accuracy: 0.8944 - precision: 0.8969 - recall: 0.8819 - auc: 0.9145 - val_loss: 3.9771 - val_tp: 73.0000 - val_fp: 71.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5069 - val_precision: 0.5069 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 10s 86ms/step - loss: 0.0286 - tp: 459.8632 - fp: 3.7436 - tn: 476.3248 - fn: 3.9316 - accuracy: 0.9938 - precision: 0.9937 - recall: 0.9937 - auc: 0.9984 - val_loss: 17.9665 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 8s 66ms/step - loss: 0.0543 - tp: 474.1709 - fp: 4.9829 - tn: 460.0855 - fn: 4.6239 - accuracy: 0.9876 - precision: 0.9902 - recall: 0.9859 - auc: 0.9972 - val_loss: 9.3659 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.5068\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0187 - tp: 467.6581 - fp: 1.4359 - tn: 472.8718 - fn: 1.8974 - accuracy: 0.9955 - precision: 0.9972 - recall: 0.9938 - auc: 0.9994 - val_loss: 3.9708 - val_tp: 74.0000 - val_fp: 58.0000 - val_tn: 12.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.5972 - val_precision: 0.5606 - val_recall: 1.0000 - val_auc: 0.6643\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 8.8350e-05 - tp: 474.8034 - fp: 0.0000e+00 - tn: 469.0598 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0709 - val_tp: 66.0000 - val_fp: 4.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9722 - val_precision: 0.9429 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0030 - tp: 466.6154 - fp: 0.3248 - tn: 475.6923 - fn: 1.2308 - accuracy: 0.9987 - precision: 0.9996 - recall: 0.9978 - auc: 1.0000 - val_loss: 0.2453 - val_tp: 70.0000 - val_fp: 6.0000 - val_tn: 68.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9583 - val_precision: 0.9211 - val_recall: 1.0000 - val_auc: 0.9797\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0638 - tp: 471.7350 - fp: 6.3846 - tn: 459.4274 - fn: 6.3162 - accuracy: 0.9879 - precision: 0.9880 - recall: 0.9881 - auc: 0.9938 - val_loss: 0.0877 - val_tp: 65.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 3.0000 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9559 - val_auc: 0.9924\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0324 - tp: 463.1368 - fp: 1.7350 - tn: 477.6410 - fn: 1.3504 - accuracy: 0.9955 - precision: 0.9947 - recall: 0.9961 - auc: 0.9962 - val_loss: 0.0334 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9857 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0012 - tp: 464.2308 - fp: 0.0000e+00 - tn: 479.5214 - fn: 0.1111 - accuracy: 0.9999 - precision: 1.0000 - recall: 0.9999 - auc: 1.0000 - val_loss: 0.0117 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 8s 66ms/step - loss: 0.0014 - tp: 479.2222 - fp: 0.2222 - tn: 464.4188 - fn: 0.0000e+00 - accuracy: 0.9999 - precision: 0.9997 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0028 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0013 - tp: 467.4872 - fp: 0.0000e+00 - tn: 476.3761 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 8.1195e-05 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 8s 66ms/step - loss: 2.5399e-04 - tp: 464.1538 - fp: 0.0000e+00 - tn: 479.7094 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0025e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 7s 64ms/step - loss: 0.0020 - tp: 474.8547 - fp: 0.6667 - tn: 468.0342 - fn: 0.3077 - accuracy: 0.9993 - precision: 0.9990 - recall: 0.9996 - auc: 1.0000 - val_loss: 0.0803 - val_tp: 71.0000 - val_fp: 2.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9726 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 7s 65ms/step - loss: 0.0029 - tp: 456.3504 - fp: 0.2308 - tn: 487.2479 - fn: 0.0342 - accuracy: 0.9998 - precision: 0.9997 - recall: 1.0000 - auc: 0.9999 - val_loss: 6.1396e-04 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 77.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 8s 65ms/step - loss: 0.0177 - tp: 464.2308 - fp: 0.9231 - tn: 478.7094 - fn: 0.0000e+00 - accuracy: 0.9987 - precision: 0.9973 - recall: 1.0000 - auc: 0.9987 - val_loss: 3.4513e-04 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 13s 91ms/step - loss: 2.0635 - tp: 356.6154 - fp: 120.3162 - tn: 347.2051 - fn: 119.7265 - accuracy: 0.6811 - precision: 0.6865 - recall: 0.6868 - auc: 0.7351 - val_loss: 1.1326 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.8060\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.1027 - tp: 448.2393 - fp: 16.8974 - tn: 461.4359 - fn: 17.2906 - accuracy: 0.9636 - precision: 0.9634 - recall: 0.9630 - auc: 0.9932 - val_loss: 2.2546 - val_tp: 70.0000 - val_fp: 74.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4861 - val_precision: 0.4861 - val_recall: 1.0000 - val_auc: 0.9330\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0601 - tp: 458.2479 - fp: 9.2906 - tn: 466.1282 - fn: 10.1966 - accuracy: 0.9778 - precision: 0.9783 - recall: 0.9765 - auc: 0.9980 - val_loss: 2.8086 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.6944\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0332 - tp: 461.6239 - fp: 5.4530 - tn: 471.2821 - fn: 5.5043 - accuracy: 0.9889 - precision: 0.9880 - recall: 0.9891 - auc: 0.9995 - val_loss: 0.0684 - val_tp: 70.0000 - val_fp: 4.0000 - val_tn: 69.0000 - val_fn: 1.0000 - val_accuracy: 0.9653 - val_precision: 0.9459 - val_recall: 0.9859 - val_auc: 0.9986\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0331 - tp: 464.6923 - fp: 5.0171 - tn: 469.1795 - fn: 4.9744 - accuracy: 0.9897 - precision: 0.9891 - recall: 0.9902 - auc: 0.9987 - val_loss: 0.1342 - val_tp: 73.0000 - val_fp: 7.0000 - val_tn: 64.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9514 - val_precision: 0.9125 - val_recall: 1.0000 - val_auc: 0.9999\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 79ms/step - loss: 0.0214 - tp: 470.3419 - fp: 4.7778 - tn: 467.0940 - fn: 1.6496 - accuracy: 0.9914 - precision: 0.9854 - recall: 0.9973 - auc: 0.9998 - val_loss: 1.4219 - val_tp: 70.0000 - val_fp: 49.0000 - val_tn: 25.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.6597 - val_precision: 0.5882 - val_recall: 1.0000 - val_auc: 0.8919\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0094 - tp: 469.8803 - fp: 1.3077 - tn: 472.2222 - fn: 0.4530 - accuracy: 0.9967 - precision: 0.9940 - recall: 0.9995 - auc: 1.0000 - val_loss: 0.0430 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 0.0078 - tp: 472.4872 - fp: 1.1709 - tn: 468.9402 - fn: 1.2650 - accuracy: 0.9982 - precision: 0.9983 - recall: 0.9981 - auc: 1.0000 - val_loss: 0.0791 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 72.0000 - val_fn: 2.0000 - val_accuracy: 0.9792 - val_precision: 0.9857 - val_recall: 0.9718 - val_auc: 0.9927\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 11s 93ms/step - loss: 0.0032 - tp: 467.3333 - fp: 0.0000e+00 - tn: 476.5299 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0725 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 69.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9865 - val_recall: 0.9865 - val_auc: 0.9927\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 17s 144ms/step - loss: 0.0022 - tp: 464.4188 - fp: 0.9402 - tn: 478.5043 - fn: 0.0000e+00 - accuracy: 0.9985 - precision: 0.9968 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.1028 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9861 - val_recall: 0.9861 - val_auc: 0.9919\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 0.0017 - tp: 468.3333 - fp: 0.2479 - tn: 475.2821 - fn: 0.0000e+00 - accuracy: 0.9998 - precision: 0.9997 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.5223 - val_tp: 60.0000 - val_fp: 0.0000e+00 - val_tn: 71.0000 - val_fn: 13.0000 - val_accuracy: 0.9097 - val_precision: 1.0000 - val_recall: 0.8219 - val_auc: 0.9589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 6.3913e-04 - tp: 457.6239 - fp: 0.0000e+00 - tn: 486.2393 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0576 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9857 - val_recall: 0.9857 - val_auc: 0.9930\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 2.1153e-04 - tp: 475.6068 - fp: 0.0000e+00 - tn: 468.2564 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0564 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9857 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 78ms/step - loss: 2.8294e-04 - tp: 476.6154 - fp: 0.0000e+00 - tn: 467.2479 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0647 - val_tp: 71.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 1.0000 - val_accuracy: 0.9861 - val_precision: 0.9861 - val_recall: 0.9861 - val_auc: 0.9994\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 77ms/step - loss: 1.0189e-04 - tp: 484.6838 - fp: 0.0000e+00 - tn: 459.1795 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0553 - val_tp: 73.0000 - val_fp: 1.0000 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9865 - val_recall: 1.0000 - val_auc: 0.9929\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 12s 90ms/step - loss: 1.1408 - tp: 413.0256 - fp: 50.7949 - tn: 423.8291 - fn: 56.2137 - accuracy: 0.8307 - precision: 0.8375 - recall: 0.8304 - auc: 0.8726 - val_loss: 0.7159 - val_tp: 0.0000e+00 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 72.0000 - val_accuracy: 0.5000 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.5975\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0199 - tp: 475.4615 - fp: 2.0855 - tn: 464.7179 - fn: 1.5983 - accuracy: 0.9971 - precision: 0.9967 - recall: 0.9976 - auc: 0.9988 - val_loss: 0.4615 - val_tp: 73.0000 - val_fp: 20.0000 - val_tn: 51.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.8611 - val_precision: 0.7849 - val_recall: 1.0000 - val_auc: 0.9690\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0150 - tp: 471.7009 - fp: 1.7179 - tn: 470.4444 - fn: 0.0000e+00 - accuracy: 0.9980 - precision: 0.9960 - recall: 1.0000 - auc: 0.9990 - val_loss: 0.1194 - val_tp: 67.0000 - val_fp: 4.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9722 - val_precision: 0.9437 - val_recall: 1.0000 - val_auc: 0.9965\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0019 - tp: 467.5897 - fp: 0.0000e+00 - tn: 476.0598 - fn: 0.2137 - accuracy: 0.9999 - precision: 1.0000 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.0282 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9857 - val_recall: 1.0000 - val_auc: 0.9999\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 9s 82ms/step - loss: 7.6641e-04 - tp: 472.4103 - fp: 0.0000e+00 - tn: 471.4530 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0134 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 2.5428e-04 - tp: 468.9829 - fp: 0.0000e+00 - tn: 474.8803 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.3285e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 10s 82ms/step - loss: 2.4932e-04 - tp: 468.2821 - fp: 0.0000e+00 - tn: 475.5812 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0138 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 9s 80ms/step - loss: 0.0074 - tp: 474.0427 - fp: 2.0684 - tn: 465.8120 - fn: 1.9402 - accuracy: 0.9972 - precision: 0.9971 - recall: 0.9974 - auc: 0.9998 - val_loss: 0.6464 - val_tp: 71.0000 - val_fp: 7.0000 - val_tn: 66.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9514 - val_precision: 0.9103 - val_recall: 1.0000 - val_auc: 0.9863\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 9s 82ms/step - loss: 0.0441 - tp: 473.9915 - fp: 2.7009 - tn: 467.1026 - fn: 0.0684 - accuracy: 0.9967 - precision: 0.9936 - recall: 0.9999 - auc: 0.9981 - val_loss: 0.0463 - val_tp: 69.0000 - val_fp: 1.0000 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9857 - val_recall: 1.0000 - val_auc: 0.9933\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0055 - tp: 472.7179 - fp: 1.3419 - tn: 469.5128 - fn: 0.2906 - accuracy: 0.9967 - precision: 0.9940 - recall: 0.9996 - auc: 0.9998 - val_loss: 0.0107 - val_tp: 70.0000 - val_fp: 2.0000 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9861 - val_precision: 0.9722 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0015 - tp: 471.8974 - fp: 0.0000e+00 - tn: 471.7521 - fn: 0.2137 - accuracy: 0.9999 - precision: 1.0000 - recall: 0.9997 - auc: 1.0000 - val_loss: 0.1313 - val_tp: 72.0000 - val_fp: 1.0000 - val_tn: 71.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9863 - val_recall: 1.0000 - val_auc: 0.9931\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0461 - tp: 467.5897 - fp: 3.3590 - tn: 469.8120 - fn: 3.1026 - accuracy: 0.9934 - precision: 0.9927 - recall: 0.9940 - auc: 0.9980 - val_loss: 0.0200 - val_tp: 70.0000 - val_fp: 1.0000 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.9931 - val_precision: 0.9859 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 0.0261 - tp: 476.5214 - fp: 3.4701 - tn: 460.9744 - fn: 2.8974 - accuracy: 0.9904 - precision: 0.9913 - recall: 0.9897 - auc: 0.9998 - val_loss: 0.0102 - val_tp: 73.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 1.0000 - val_accuracy: 0.9931 - val_precision: 1.0000 - val_recall: 0.9865 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 9s 82ms/step - loss: 0.0030 - tp: 461.1197 - fp: 0.5299 - tn: 481.2393 - fn: 0.9744 - accuracy: 0.9982 - precision: 0.9993 - recall: 0.9969 - auc: 1.0000 - val_loss: 2.2608e-04 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 9s 81ms/step - loss: 8.6357e-04 - tp: 477.3932 - fp: 0.0000e+00 - tn: 466.4701 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 3.5000e-04 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 128, 128, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 128, 128, 8)  80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 128, 128, 8)  80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 128, 128, 8)  80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 128, 128, 8)  32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128, 128, 8)  32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128, 128, 8)  32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 128, 128, 8)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 128, 128, 8)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 128, 128, 8)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 128, 128, 8)  584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 128, 128, 8)  584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 128, 128, 8)  584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128, 128, 8)  32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128, 128, 8)  32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 8)  32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 128, 128, 8)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 128, 128, 8)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 128, 128, 8)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 64, 64, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 64, 64, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 64, 64, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, 64, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, 64, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, 64, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, 64, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, 64, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, 64, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, 64, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 32, 32, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 32, 32, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 32)   0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8192)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8192)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8192)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2097408     add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,153,129\n",
      "Trainable params: 2,152,457\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n",
      "GLOBAL BATCH SIZE:  16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "Epoch 1/15\n",
      "116/116 [==============================] - 16s 110ms/step - loss: 2.7788 - tp: 403.8718 - fp: 65.9487 - tn: 399.1880 - fn: 74.8547 - accuracy: 0.7822 - precision: 0.7972 - recall: 0.7799 - auc: 0.8196 - val_loss: 1.4390 - val_tp: 71.0000 - val_fp: 73.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.4931 - val_precision: 0.4931 - val_recall: 1.0000 - val_auc: 0.6679\n",
      "Epoch 2/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0234 - tp: 466.0000 - fp: 5.3590 - tn: 468.7179 - fn: 3.7863 - accuracy: 0.9898 - precision: 0.9880 - recall: 0.9916 - auc: 0.9996 - val_loss: 4.2376 - val_tp: 72.0000 - val_fp: 72.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5000 - val_precision: 0.5000 - val_recall: 1.0000 - val_auc: 0.5000\n",
      "Epoch 3/15\n",
      "116/116 [==============================] - 12s 102ms/step - loss: 0.0326 - tp: 464.6923 - fp: 4.8376 - tn: 470.6154 - fn: 3.7179 - accuracy: 0.9907 - precision: 0.9895 - recall: 0.9916 - auc: 0.9973 - val_loss: 2.3711 - val_tp: 75.0000 - val_fp: 69.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.5208 - val_precision: 0.5208 - val_recall: 1.0000 - val_auc: 0.7826\n",
      "Epoch 4/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0043 - tp: 471.5214 - fp: 1.8205 - tn: 469.5214 - fn: 1.0000 - accuracy: 0.9976 - precision: 0.9969 - recall: 0.9983 - auc: 0.9999 - val_loss: 0.5486 - val_tp: 71.0000 - val_fp: 29.0000 - val_tn: 44.0000 - val_fn: 0.0000e+00 - val_accuracy: 0.7986 - val_precision: 0.7100 - val_recall: 1.0000 - val_auc: 0.9932\n",
      "Epoch 5/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0032 - tp: 465.2393 - fp: 0.0000e+00 - tn: 478.6239 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 0.0175 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 6/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0012 - tp: 465.5470 - fp: 0.0000e+00 - tn: 478.3162 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 2.3822e-04 - val_tp: 71.0000 - val_fp: 0.0000e+00 - val_tn: 73.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 7/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 2.4496e-04 - tp: 469.1880 - fp: 0.0000e+00 - tn: 474.6752 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0844e-05 - val_tp: 67.0000 - val_fp: 0.0000e+00 - val_tn: 77.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 8/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 5.5737e-04 - tp: 479.1453 - fp: 0.5556 - tn: 464.1624 - fn: 0.0000e+00 - accuracy: 0.9996 - precision: 0.9992 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.6829e-04 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 9/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0063 - tp: 474.1709 - fp: 1.1624 - tn: 466.9402 - fn: 1.5897 - accuracy: 0.9970 - precision: 0.9979 - recall: 0.9962 - auc: 1.0000 - val_loss: 1.6627e-05 - val_tp: 68.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 10/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 3.1956e-04 - tp: 474.9231 - fp: 0.0000e+00 - tn: 468.9402 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 5.9419e-07 - val_tp: 69.0000 - val_fp: 0.0000e+00 - val_tn: 75.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 11/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 9.1947e-05 - tp: 470.3419 - fp: 0.0000e+00 - tn: 473.5214 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 1.0211e-06 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 12/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 2.2047e-04 - tp: 472.2222 - fp: 0.0000e+00 - tn: 471.6410 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 7.6561e-07 - val_tp: 70.0000 - val_fp: 0.0000e+00 - val_tn: 74.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 13/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 1.7321e-04 - tp: 458.4188 - fp: 0.0000e+00 - tn: 485.4444 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 - val_loss: 4.5919e-06 - val_tp: 74.0000 - val_fp: 0.0000e+00 - val_tn: 70.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n",
      "Epoch 14/15\n",
      "116/116 [==============================] - 12s 101ms/step - loss: 0.0149 - tp: 458.1453 - fp: 3.4786 - tn: 480.4444 - fn: 1.7949 - accuracy: 0.9954 - precision: 0.9931 - recall: 0.9974 - auc: 0.9994 - val_loss: 2.2774 - val_tp: 1.0000 - val_fp: 0.0000e+00 - val_tn: 76.0000 - val_fn: 67.0000 - val_accuracy: 0.5347 - val_precision: 1.0000 - val_recall: 0.0147 - val_auc: 0.8088\n",
      "Epoch 15/15\n",
      "116/116 [==============================] - 12s 100ms/step - loss: 0.0172 - tp: 472.9744 - fp: 2.0855 - tn: 467.0085 - fn: 1.7949 - accuracy: 0.9967 - precision: 0.9963 - recall: 0.9972 - auc: 0.9994 - val_loss: 4.3236e-04 - val_tp: 72.0000 - val_fp: 0.0000e+00 - val_tn: 72.0000 - val_fn: 0.0000e+00 - val_accuracy: 1.0000 - val_precision: 1.0000 - val_recall: 1.0000 - val_auc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_B/128'\n",
    "\n",
    "history = []\n",
    "for fold in range(1, 16):\n",
    "    print(\"FOLD {}\".format(fold))\n",
    "    csv_path = os.path.join(ROOT_DIR, \"csv_F15/train_test_fold_{}/csv\".format(fold))\n",
    "    model_save_path = os.path.join(ROOT_DIR, \"model_save_dir_F15/train_test_fold_{}\".format(fold))\n",
    "    tfrecords_path = os.path.join(ROOT_DIR, 'tfrecords_F15/tfrecords_fold_{}'.format(fold))\n",
    "\n",
    "    h = train(\n",
    "        csv_path,\n",
    "        model_save_path,\n",
    "        tfrecords_path,\n",
    "        volume_shape=(128, 128, 128),\n",
    "        image_size=(128, 128),\n",
    "        mode='CV'\n",
    "    )\n",
    "    \n",
    "    history.append(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json, re\n",
    "\n",
    "ROOT_DIR = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_B/128'\n",
    "\n",
    "plane = 'combined'\n",
    "\n",
    "files = glob(ROOT_DIR + '/model_save_dir_F15/train_test_fold*/metrics/' + plane + '*' )\n",
    "all_metrics = {}\n",
    "\n",
    "for file in files:\n",
    "    fold = file.split('/')[-3].split('fold_')[-1]\n",
    "    all_metrics[int(fold)] = json.load(open(file))\n",
    "    \n",
    "# print(all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_folds = 15\n",
    "train_acc_arr = []\n",
    "val_acc_arr = []\n",
    "train_loss_arr = []\n",
    "val_loss_arr = []\n",
    "\n",
    "# print(all_metrics[1].keys())\n",
    "\n",
    "def get_metrics_arr(all_metrics, metric='accuracy', n_folds=15):\n",
    "    arr = []\n",
    "    for i in range(1, n_folds+1):\n",
    "        metrics = all_metrics[i]\n",
    "        temp = list(range(n_folds))\n",
    "        for j in range(len(temp)):\n",
    "            temp[j] = metrics[metric][str(j)]\n",
    "\n",
    "        arr.append(temp)  \n",
    "    return np.array(arr)\n",
    "\n",
    "# train_acc_arr = get_metrics_arr(all_metrics)\n",
    "# val_acc_arr = get_metrics_arr(all_metrics, metric='val_accuracy')\n",
    "# train_loss_arr = get_metrics_arr(all_metrics, metric='loss')\n",
    "# val_loss_arr = get_metrics_arr(all_metrics, metric='val_loss')\n",
    "\n",
    "# print(train_loss_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install pandas seaborn scipy tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>tag</th>\n",
       "      <th>step</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>0</td>\n",
       "      <td>0.959052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>3</td>\n",
       "      <td>0.989224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_test_fold_1/tb_logs/axial/train</td>\n",
       "      <td>epoch_accuracy</td>\n",
       "      <td>4</td>\n",
       "      <td>0.990840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16195</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>10</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16196</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>11</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16197</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>12</td>\n",
       "      <td>65.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16198</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>13</td>\n",
       "      <td>68.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16199</th>\n",
       "      <td>train_test_fold_9/tb_logs/sagittal/validation</td>\n",
       "      <td>epoch_tp</td>\n",
       "      <td>14</td>\n",
       "      <td>72.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 run             tag  step  \\\n",
       "0              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     0   \n",
       "1              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     1   \n",
       "2              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     2   \n",
       "3              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     3   \n",
       "4              train_test_fold_1/tb_logs/axial/train  epoch_accuracy     4   \n",
       "...                                              ...             ...   ...   \n",
       "16195  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp    10   \n",
       "16196  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp    11   \n",
       "16197  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp    12   \n",
       "16198  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp    13   \n",
       "16199  train_test_fold_9/tb_logs/sagittal/validation        epoch_tp    14   \n",
       "\n",
       "           value  \n",
       "0       0.959052  \n",
       "1       0.994612  \n",
       "2       0.998922  \n",
       "3       0.989224  \n",
       "4       0.990840  \n",
       "...          ...  \n",
       "16195  71.000000  \n",
       "16196  71.000000  \n",
       "16197  65.000000  \n",
       "16198  68.000000  \n",
       "16199  72.000000  \n",
       "\n",
       "[16200 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import tensorboard as tb\n",
    "\n",
    "# Cloud tensorboard at https://tensorboard.dev/experiment/sl5VNFZYSJecLEwKlDDw4w/\n",
    "\n",
    "experiment_id = \"sl5VNFZYSJecLEwKlDDw4w\"\n",
    "experiment = tb.data.experimental.ExperimentFromDev(experiment_id)\n",
    "df = experiment.get_scalars()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Metrics:  ['epoch_accuracy' 'epoch_auc' 'epoch_fn' 'epoch_fp' 'epoch_loss'\n",
      " 'epoch_precision' 'epoch_recall' 'epoch_tn' 'epoch_tp']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Available Metrics: \", df['tag'].unique())\n",
    "\n",
    "def get_metrics(df, metric='epoch_accuracy', n_epochs = 15, n_folds=15, plane='combined', run='train'):\n",
    "    \n",
    "    temp = df[df[\"run\"].str.contains(plane + '/' + run)]\n",
    "    m = temp[temp[\"tag\"].str.contains(metric)]\n",
    "\n",
    "    arr = []\n",
    "    for i in range(1, n_epochs+1):\n",
    "        fold_i = m[m[\"run\"].str.match('train_test_fold_' + str(i) + '/')]\n",
    "        arr.append(fold_i['value'].tolist())\n",
    "    \n",
    "    return np.array(arr)\n",
    "\n",
    "train_acc_arr = get_metrics(df)\n",
    "val_acc_arr = get_metrics(df, metric='epoch_accuracy', run='validation')\n",
    "train_loss_arr = get_metrics(df, metric='epoch_loss', run='train')\n",
    "val_loss_arr = get_metrics(df, metric='epoch_loss', run='validation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f2c2142f588>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_folds = 15\n",
    "\n",
    "tr_acc_fold_mean = np.mean(train_acc_arr, axis=1)\n",
    "tr_acc_fold_std = np.std(train_acc_arr, axis=1)\n",
    "\n",
    "val_acc_fold_mean = np.mean(val_acc_arr, axis=1)\n",
    "val_acc_fold_std = np.std(val_acc_arr, axis=1)\n",
    "\n",
    "tr_loss_fold_mean = np.mean(train_loss_arr, axis=1)\n",
    "tr_loss_fold_std = np.std(train_loss_arr, axis=1)\n",
    "\n",
    "val_loss_fold_mean = np.mean(val_loss_arr, axis=1)\n",
    "val_loss_fold_std = np.std(val_loss_arr, axis=1)\n",
    "\n",
    "\n",
    "_, axes = plt.subplots(2, 2, figsize=(20, 8))\n",
    "\n",
    "axes[0, 0].grid()\n",
    "axes[0, 1].grid()\n",
    "\n",
    "axes[0, 0].fill_between(range(1, n_folds+1), tr_acc_fold_mean - tr_acc_fold_std,\n",
    "                         tr_acc_fold_mean + tr_acc_fold_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "axes[0, 1].fill_between(range(1, n_folds+1), val_acc_fold_mean - val_acc_fold_std,\n",
    "                         val_acc_fold_mean + val_acc_fold_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "axes[0, 0].plot(range(1, n_folds+1), tr_acc_fold_mean, 'o-', color=\"g\",\n",
    "                 label=\"Training accuracy\")\n",
    "\n",
    "axes[0, 1].plot(range(1, n_folds+1), val_acc_fold_mean, 'o-', color=\"r\",\n",
    "                 label=\"Validation accuracy\")\n",
    "\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "\n",
    "axes[0, 0].legend(loc=\"best\")\n",
    "axes[0, 1].legend(loc=\"best\")\n",
    "\n",
    "\n",
    "axes[1, 0].grid()\n",
    "axes[1, 1].grid()\n",
    "axes[1, 0].fill_between(range(1, n_folds+1), tr_loss_fold_mean - tr_loss_fold_std,\n",
    "                         tr_loss_fold_mean + tr_loss_fold_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "axes[1, 1].fill_between(range(1, n_folds+1), val_loss_fold_mean - val_loss_fold_std,\n",
    "                         val_loss_fold_mean + val_loss_fold_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "axes[1, 0].plot(range(1, n_folds+1), tr_loss_fold_mean, 'o-', color=\"g\",\n",
    "                 label=\"Training loss\")\n",
    "\n",
    "axes[1, 1].plot(range(1, n_folds+1), val_loss_fold_mean, 'o-', color=\"r\",\n",
    "                 label=\"Validation loss\")\n",
    "\n",
    "axes[1, 0].legend(loc=\"best\")\n",
    "axes[1, 1].legend(loc=\"best\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard graph visualization\n",
    "\n",
    "NOTE: For this to work, you need to have the 6006 port binded from the localhost as shown in the first block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_A/model_save_dir_F15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.9934934934934935, 1: 1.0065922920892494}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "125\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "axial (InputLayer)           [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 128, 128, 8)       80        \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 128, 128, 8)       32        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128, 128, 8)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 128, 128, 8)       584       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128, 128, 8)       32        \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128, 128, 8)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 64, 64, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 64, 64, 16)        1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 64, 64, 16)        2320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "output_node (Dense)          (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 2,116,153\n",
      "Trainable params: 2,115,929\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "None\n",
      "Steps/Epoch:  125\n",
      "Epoch 1/15\n",
      "125/125 [==============================] - 10s 53ms/step - loss: 0.6723 - tp: 459.7222 - fp: 33.0079 - tn: 485.8651 - fn: 37.2778 - accuracy: 0.8813 - precision: 0.8787 - recall: 0.8656 - auc: 0.9232\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.0169 - tp: 499.3492 - fp: 3.9524 - tn: 510.2302 - fn: 2.3413 - accuracy: 0.9948 - precision: 0.9935 - recall: 0.9960 - auc: 0.9989\n",
      "Epoch 3/15\n",
      "  1/125 [..............................] - ETA: 3s - loss: 0.0060 - tp: 9.0000 - fp: 0.0000e+00 - tn: 7.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0bc6cdc65c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mvolume_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'full'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-3-c808046fe672>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(csv_path, model_save_path, tfrecords_path, volume_shape, image_size, dropout, batch_size, n_classes, n_epochs, percent, mode)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtbCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             )\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 _r=1):\n\u001b[1;32m   1133\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    844\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2992\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2993\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2994\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2996\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1937\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1939\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1940\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1941\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    567\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    570\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ROOTDIR = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_B/128'\n",
    "\n",
    "csv_path = os.path.join(ROOTDIR, \"csv_full\")\n",
    "model_save_path = os.path.join(ROOTDIR, \"model_save_dir_full_local\")\n",
    "tfrecords_path = os.path.join(ROOTDIR, 'tfrecords_full')\n",
    "\n",
    "\n",
    "history = train(\n",
    "    csv_path,\n",
    "    model_save_path,\n",
    "    tfrecords_path,\n",
    "    volume_shape=(128, 128, 128),\n",
    "    image_size=(128, 128),\n",
    "    mode='full'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "history_dict.keys()\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "loss = history_dict['loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "plt.title('Training loss and accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('../defacing')\n",
    "from models.modelN import CombinedClassifier\n",
    "\n",
    "# Tf packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "## Hard-coded directory paths\n",
    "ROOTDIR_B = '/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_B/128'\n",
    "ROOTDIR_A = '/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_A/128'\n",
    "\n",
    "csv_path = os.path.join(ROOTDIR, \"csv_full\")\n",
    "model_save_path = os.path.join(ROOTDIR_B, \"model_save_dir_full\")\n",
    "tfrecords_path = os.path.join(ROOTDIR_B, 'tfrecords_full')\n",
    "weights_path = os.path.join(model_save_path, 'weights/combined/best-wts.h5')\n",
    "\n",
    "planes = ['axial', 'coronal', 'sagittal']\n",
    "for plane in planes:\n",
    "    model = modelN.Submodel(\n",
    "        input_shape=image_size,\n",
    "        dropout=dropout,\n",
    "        name=plane,\n",
    "        include_top=True,\n",
    "         weights=None)\n",
    "    \n",
    "    model.load_weights(os.path.join(weights_path, ))\n",
    "    \n",
    "    \n",
    "    \n",
    "model = CombinedClassifier(\n",
    "    input_shape=(128, 128), dropout=0.4, wts_root=None, trainable=True\n",
    ")\n",
    "model.load_weights(os.path.abspath(weights_path))\n",
    "\n",
    "\n",
    "dataset_test = get_dataset(\n",
    "    file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "    n_classes=2,\n",
    "    n_slices = 24,\n",
    "    batch_size=16,\n",
    "    volume_shape=(128, 128, 128),\n",
    "    plane='combined',\n",
    "    mode='test'\n",
    ")\n",
    "\n",
    "METRICS = [\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            metrics.Precision(name=\"precision\"),\n",
    "            metrics.Recall(name=\"recall\"),\n",
    "            metrics.AUC(name=\"auc\"),\n",
    "        ]\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    metrics=METRICS,\n",
    ")\n",
    "\n",
    "    \n",
    "results = model.evaluate(dataset_test)\n",
    "predictions = (model.predict(dataset_test) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_test)\n",
    "import matplotlib.pyplot as plt\n",
    "corr_pred_map = {}\n",
    "\n",
    "# for x, y in dataset_test.enumerate():\n",
    "for x, y in dataset_test.as_numpy_iterator():\n",
    "    \n",
    "    batch_predictions = (model.predict(x) > 0.5).astype(int)\n",
    "    all_imgs = []\n",
    "    for i in range(len(batch_predictions)):\n",
    "        if batch_predictions.flatten()[i] != y.flatten()[i].astype(int):\n",
    "            print(\"Predicted: \",batch_predictions.flatten()[i], \"Actual: \", y.flatten()[i].astype(int))\n",
    "            \n",
    "            fig = plt.figure(figsize=(25, 8))\n",
    "            rows, cols = 3, 16\n",
    "            use = x['axial']\n",
    "            for i in range(1, cols*rows + 1):\n",
    "                if i/cols == 1:\n",
    "                    use = x['coronal']\n",
    "                if i/cols == 2:\n",
    "                    use = x['sagittal']\n",
    "                    \n",
    "                fig.add_subplot(rows, cols, i)\n",
    "                \n",
    "                plt.imshow(use[(i-1)%cols,:,:, 0])\n",
    "\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IXI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "\n",
    "ixi_root_dir = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/conformed/ixi/face/128'\n",
    "ixi_deface_root_dir = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/conformed/ixi/deface/128'\n",
    "cpaths = glob.glob(os.path.join(ixi_root_dir, '*.nii*'))\n",
    "cpaths_deface = glob.glob(os.path.join(ixi_deface_root_dir, '*.nii*'))\n",
    "\n",
    "# true_labels = pd.read_csv(os.path.join(test_root_dir, 'test1_images_gt.csv'))\n",
    "# tl_dict = {}\n",
    "# for i, row in enumerate(true_labels.values):\n",
    "    \n",
    "#     dataset, volume, label = row\n",
    "#     l = 0 if 'deface' in label else 1\n",
    "#     tl_dict[str(dataset) + '/' + str(volume)] = l\n",
    "    \n",
    "test_df_dict = {}\n",
    "X, Y = [], []\n",
    "\n",
    "for path in cpaths:\n",
    "#     print(path)\n",
    "    name = '/'.join(path.rsplit('/', 2)[-2:])\n",
    "    X.append(path)\n",
    "    Y.append(1)\n",
    "    \n",
    "for path in cpaths_deface:\n",
    "#     print(path)\n",
    "    name = '/'.join(path.rsplit('/', 2)[-2:])\n",
    "    X.append(path)\n",
    "    Y.append(0)\n",
    "\n",
    "\n",
    "# print(X, Y)\n",
    "import random\n",
    "\n",
    "zipped = list(zip(X, Y))\n",
    "random.shuffle(zipped)\n",
    "_X, _Y = zip(*zipped)\n",
    "\n",
    "test_df_dict['X'] = _X\n",
    "test_df_dict['Y'] = _Y\n",
    "\n",
    "df_test = pd.DataFrame(data=test_df_dict)\n",
    "df_test.to_csv(os.path.join(\"/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi/csv/testing.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert test data to tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "test_root_dir = \"/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi\"\n",
    "csv_path = os.path.join(test_root_dir, \"csv\")\n",
    "tf_records_dir = os.path.join(test_root_dir, \"tfrecords\")\n",
    "\n",
    "os.makedirs(tf_records_dir, exist_ok=True)\n",
    "\n",
    "test_csv_path = os.path.join(csv_path, \"testing.csv\")\n",
    "test_paths = pd.read_csv(test_csv_path)[\"X\"].values\n",
    "test_labels = pd.read_csv(test_csv_path)[\"Y\"].values\n",
    "test_D = list(zip(test_paths, test_labels))\n",
    "test_write_path = os.path.join(tf_records_dir, 'data-test_shard-{shard:03d}.tfrec')\n",
    "\n",
    "nobrainer.tfrecord.write(\n",
    "    features_labels=test_D,\n",
    "    filename_template=test_write_path,\n",
    "    examples_per_shard=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root_dir = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi'\n",
    "model_save_path = os.path.join(ROOTDIR_B, \"model_save_dir_full\")\n",
    "tfrecords_path = os.path.join(test_root_dir, \"tfrecords\")\n",
    "plane = \"axial\"\n",
    "dataset_plane = get_dataset(\n",
    "        file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "        n_classes=2,\n",
    "        batch_size=16,\n",
    "        volume_shape=(128, 128, 128),\n",
    "        plane=plane,\n",
    "        mode='test'\n",
    "    )\n",
    "\n",
    "print(dataset_plane)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('../defacing')\n",
    "from models.modelN import CombinedClassifier\n",
    "import glob\n",
    "\n",
    "# Tf packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "ROOTDIR_B = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_B/128'\n",
    "ROOTDIR_A = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/experiments/experiment_A/128'\n",
    "test_root_dir = '/tf/shank/HDDLinux/Stanford/data/mriqc-shared/test_ixi'\n",
    "\n",
    "model_save_path = os.path.join(ROOTDIR_B, \"model_save_dir_full\")\n",
    "tfrecords_path = os.path.join(test_root_dir, \"tfrecords\")\n",
    "print(\"TFRECORDS: \", tfrecords_path)\n",
    "weights_path = os.path.join(model_save_path, 'weights/combined/best-wts.h5')\n",
    "\n",
    "planes = ['coronal'] #, 'coronal', 'sagittal']\n",
    "for plane in planes:\n",
    "    \n",
    "    model = modelN.Submodel(\n",
    "        input_shape=(128, 128),\n",
    "        dropout=0.2,\n",
    "        name=plane,\n",
    "        include_top=True,\n",
    "        weights=None,\n",
    "        trainable=False,\n",
    "    )\n",
    "    \n",
    "    print(os.path.join(model_save_path, plane, 'best-wts.h5'))\n",
    "    \n",
    "    model.load_weights(os.path.join(model_save_path, 'weights', plane, 'best-wts.h5'))\n",
    "    \n",
    "    dataset_plane = get_dataset(\n",
    "        file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "        n_classes=2,\n",
    "        batch_size=16,\n",
    "        volume_shape=(128, 128, 128),\n",
    "        plane=plane,\n",
    "        mode='test',)\n",
    "    \n",
    "    METRICS = [\n",
    "        metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "        metrics.Precision(name=\"precision\"),\n",
    "        metrics.Recall(name=\"recall\"),\n",
    "        metrics.AUC(name=\"auc\"),\n",
    "    ]\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.binary_crossentropy,\n",
    "        optimizer=Adam(learning_rate=1e-3),\n",
    "        metrics=METRICS,\n",
    "    )\n",
    "    \n",
    "#     results = model.evaluate(dataset_plane, batch_size=16)\n",
    "    predictions = (model.predict(dataset_plane) > 0.5).astype(int)\n",
    "    \n",
    "# model = CombinedClassifier(\n",
    "#     input_shape=(128, 128), dropout=0.4, wts_root=None, trainable=True\n",
    "# )\n",
    "# model.load_weights(os.path.abspath(weights_path))\n",
    "\n",
    "# print(os.path.join(tfrecords_path, \"data-test_*\"))\n",
    "\n",
    "# dataset_test = get_dataset(\n",
    "#     file_pattern=os.path.join(tfrecords_path, \"data-test_*\"),\n",
    "#     n_classes=2,\n",
    "# #     n_slices = 24,\n",
    "#     batch_size=16,\n",
    "#     volume_shape=(128, 128, 128),\n",
    "#     plane='combined',\n",
    "#     mode='test'\n",
    "# )\n",
    "\n",
    "# print(dataset_test)\n",
    "\n",
    "# METRICS = [\n",
    "#             metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "#             metrics.Precision(name=\"precision\"),\n",
    "#             metrics.Recall(name=\"recall\"),\n",
    "#             metrics.AUC(name=\"auc\"),\n",
    "#         ]\n",
    "\n",
    "# model.compile(\n",
    "#     loss=tf.keras.losses.binary_crossentropy,\n",
    "#     optimizer=Adam(learning_rate=1e-3),\n",
    "#     metrics=METRICS,\n",
    "# )\n",
    "\n",
    "    \n",
    "# results = model.evaluate(dataset_test, batch_size=16)\n",
    "# predictions = (model.predict(dataset_test) > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(predictions.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "corr_pred_map = {}\n",
    "corr = 0\n",
    "incorr = 0\n",
    "for x, y in dataset_plane.as_numpy_iterator():\n",
    "    \n",
    "    batch_predictions = (model.predict(x) > 0.5).astype(int)\n",
    "    all_imgs = []\n",
    "    for i in range(len(batch_predictions)):\n",
    "        if batch_predictions.flatten()[i] != y.flatten()[i].astype(int):\n",
    "            incorr += 1\n",
    "            print(\"Predicted: \",batch_predictions.flatten()[i], \"Actual: \", y.flatten()[i].astype(int))\n",
    "        else:\n",
    "            corr += 1\n",
    "            \n",
    "#             fig = plt.figure(figsize=(25, 8))\n",
    "#             rows, cols = 3, 16\n",
    "            \n",
    "#             for i in range(1, cols*rows + 1):\n",
    "# #                 if i/cols == 1:\n",
    "# #                     use = x['coronal']\n",
    "# #                 if i/cols == 2:\n",
    "# #                     use = x['sagittal']\n",
    "                    \n",
    "#                 fig.add_subplot(rows, cols, i)\n",
    "                \n",
    "#                 plt.imshow(use[(i-1)%cols,:,:, 0])\n",
    "\n",
    "\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr, incorr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
