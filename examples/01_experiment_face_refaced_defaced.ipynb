{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run this notebook\n",
    "\n",
    "1. Open 2 terminal tabs\n",
    "2. From Terminal A, ssh into TACC the normal way - `ssh <username>@maverick2.tacc.utexas.edu`\n",
    "3. From terminal B, ssh into TACC using the command - `ssh -L localhost:8888:127.0.0.1:8888 -L localhost:6006:localhost:6006 sbansal6@maverick2.tacc.utexas.edu`\n",
    "4. From terminal A, request some resources on the GPU compute resource - `idev -p gtx -L work -m 180`\n",
    "5. From terminal B, log into the compute resource provisioned using - `ssh -L localhost:8888:127.0.0.1:8888 -L localhost:6006:localhost:6006 <resource-name>`\n",
    "6. From terminal B, run jupyter notebook - `jupyter lab --port 8888`\n",
    "7. You should be able to open the jupyter notebook on the browser of your local computer.\n",
    "\n",
    "# Assumptions\n",
    "\n",
    "Following are the requirements:\n",
    "\n",
    "Packages:\n",
    "\n",
    "```\n",
    "tensorflow                         2.1.0\n",
    "tensorflow-estimator               2.1.0\n",
    "tensorflow-gpu                     2.1.0\n",
    "tensorflow-probability             0.9.0\n",
    "tensorboard                        2.3.0\n",
    "```\n",
    "\n",
    "Modules:\n",
    "\n",
    "`module load intel/17.0.4 python3/3.6.3 cuda/10.0 cudnn/7.6.2 nccl/2.4.7 tacc-singularity/3.4.2`\n",
    "\n",
    "In addition to the above module you might need to add the following to your path:\n",
    "\n",
    "`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/apps/cuda/10.1/targets/x86_64-linux/lib`\n",
    "\n",
    "\n",
    "Because of some missing library issues in TACC and depending on the missing library you might need to find and add the library somewhere accessible and add that path to the `LD_LIBRARY_PATH`:\n",
    "\n",
    "```\n",
    "login1.maverick2(1006)$ find -O3 -L /opt/apps/ -name \"*libcupti.so.10.1*\"\n",
    "/opt/apps/cuda/10.1/extras/CUPTI/lib64/libcupti.so.10.1\n",
    "/opt/apps/cuda/10.1/extras/CUPTI/lib64/libcupti.so.10.1.208\n",
    "\n",
    "login1.maverick2(1006)$ cp /opt/apps/cuda/10.1/extras/CUPTI/lib64/libcupti.so.10.1 $WORK/libcupti.so.10.1\n",
    "login1.maverick2(1006)$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$WORK\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Details\n",
    "\n",
    "There are 2 datasets - \n",
    "\n",
    "**A**: orig_faced + defaced volumes\n",
    "\n",
    "**B**: refaced + defaced volumes\n",
    "\n",
    "\n",
    "1. Randomly select 49 volumes from faced and refaced datasets. \n",
    "2. Generate a new dataset T_A, with these 49 faced + the corresponding 49 defaced, and T_B with the 49 defaced + 49 re-faced.\n",
    "3. Put T_A and T_B aside\n",
    "4. Run your 15-fold CV on the remainder:\n",
    "\n",
    "**A_2 (N = 1000x2)**: 1000 faced + 1000 defaced\n",
    "\n",
    "**B_2 (N = 1000x2)**: 1000 defaced + 1000 re-faced\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1 and Step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 98\n",
      "1985 99\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle\n",
    "\n",
    "# Define paths\n",
    "ROOT_DIR = '/work/06850/sbansal6/maverick2/mriqc-shared/conformed'\n",
    "\n",
    "face_path = os.path.join(ROOT_DIR, 'face')\n",
    "defaced_path = os.path.join(ROOT_DIR, 'face_defaced')\n",
    "refaced_path = os.path.join(ROOT_DIR, 'face_refaced')\n",
    "\n",
    "paths_d = []\n",
    "paths_f = []\n",
    "paths_r = []\n",
    "\n",
    "for path in glob(defaced_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_d.append(path)\n",
    "    \n",
    "for path in glob(refaced_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_r.append(path)\n",
    "    \n",
    "for path in glob(face_path + \"/*/*.nii*\"):\n",
    "    DS = path.split('/')[-2]\n",
    "    paths_f.append(path)\n",
    "    \n",
    "\n",
    "def generate_datasets(fpaths, dpaths, size, typ ='faced'):\n",
    "    \n",
    "    if typ not in ['faced', 'refaced']:\n",
    "        print(\"Incorrect value for t. Choose from [faced, refaced]\")\n",
    "        return\n",
    "    \n",
    "    random.shuffle(fpaths)\n",
    "    test_f = fpaths[:size]\n",
    "    main_f = fpaths[size:]\n",
    "\n",
    "    test_d = []\n",
    "    for t in test_f:\n",
    "        if typ == 'faced':\n",
    "            test_d.append(t.replace('face', 'face_defaced'))\n",
    "        \n",
    "        if typ == 'refaced':\n",
    "            DS = t.split('/')[-2]\n",
    "            sub = t.split('/')[-1].replace('_defaced_refaced', '').split('.nii.gz')[0]\n",
    "            search_pattern = os.path.join(DS, sub)\n",
    "            \n",
    "            # match pattern from defaced dataset\n",
    "            for _d in dpaths:\n",
    "                if search_pattern in _d:\n",
    "                    test_d.append(_d)\n",
    "                \n",
    "\n",
    "    test = test_f + test_d\n",
    "    labels_test = [1]*len(test_f) + [0]*len(test_d)\n",
    "    \n",
    "    # remove T_A_D from defaced volume set\n",
    "    main_d = list(set(dpaths) - set(test_d))\n",
    "    \n",
    "    labels_main = [1]*len(main_f) + [0]*len(main_d)\n",
    "    main = main_f + main_d\n",
    "    \n",
    "    return main, labels_main, test, labels_test\n",
    "\n",
    "A_2, L_A_2, T_A, L_T_A = generate_datasets(paths_f, paths_d, 49, typ='faced')\n",
    "B_2, L_B_2, T_B, L_T_B = generate_datasets(paths_r, paths_d, 49, typ='refaced')\n",
    "\n",
    "print(len(A_2), len(T_A))\n",
    "print(len(B_2), len(T_B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate n-fold CV Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle\n",
    "import os\n",
    "\n",
    "def generate_CSV(paths, labels, save_path, test_paths=None, test_labels=None, n=15, mode='CV'):\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"X\"] = paths\n",
    "    df[\"Y\"] = labels\n",
    "    df.to_csv(os.path.join(save_path, \"all.csv\"))\n",
    "    \n",
    "    if mode == 'CV':\n",
    "        SPLITS = n\n",
    "        skf = StratifiedKFold(n_splits=SPLITS)\n",
    "        fold_no = 1\n",
    "\n",
    "        for train_index, test_index in skf.split(paths, labels):\n",
    "            out_path = os.path.join(save_path, \"train_test_fold_{}/csv/\".format(fold_no))\n",
    "\n",
    "            if not os.path.exists(out_path):\n",
    "                os.makedirs(out_path)\n",
    "\n",
    "            image_train, image_test = (\n",
    "                itemgetter(*train_index)(paths),\n",
    "                itemgetter(*test_index)(paths),\n",
    "            )\n",
    "\n",
    "            label_train, label_test = (\n",
    "                itemgetter(*train_index)(labels),\n",
    "                itemgetter(*test_index)(labels),\n",
    "            )\n",
    "\n",
    "            train_data = {\"X\": image_train , \"Y\": label_train}\n",
    "            df_train = pd.DataFrame(train_data)\n",
    "            df_train.to_csv(os.path.join(out_path, \"training.csv\"), index=False)\n",
    "\n",
    "            validation_data = {\"X\": image_test, \"Y\": label_test}\n",
    "            df_validation = pd.DataFrame(validation_data)\n",
    "            df_validation.to_csv(os.path.join(out_path, \"validation.csv\"), index=False)\n",
    "\n",
    "            fold_no += 1\n",
    "    else:\n",
    "        train_data = {\"X\": paths , \"Y\": labels}\n",
    "        df_train = pd.DataFrame(train_data)\n",
    "        df_train.to_csv(os.path.join(save_path, \"training.csv\"), index=False)\n",
    "        \n",
    "        test_data = {\"X\": test_paths , \"Y\": test_labels}\n",
    "        df_test = pd.DataFrame(test_data)\n",
    "        df_test.to_csv(os.path.join(save_path, \"testing.csv\"), index=False)\n",
    "        \n",
    "## CROSS VALIDATION\n",
    "# generate_CSV(A_2, L_A_2, \"/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_A/csv_F15\")\n",
    "# generate_CSV(B_2, L_B_2, \"/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_B/csv_F15\")\n",
    "\n",
    "\n",
    "## FULL DATASET\n",
    "generate_CSV(A_2,\n",
    "             L_A_2,\n",
    "             \"/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_A/csv_full\",\n",
    "             test_paths=T_A,\n",
    "             test_labels=L_T_A,\n",
    "             mode='full')\n",
    "\n",
    "generate_CSV(B_2,\n",
    "             L_B_2,\n",
    "             \"/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_B/csv_full\",\n",
    "             test_paths=T_B,\n",
    "             test_labels=L_T_B,\n",
    "             mode='full')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate tfrecords for n-fold CV datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667/667 [==============================] - 10s 15ms/step\n",
      "98/98 [==============================] - 2s 23ms/step\n",
      "662/662 [==============================] - 11s 17ms/step\n",
      "99/99 [==============================] - 2s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nobrainer\n",
    "import os, sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from shutil import *\n",
    "import subprocess\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_tfrecords(csv_path, records_save_path, mode='CV'):\n",
    "    \n",
    "    \n",
    "    os.makedirs(records_save_path, exist_ok=True)\n",
    "\n",
    "    train_csv_path = os.path.join(csv_path, \"training.csv\")\n",
    "    \n",
    "\n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "    train_D = list(zip(train_paths, train_labels))\n",
    "    \n",
    "    random.shuffle(train_D)\n",
    "\n",
    "    train_write_path = os.path.join(records_save_path, 'data-train_shard-{shard:03d}.tfrec')\n",
    "    \n",
    "    nobrainer.tfrecord.write(\n",
    "        features_labels=train_D,\n",
    "        filename_template=train_write_path,\n",
    "        examples_per_shard=3)\n",
    "    \n",
    "    if mode =='CV':\n",
    "        valid_csv_path = os.path.join(csv_path, \"validation.csv\")\n",
    "        valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "        valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "        valid_D = list(zip(valid_paths, valid_labels))\n",
    "        random.shuffle(valid_D)\n",
    "        valid_write_path = os.path.join(records_save_path, 'data-valid_shard-{shard:03d}.tfrec')\n",
    "\n",
    "        nobrainer.tfrecord.write(\n",
    "            features_labels=valid_D,\n",
    "            filename_template=valid_write_path,\n",
    "            examples_per_shard=1)\n",
    "    \n",
    "    if mode == 'test':\n",
    "        test_csv_path = os.path.join(csv_path, \"testing.csv\")\n",
    "        test_paths = pd.read_csv(test_csv_path)[\"X\"].values\n",
    "        test_labels = pd.read_csv(test_csv_path)[\"Y\"].values\n",
    "        test_D = list(zip(test_paths, test_labels))\n",
    "        random.shuffle(test_D)\n",
    "        test_write_path = os.path.join(records_save_path, 'data-test_shard-{shard:03d}.tfrec')\n",
    "\n",
    "        nobrainer.tfrecord.write(\n",
    "            features_labels=test_D,\n",
    "            filename_template=test_write_path,\n",
    "            examples_per_shard=1)\n",
    "\n",
    "## Cross-Validation \n",
    "# SPLITS = 15\n",
    "# for fold in range(1, SPLITS+1):\n",
    "#     print(\"FOLD: \", fold)\n",
    "#     csv_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_B/csv_F15/train_test_fold_{}/csv/\".format(fold)\n",
    "#     tf_records_dir = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_B/tfrecords_F15/tfrecords_fold_{}/\".format(fold)\n",
    "#     generate_tfrecords(csv_path, tf_records_dir)\n",
    "\n",
    "\n",
    "## Test (full dataset)\n",
    "## experiment_A\n",
    "csv_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_A/csv_full/\"\n",
    "tf_records_dir = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_A/tfrecords_full\"\n",
    "generate_tfrecords(csv_path, tf_records_dir, mode='test')\n",
    "\n",
    "## experiment_B\n",
    "csv_path = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_B/csv_full/\"\n",
    "tf_records_dir = \"/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_B/tfrecords_full\"\n",
    "generate_tfrecords(csv_path, tf_records_dir, mode='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize dataset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RepeatDataset shapes: ((8, 64, 64, 1), (8, 1)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "import nobrainer\n",
    "from nobrainer.io import _is_gzipped\n",
    "from nobrainer.volume import to_blocks\n",
    "import sys, os\n",
    "sys.path.append('../defacing')\n",
    "from preprocessing.augmentation import VolumeAugmentations, SliceAugmentations\n",
    "from helpers.utils import load_vol\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_A/tfrecords_F15'\n",
    "DISTRIBUTION = load_vol('../defacing/helpers/distribution.nii.gz')[0]\n",
    "DISTRIBUTION /= DISTRIBUTION.sum()\n",
    "COM = np.unravel_index(int(np.sum(DISTRIBUTION.ravel()*np.arange(len(DISTRIBUTION.ravel())))/np.sum(DISTRIBUTION.ravel())), DISTRIBUTION.shape)\n",
    "\n",
    "\n",
    "# sampling from augmented distribution is same as augmenting the sampled points\n",
    "# augmenting distribution at every iteration is expensive, so this way\n",
    "sampler = lambda n, distribution = DISTRIBUTION, threshold = 0.1: np.array([ np.unravel_index(\n",
    "          np.random.choice(np.arange(np.prod(distribution.shape)),\n",
    "                                     p = distribution.ravel()),\n",
    "          distribution.shape) + (+1 if np.random.randn() > 0.5 else -1)*np.random.randint(0, \n",
    "                                        int(distribution.shape[0]*threshold) + 1, 3) for _ in range(n)]) \n",
    "\n",
    "\n",
    "three_d_augmentations = {'rotation': 0.5,\n",
    "                         'translation': 0.5,\n",
    "                         'noop': 0.3\n",
    "                        }\n",
    "\n",
    "augmentvolume = VolumeAugmentations(DISTRIBUTION, three_d_augmentations)\n",
    "\n",
    "two_d_augmentations = {'rotation': 0.5,\n",
    "                       'fliplr': 0.5,\n",
    "                       'flipud': 0.5,\n",
    "                       'zoom': 0.5,\n",
    "                       'noop': 0.3\n",
    "                      }\n",
    "\n",
    "# augmentslice = VolumeAugmentations(DISTRIBUTION, two_d_augmentations)\n",
    "\n",
    "\n",
    "def get_dataset(\n",
    "    file_pattern,\n",
    "    n_classes,\n",
    "    batch_size,\n",
    "    volume_shape,\n",
    "    plane,\n",
    "    n = 24,\n",
    "    block_shape=None,\n",
    "    n_epochs=None,\n",
    "    mapping=None,\n",
    "    augment=False,\n",
    "    shuffle_buffer_size=None,\n",
    "    num_parallel_calls=AUTOTUNE,\n",
    "    mode='train',\n",
    "):\n",
    "\n",
    "    \"\"\" Returns tf.data.Dataset after preprocessing from\n",
    "    tfrecords for training and validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_pattern:\n",
    "\n",
    "    n_classes:\n",
    "    \"\"\"\n",
    "\n",
    "    files = glob.glob(file_pattern)\n",
    "\n",
    "    if not files:\n",
    "        raise ValueError(\"no files found for pattern '{}'\".format(file_pattern))\n",
    "\n",
    "    compressed = _is_gzipped(files[0])\n",
    "    shuffle = bool(shuffle_buffer_size)\n",
    "\n",
    "    ds = nobrainer.dataset.tfrecord_dataset(\n",
    "        file_pattern=file_pattern,\n",
    "        volume_shape=volume_shape,\n",
    "        shuffle=shuffle,\n",
    "        scalar_label=True,\n",
    "        compressed=compressed,\n",
    "        num_parallel_calls=num_parallel_calls,\n",
    "    )\n",
    "\n",
    "    # if augment:\n",
    "    #     ds = ds.map(\n",
    "    #         lambda x, y: tf.cond(\n",
    "    #             tf.random.uniform((1,)) > 0.5,\n",
    "    #             true_fn=lambda: apply_augmentations(x, y),\n",
    "    #             false_fn=lambda: (x, y),\n",
    "    #         ),\n",
    "    #         num_parallel_calls=num_parallel_calls,\n",
    "    #     )\n",
    "\n",
    "    def _ss(x, y):\n",
    "        if augment:\n",
    "            if three_d_augmentations['noop'] < 1:\n",
    "                x, y = augmentvolume(x,y)\n",
    "        x, y = structural_slice(x, y, \n",
    "                                plane, \n",
    "                                n, \n",
    "                                augment, \n",
    "                                augmentvolume.distribution)\n",
    "        return (x, y)\n",
    "    \n",
    "    \n",
    "    ds = ds.map(_ss, num_parallel_calls)\n",
    "    \n",
    "    ds = ds.prefetch(buffer_size=batch_size)\n",
    "\n",
    "    if batch_size is not None:\n",
    "        ds = ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "        \n",
    "    if mode == 'train':\n",
    "        if shuffle_buffer_size:\n",
    "            ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "        # Repeat the dataset n_epochs times\n",
    "        ds = ds.repeat(n_epochs)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def structural_slice(x, y, \n",
    "                plane, \n",
    "                n = 4, \n",
    "                augment = False, \n",
    "                distribution = DISTRIBUTION):\n",
    "\n",
    "    \"\"\" Transpose dataset based on the plane\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x:\n",
    "\n",
    "    y:\n",
    "\n",
    "    plane:\n",
    "    \n",
    "    n:\n",
    "\n",
    "    augment:\n",
    "    \"\"\"\n",
    "\n",
    "    threshold = 0.1 if augment else 0.0 \n",
    "    options = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "    shape = np.array(x.shape)\n",
    "\n",
    "    if isinstance(plane, str) and plane in options:\n",
    "        idxs = sampler(n, \n",
    "                        distribution, \n",
    "                        threshold)\n",
    "\n",
    "        if plane == \"axial\":\n",
    "            idx = np.random.randint(shape[0]**0.5)\n",
    "            midx = idxs[:, 0]\n",
    "            x = x\n",
    "\n",
    "        if plane == \"coronal\":\n",
    "            idx = np.random.randint(shape[1]**0.5)\n",
    "            midx = idxs[:, 1]\n",
    "            x = tf.transpose(x, perm=[1, 2, 0])\n",
    "\n",
    "\n",
    "        if plane == \"sagittal\":\n",
    "            idx = np.random.randint(shape[2]**0.5)\n",
    "            midx = idxs[:, 2]\n",
    "            x = tf.transpose(x, perm=[2, 0, 1])\n",
    "\n",
    "\n",
    "        if plane == \"combined\":\n",
    "            temp = {}\n",
    "            for op in options[:-1]:\n",
    "                temp[op] = structural_slice(x, y, \n",
    "                                            op, \n",
    "                                            n, \n",
    "                                            augment, \n",
    "                                            distribution)[0]\n",
    "            x = temp\n",
    "\n",
    "        if not plane == \"combined\":\n",
    "            x = tf.squeeze(tf.gather_nd(x, midx.reshape(n, 1, 1)), axis=1)\n",
    "            x = tf.math.reduce_mean(x, axis=0)\n",
    "            x = tf.expand_dims(x, axis=-1)\n",
    "            \n",
    "            if augment:\n",
    "                x = two_d_augmentations(x)\n",
    "                \n",
    "            x = tf.convert_to_tensor(x)\n",
    "        return x, y\n",
    "    else:\n",
    "        raise ValueError(\"expected plane to be one of ['axial', 'coronal', 'sagittal']\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    n_classes = 2\n",
    "    global_batch_size = 8\n",
    "    volume_shape = (64, 64, 64)\n",
    "    ds = get_dataset(\n",
    "        os.path.join(ROOTDIR, \"tfrecords_fold_1/data-train_*\"),\n",
    "        n_classes=n_classes,\n",
    "        batch_size=global_batch_size,\n",
    "        volume_shape=volume_shape,\n",
    "        plane=\"sagittal\",\n",
    "        augment = False,\n",
    "        shuffle_buffer_size=3,\n",
    "    )\n",
    "\n",
    "    import matplotlib\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Std packages\n",
    "import sys, os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "sys.path.append(\"../defacing\")\n",
    "\n",
    "# Custom packages\n",
    "from models import modelN\n",
    "# from dataloaders.dataset import get_dataset\n",
    "\n",
    "# Tf packages\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateScheduler,\n",
    "    TensorBoard,\n",
    ")\n",
    "# import nobrainer\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "\n",
    "def scheduler(epoch):\n",
    "    if epoch < 3:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.001 * tf.math.exp(0.1 * (10 - epoch))\n",
    "\n",
    "\n",
    "def train(\n",
    "    csv_path,\n",
    "    model_save_path,\n",
    "    tfrecords_path,\n",
    "    volume_shape=(64, 64, 64),\n",
    "    image_size=(64, 64),\n",
    "    dropout=0.2,\n",
    "    batch_size=16,\n",
    "    n_classes=2,\n",
    "    n_epochs=15,\n",
    "    percent=100,\n",
    "    mode='CV',\n",
    "):\n",
    "    \n",
    "    \n",
    "    train_csv_path = os.path.join(csv_path, \"training.csv\")\n",
    "    train_paths = pd.read_csv(train_csv_path)[\"X\"].values\n",
    "    train_labels = pd.read_csv(train_csv_path)[\"Y\"].values\n",
    "    \n",
    "    if mode == 'CV':\n",
    "        valid_csv_path = os.path.join(csv_path, \"validation.csv\")\n",
    "        valid_paths = pd.read_csv(valid_csv_path)[\"X\"].values\n",
    "        valid_labels = pd.read_csv(valid_csv_path)[\"Y\"].values\n",
    "    \n",
    "    weights = class_weight.compute_class_weight('balanced',\n",
    "                                                np.unique(train_labels),\n",
    "                                                train_labels)\n",
    "    weights = dict(enumerate(weights))\n",
    "    \n",
    "    print(weights)\n",
    "    \n",
    "    planes = [\"axial\", \"coronal\", \"sagittal\", \"combined\"]\n",
    "    \n",
    "\n",
    "    global_batch_size = batch_size\n",
    "    \n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    cp_save_path = os.path.join(model_save_path, \"weights\")\n",
    "\n",
    "    logdir_path = os.path.join(model_save_path, \"tb_logs\")\n",
    "    if not os.path.exists(logdir_path):\n",
    "        os.makedirs(logdir_path)\n",
    "\n",
    "    for plane in planes:\n",
    "\n",
    "        logdir = os.path.join(logdir_path, plane)\n",
    "        os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "        tbCallback = TensorBoard(\n",
    "            log_dir=logdir, histogram_freq=1, write_graph=True, write_images=True,\n",
    "        )\n",
    "\n",
    "        os.makedirs(os.path.join(cp_save_path, plane), exist_ok=True)\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint(\n",
    "            os.path.join(cp_save_path, plane, \"best-wts.h5\"),\n",
    "            monitor=\"val_loss\",\n",
    "            save_weights_only=True,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "\n",
    "#         with strategy.scope():\n",
    "\n",
    "        if not plane == \"combined\": \n",
    "            lr = 1e-3\n",
    "            model = modelN.Submodel(\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                name=plane,\n",
    "                include_top=True,\n",
    "                weights=None,\n",
    "            )\n",
    "        else:\n",
    "            lr = 5e-4\n",
    "            model = modelN.CombinedClassifier(\n",
    "                input_shape=image_size,\n",
    "                dropout=dropout,\n",
    "                trainable=True,\n",
    "                wts_root=cp_save_path,\n",
    "            )\n",
    "\n",
    "        print(\"Submodel: \", plane)\n",
    "#         print(model.summary())\n",
    "\n",
    "        METRICS = [\n",
    "            metrics.TruePositives(name=\"tp\"),\n",
    "            metrics.FalsePositives(name=\"fp\"),\n",
    "            metrics.TrueNegatives(name=\"tn\"),\n",
    "            metrics.FalseNegatives(name=\"fn\"),\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            metrics.Precision(name=\"precision\"),\n",
    "            metrics.Recall(name=\"recall\"),\n",
    "            metrics.AUC(name=\"auc\"),\n",
    "        ]\n",
    "\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.binary_crossentropy,\n",
    "            optimizer=Adam(learning_rate=lr),\n",
    "            metrics=METRICS,\n",
    "        )\n",
    "\n",
    "        print(\"GLOBAL BATCH SIZE: \", global_batch_size)\n",
    "\n",
    "        dataset_train = get_dataset(\n",
    "            file_pattern=os.path.join(tfrecords_path, 'data-train_*'),\n",
    "            n_classes=n_classes,\n",
    "            batch_size=global_batch_size,\n",
    "            volume_shape=volume_shape,\n",
    "            plane=plane,\n",
    "            shuffle_buffer_size=global_batch_size,\n",
    "        )\n",
    "        \n",
    "        steps_per_epoch = math.ceil(len(train_paths)/batch_size)\n",
    "        print(steps_per_epoch)\n",
    "        lrcallback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "        \n",
    "        if mode == 'CV':\n",
    "            dataset_valid = get_dataset(\n",
    "                file_pattern=os.path.join(tfrecords_path, \"data-valid_*\"),\n",
    "                n_classes=n_classes,\n",
    "                batch_size=global_batch_size,\n",
    "                volume_shape=volume_shape,\n",
    "                plane=plane,\n",
    "                shuffle_buffer_size=global_batch_size,\n",
    "            )\n",
    "            \n",
    "            validation_steps = math.ceil(len(valid_paths)/batch_size)\n",
    "            \n",
    "            model.fit(\n",
    "                dataset_train,\n",
    "                epochs=n_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                validation_data=dataset_valid,\n",
    "                validation_steps=validation_steps,\n",
    "                callbacks=[tbCallback, model_checkpoint],\n",
    "                class_weight = weights,\n",
    "            )\n",
    "        else:\n",
    "            model.fit(\n",
    "                dataset_train,\n",
    "                epochs=n_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                callbacks=[tbCallback, model_checkpoint],\n",
    "                class_weight = weights,\n",
    "            )\n",
    "\n",
    "        del model\n",
    "        \n",
    "        K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_A'\n",
    "for fold in range(1, 15):\n",
    "    print(\"FOLD {}\".format(fold))\n",
    "    csv_path = os.path.join(ROOTDIR, \"csv_F15/train_test_fold_{}/csv\".format(fold))\n",
    "    model_save_path = os.path.join(ROOTDIR, \"model_save_dir_F15/train_test_fold_{}\".format(fold))\n",
    "    tfrecords_path = os.path.join(ROOTDIR, 'tfrecords_F15/tfrecords_fold_{}'.format(fold))\n",
    "\n",
    "    train(\n",
    "        csv_path,\n",
    "        model_save_path,\n",
    "        tfrecords_path,\n",
    "        mode='CV'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard graph visualization\n",
    "\n",
    "NOTE: For this to work, you need to have the 6006 port binded from the localhost as shown in the first block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_A/model_save_dir_F15 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.9934934934934935, 1: 1.0065922920892494}\n",
      "Submodel:  axial\n",
      "GLOBAL BATCH SIZE:  16\n",
      "125\n",
      "Train for 125 steps\n",
      "Epoch 1/15\n",
      "125/125 [==============================] - 4s 34ms/step - loss: 0.2162 - tp: 930.0000 - fp: 58.0000 - tn: 947.0000 - fn: 65.0000 - accuracy: 0.9385 - precision: 0.9413 - recall: 0.9347 - auc: 0.9786\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.0112 - tp: 986.0000 - fp: 4.0000 - tn: 1006.0000 - fn: 4.0000 - accuracy: 0.9960 - precision: 0.9960 - recall: 0.9960 - auc: 0.9999\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.0076 - tp: 986.0000 - fp: 2.0000 - tn: 1009.0000 - fn: 3.0000 - accuracy: 0.9975 - precision: 0.9980 - recall: 0.9970 - auc: 1.0000\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.0035 - tp: 1007.0000 - fp: 1.0000 - tn: 991.0000 - fn: 1.0000 - accuracy: 0.9990 - precision: 0.9990 - recall: 0.9990 - auc: 1.0000\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.0069 - tp: 986.0000 - fp: 5.0000 - tn: 1006.0000 - fn: 3.0000 - accuracy: 0.9960 - precision: 0.9950 - recall: 0.9970 - auc: 1.0000\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 2s 14ms/step - loss: 6.9718e-04 - tp: 989.0000 - fp: 0.0000e+00 - tn: 1011.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 6.3959e-05 - tp: 994.0000 - fp: 0.0000e+00 - tn: 1006.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 7.5806e-05 - tp: 993.0000 - fp: 0.0000e+00 - tn: 1007.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 1.8538e-04 - tp: 992.0000 - fp: 0.0000e+00 - tn: 1008.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 5.8399e-05 - tp: 997.0000 - fp: 0.0000e+00 - tn: 1003.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 3.5991e-05 - tp: 986.0000 - fp: 0.0000e+00 - tn: 1014.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 2.4016e-05 - tp: 1002.0000 - fp: 0.0000e+00 - tn: 998.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 2.2406e-05 - tp: 984.0000 - fp: 0.0000e+00 - tn: 1016.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 4.2700e-05 - tp: 997.0000 - fp: 0.0000e+00 - tn: 1003.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 2.0470e-05 - tp: 995.0000 - fp: 0.0000e+00 - tn: 1005.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Submodel:  coronal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "125\n",
      "Train for 125 steps\n",
      "Epoch 1/15\n",
      "125/125 [==============================] - 4s 34ms/step - loss: 0.1697 - tp: 939.0000 - fp: 44.0000 - tn: 966.0000 - fn: 51.0000 - accuracy: 0.9525 - precision: 0.9552 - recall: 0.9485 - auc: 0.9839\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.0466 - tp: 983.0000 - fp: 6.0000 - tn: 994.0000 - fn: 17.0000 - accuracy: 0.9885 - precision: 0.9939 - recall: 0.9830 - auc: 0.9965\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.0282 - tp: 980.0000 - fp: 7.0000 - tn: 1002.0000 - fn: 11.0000 - accuracy: 0.9910 - precision: 0.9929 - recall: 0.9889 - auc: 0.9993\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.0191 - tp: 988.0000 - fp: 5.0000 - tn: 1001.0000 - fn: 6.0000 - accuracy: 0.9945 - precision: 0.9950 - recall: 0.9940 - auc: 0.9988\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 3s 21ms/step - loss: 0.0069 - tp: 989.0000 - fp: 2.0000 - tn: 1008.0000 - fn: 1.0000 - accuracy: 0.9985 - precision: 0.9980 - recall: 0.9990 - auc: 0.9995\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.0203 - tp: 991.0000 - fp: 3.0000 - tn: 1000.0000 - fn: 6.0000 - accuracy: 0.9955 - precision: 0.9970 - recall: 0.9940 - auc: 0.9984\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.0140 - tp: 978.0000 - fp: 5.0000 - tn: 1013.0000 - fn: 4.0000 - accuracy: 0.9955 - precision: 0.9949 - recall: 0.9959 - auc: 0.9994\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.0019 - tp: 999.0000 - fp: 0.0000e+00 - tn: 1000.0000 - fn: 1.0000 - accuracy: 0.9995 - precision: 1.0000 - recall: 0.9990 - auc: 1.0000\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.0384 - tp: 992.0000 - fp: 9.0000 - tn: 992.0000 - fn: 7.0000 - accuracy: 0.9920 - precision: 0.9910 - recall: 0.9930 - auc: 0.9992\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.0079 - tp: 989.0000 - fp: 3.0000 - tn: 1006.0000 - fn: 2.0000 - accuracy: 0.9975 - precision: 0.9970 - recall: 0.9980 - auc: 0.9995\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.0190 - tp: 993.0000 - fp: 5.0000 - tn: 996.0000 - fn: 6.0000 - accuracy: 0.9945 - precision: 0.9950 - recall: 0.9940 - auc: 0.9993\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.0176 - tp: 984.0000 - fp: 4.0000 - tn: 1007.0000 - fn: 5.0000 - accuracy: 0.9955 - precision: 0.9960 - recall: 0.9949 - auc: 0.9990\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 2.6255e-04 - tp: 1000.0000 - fp: 0.0000e+00 - tn: 1000.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 3s 22ms/step - loss: 0.0011 - tp: 993.0000 - fp: 0.0000e+00 - tn: 1007.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.0072 - tp: 970.0000 - fp: 2.0000 - tn: 1026.0000 - fn: 2.0000 - accuracy: 0.9980 - precision: 0.9979 - recall: 0.9979 - auc: 0.9995\n",
      "Submodel:  sagittal\n",
      "GLOBAL BATCH SIZE:  16\n",
      "125\n",
      "Train for 125 steps\n",
      "Epoch 1/15\n",
      "125/125 [==============================] - 5s 37ms/step - loss: 0.1039 - tp: 953.0000 - fp: 30.0000 - tn: 980.0000 - fn: 37.0000 - accuracy: 0.9665 - precision: 0.9695 - recall: 0.9626 - auc: 0.9917\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 0.0326 - tp: 990.0000 - fp: 9.0000 - tn: 993.0000 - fn: 8.0000 - accuracy: 0.9915 - precision: 0.9910 - recall: 0.9920 - auc: 0.9982\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.0214 - tp: 992.0000 - fp: 6.0000 - tn: 994.0000 - fn: 8.0000 - accuracy: 0.9930 - precision: 0.9940 - recall: 0.9920 - auc: 0.9989\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 0.0091 - tp: 986.0000 - fp: 3.0000 - tn: 1009.0000 - fn: 2.0000 - accuracy: 0.9975 - precision: 0.9970 - recall: 0.9980 - auc: 0.9995\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 6.0274e-04 - tp: 997.0000 - fp: 0.0000e+00 - tn: 1003.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 2.5582e-04 - tp: 985.0000 - fp: 0.0000e+00 - tn: 1015.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 4.0822e-04 - tp: 995.0000 - fp: 0.0000e+00 - tn: 1005.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 2s 17ms/step - loss: 1.6949e-04 - tp: 993.0000 - fp: 0.0000e+00 - tn: 1007.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 5.3871e-05 - tp: 996.0000 - fp: 0.0000e+00 - tn: 1004.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 0s - loss: 5.8753e-05 - tp: 905.0000 - fp: 0.0000e+00 - tn: 919.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 1.1896e-04 - tp: 988.0000 - fp: 0.0000e+00 - tn: 1012.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 5.9717e-05 - tp: 995.0000 - fp: 0.0000e+00 - tn: 1005.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 0s - loss: 6.4531e-05 - tp: 825.0000 - fp: 0.0000e+00 - tn: 855.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 1.7120e-05 - tp: 1003.0000 - fp: 0.0000e+00 - tn: 997.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 2s 18ms/step - loss: 4.1188e-05 - tp: 981.0000 - fp: 0.0000e+00 - tn: 1019.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 8.3565e-06 - tp: 1003.0000 - fp: 0.0000e+00 - tn: 997.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 2s 17ms/step - loss: 1.6863e-05 - tp: 991.0000 - fp: 0.0000e+00 - tn: 1009.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000 0s - loss: 1.8355e-05 - tp: 778.0000 - fp: 0.0000e+00 - tn: 806.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 -\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 64, 64, 8)    80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, 64, 8)    80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 64, 64, 8)    80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64, 8)    32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, 64, 8)    32          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 8)    32          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 64, 64, 8)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, 64, 8)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 64, 64, 8)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 64, 64, 8)    584         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 64, 64, 8)    584         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, 64, 8)    584         activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64, 64, 8)    32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64, 64, 8)    32          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, 64, 8)    32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 64, 64, 8)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 64, 64, 8)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, 64, 8)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 32, 32, 8)    0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 8)    0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 8)    0           activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   1168        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   1168        max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 16)   1168        max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 32, 32, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2320        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2320        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 16)   2320        activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 32, 32, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 16)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 16)   0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 16, 16, 16)   0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 16, 16, 32)   4640        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 32)   4640        max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 16, 16, 32)   128         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 32)   128         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 16, 16, 32)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 32)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 16, 16, 32)   9248        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9248        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 32)   9248        activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 16, 16, 32)   128         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 32)   128         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 16, 16, 32)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 32)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 32)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 8, 8, 32)     0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 8, 8, 32)     0           activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 2048)         0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2048)         0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 2048)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          524544      add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 580,265\n",
      "Trainable params: 579,593\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Submodel:  combined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL BATCH SIZE:  16\n",
      "125\n",
      "Train for 125 steps\n",
      "Epoch 1/15\n",
      "125/125 [==============================] - 6s 50ms/step - loss: 0.1352 - tp: 961.0000 - fp: 33.0000 - tn: 972.0000 - fn: 34.0000 - accuracy: 0.9665 - precision: 0.9668 - recall: 0.9658 - auc: 0.9875\n",
      "Epoch 2/15\n",
      "125/125 [==============================] - 4s 29ms/step - loss: 0.0214 - tp: 988.0000 - fp: 4.0000 - tn: 1002.0000 - fn: 6.0000 - accuracy: 0.9950 - precision: 0.9960 - recall: 0.9940 - auc: 0.9988\n",
      "Epoch 3/15\n",
      "125/125 [==============================] - 3s 24ms/step - loss: 0.0162 - tp: 990.0000 - fp: 5.0000 - tn: 1000.0000 - fn: 5.0000 - accuracy: 0.9950 - precision: 0.9950 - recall: 0.9950 - auc: 0.9994\n",
      "Epoch 4/15\n",
      "125/125 [==============================] - 3s 24ms/step - loss: 0.0168 - tp: 988.0000 - fp: 4.0000 - tn: 1004.0000 - fn: 4.0000 - accuracy: 0.9960 - precision: 0.9960 - recall: 0.9960 - auc: 0.9984\n",
      "Epoch 5/15\n",
      "125/125 [==============================] - 3s 23ms/step - loss: 0.0067 - tp: 995.0000 - fp: 2.0000 - tn: 1000.0000 - fn: 3.0000 - accuracy: 0.9975 - precision: 0.9980 - recall: 0.9970 - auc: 1.0000\n",
      "Epoch 6/15\n",
      "125/125 [==============================] - 3s 24ms/step - loss: 5.6638e-04 - tp: 981.0000 - fp: 0.0000e+00 - tn: 1019.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 7/15\n",
      "125/125 [==============================] - 3s 24ms/step - loss: 2.6293e-04 - tp: 1006.0000 - fp: 0.0000e+00 - tn: 994.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 8/15\n",
      "125/125 [==============================] - 3s 23ms/step - loss: 1.4403e-04 - tp: 983.0000 - fp: 0.0000e+00 - tn: 1017.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 9/15\n",
      "125/125 [==============================] - 3s 28ms/step - loss: 3.9466e-05 - tp: 997.0000 - fp: 0.0000e+00 - tn: 1003.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 10/15\n",
      "125/125 [==============================] - 3s 23ms/step - loss: 4.9660e-05 - tp: 983.0000 - fp: 0.0000e+00 - tn: 1017.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 11/15\n",
      "125/125 [==============================] - 3s 23ms/step - loss: 2.3975e-04 - tp: 1015.0000 - fp: 0.0000e+00 - tn: 985.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 12/15\n",
      "125/125 [==============================] - 3s 24ms/step - loss: 7.5296e-06 - tp: 976.0000 - fp: 0.0000e+00 - tn: 1024.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 13/15\n",
      "125/125 [==============================] - 3s 23ms/step - loss: 2.2278e-05 - tp: 1002.0000 - fp: 0.0000e+00 - tn: 998.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 14/15\n",
      "125/125 [==============================] - 3s 22ms/step - loss: 1.2144e-05 - tp: 987.0000 - fp: 0.0000e+00 - tn: 1013.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n",
      "Epoch 15/15\n",
      "125/125 [==============================] - 3s 23ms/step - loss: 8.3596e-06 - tp: 1014.0000 - fp: 0.0000e+00 - tn: 986.0000 - fn: 0.0000e+00 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - auc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_B'\n",
    "\n",
    "csv_path = os.path.join(ROOTDIR, \"csv_full\")\n",
    "model_save_path = os.path.join(ROOTDIR, \"model_save_dir_full\")\n",
    "tfrecords_path = os.path.join(ROOTDIR, 'tfrecords_full')\n",
    "\n",
    "\n",
    "train(\n",
    "    csv_path,\n",
    "    model_save_path,\n",
    "    tfrecords_path,\n",
    "    mode='full'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "axial (InputLayer)              [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sagittal (InputLayer)           [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "coronal (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 64, 64, 8)    80          axial[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 64, 64, 8)    80          sagittal[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 64, 64, 8)    80          coronal[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 64, 64, 8)    32          conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 64, 64, 8)    32          conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 64, 64, 8)    32          conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 64, 64, 8)    0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 64, 64, 8)    0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 64, 64, 8)    0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 64, 64, 8)    584         activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 64, 64, 8)    584         activation_132[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 64, 64, 8)    584         activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 64, 64, 8)    32          conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 64, 64, 8)    32          conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 64, 64, 8)    32          conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 64, 64, 8)    0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 64, 64, 8)    0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 64, 64, 8)    0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_63 (MaxPooling2D) (None, 32, 32, 8)    0           activation_127[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_66 (MaxPooling2D) (None, 32, 32, 8)    0           activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_69 (MaxPooling2D) (None, 32, 32, 8)    0           activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 32, 32, 16)   1168        max_pooling2d_63[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 32, 32, 16)   1168        max_pooling2d_66[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 32, 32, 16)   1168        max_pooling2d_69[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 32, 32, 16)   64          conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 32, 32, 16)   64          conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 32, 32, 16)   64          conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 32, 32, 16)   0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 32, 32, 16)   0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 32, 32, 16)   0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 32, 32, 16)   2320        activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 32, 32, 16)   2320        activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 32, 32, 16)   2320        activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 32, 32, 16)   64          conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 32, 32, 16)   64          conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 32, 32, 16)   64          conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 32, 32, 16)   0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 32, 32, 16)   0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 32, 32, 16)   0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_64 (MaxPooling2D) (None, 16, 16, 16)   0           activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_67 (MaxPooling2D) (None, 16, 16, 16)   0           activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_70 (MaxPooling2D) (None, 16, 16, 16)   0           activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 16, 16, 32)   4640        max_pooling2d_64[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 16, 16, 32)   4640        max_pooling2d_67[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 16, 16, 32)   4640        max_pooling2d_70[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 16, 16, 32)   128         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 16, 16, 32)   128         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 16, 16, 32)   128         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 16, 16, 32)   0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 16, 16, 32)   0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 16, 16, 32)   0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 16, 16, 32)   9248        activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 16, 16, 32)   9248        activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 16, 16, 32)   9248        activation_142[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 16, 16, 32)   128         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 16, 16, 32)   128         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 16, 16, 32)   128         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 16, 16, 32)   0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 16, 16, 32)   0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 16, 16, 32)   0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_65 (MaxPooling2D) (None, 8, 8, 32)     0           activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_68 (MaxPooling2D) (None, 8, 8, 32)     0           activation_137[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_71 (MaxPooling2D) (None, 8, 8, 32)     0           activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 2048)         0           max_pooling2d_65[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 2048)         0           max_pooling2d_68[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 2048)         0           max_pooling2d_71[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 2048)         0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          524544      add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 256)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output_node (Dense)             (None, 1)            257         dropout_7[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 580,265\n",
      "Trainable params: 579,593\n",
      "Non-trainable params: 672\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6/Unknown - 1s 126ms/step - loss: 0.7434 - accuracy: 0.8958 - precision: 1.0000 - recall: 0.7917 - auc: 0.9479"
     ]
    }
   ],
   "source": [
    "sys.path.append('../defacing')\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from models.modelN import CombinedClassifier\n",
    "\n",
    "\n",
    "ROOTDIR = '/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_B'\n",
    "\n",
    "csv_path = os.path.join(ROOTDIR, \"csv_full\")\n",
    "model_save_path = os.path.join(ROOTDIR, \"model_save_dir_full\")\n",
    "tfrecords_path = os.path.join(ROOTDIR, 'tfrecords_full')\n",
    "weights_path = os.path.join(model_save_path, 'weights/combined/best-wts.h5')\n",
    "\n",
    "\n",
    "model = CombinedClassifier(\n",
    "    input_shape=(64, 64), dropout=0.4, wts_root=None, trainable=True\n",
    ")\n",
    "model.load_weights(os.path.abspath(weights_path))\n",
    "\n",
    "\n",
    "dataset_test = get_dataset(\n",
    "    file_pattern=os.path.join('/work/06850/sbansal6/maverick2/mriqc-shared/experiments/experiment_A', \"tfrecords_full/data-test_*\"),\n",
    "    n_classes=2,\n",
    "    batch_size=16,\n",
    "    volume_shape=(64, 64, 64),\n",
    "    plane='combined',\n",
    "    shuffle_buffer_size=16,\n",
    "    mode='test'\n",
    ")\n",
    "\n",
    "METRICS = [\n",
    "            metrics.BinaryAccuracy(name=\"accuracy\"),\n",
    "            metrics.Precision(name=\"precision\"),\n",
    "            metrics.Recall(name=\"recall\"),\n",
    "            metrics.AUC(name=\"auc\"),\n",
    "        ]\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    metrics=METRICS,\n",
    ")\n",
    "\n",
    "    \n",
    "results = model.evaluate(dataset_test)\n",
    "predictions = (model.predict(dataset_test) > 0.5).astype(int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
